{
  "@context": "https://schema.org",
  "@type": "Collection",
  "name": "Supabase Backend Infrastructure Documentation",
  "description": "Complete flattened documentation for Supabase backend infrastructure categories including Database, Edge Functions, Storage, Deployment, Security, Queues, Cron, Resources, and CLI",
  "publisher": {
    "@type": "Organization",
    "name": "Supabase",
    "url": "https://supabase.com"
  },
  "dateCreated": "2025-06-13T12:45:11.312026",
  "totalItems": 172,
  "categories": [
    "database",
    "functions",
    "storage",
    "deployment",
    "security",
    "queues",
    "cron",
    "resources",
    "cli"
  ],
  "processingInfo": {
    "totalFilesProcessed": 172,
    "expectedTotalFiles": 172,
    "categoryCounts": {
      "database": 80,
      "functions": 51,
      "storage": 25,
      "deployment": 5,
      "security": 4,
      "queues": 3,
      "cron": 2,
      "resources": 1,
      "cli": 1
    }
  },
  "hasPart": [
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:cli",
      "identifier": "cli",
      "name": "Local Dev with CLI",
      "description": "Developing locally using the Supabase CLI.",
      "category": "cli",
      "url": "https://supabase.com/docs/guides/cli",
      "dateModified": "2025-06-13T12:45:11.277214",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/cli.mdx",
      "frontmatter": {
        "id": "cli",
        "title": "Local Dev with CLI",
        "description": "Developing locally using the Supabase CLI.",
        "subtitle": "Developing locally using the Supabase CLI.",
        "sidebar_label": "Overview"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You can use the Supabase CLI to run the entire Supabase stack locally on your machine, by running `supabase init` and then `supabase start`. To install the CLI, see the [installation guide](/docs/guides/cli/getting-started#installing-the-supabase-cli).\n\nThe Supabase CLI provides tools to develop your project locally, deploy to the Supabase Platform, handle database migrations, and generate types directly from your database schema."
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "{[\n {\n name: 'Supabase CLI',\n description:\n 'The Supabase CLI provides tools to develop manage your Supabase projects from your local machine.',\n href: 'https://github.com/supabase/cli',\n },\n {\n name: 'GitHub Action',\n description: ' A GitHub action for interacting with your Supabase projects using the CLI.',\n href: 'https://github.com/supabase/setup-cli',\n },\n ].map((x) => (\n\n {x.description}\n\n ))}",
          "level": 2
        }
      ],
      "wordCount": 110,
      "characterCount": 853
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:cron-install",
      "identifier": "cron-install",
      "name": "Install",
      "description": "",
      "category": "cron",
      "url": "https://supabase.com/docs/guides/cron/install",
      "dateModified": "2025-06-13T12:45:11.277361",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/cron/install.mdx",
      "frontmatter": {
        "title": "Install"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Install the Supabase Cron Postgres Module to begin scheduling recurring Jobs.\n\n1. Go to the [Cron Postgres Module](/dashboard/project/_/integrations/cron/overview) under Integrations in the Dashboard.\n2. Enable the `pg_cron` extension.\n\n```sql\ncreate extension pg_cron with schema pg_catalog;\n\ngrant usage on schema cron to postgres;\ngrant all privileges on all tables in schema cron to postgres;\n```"
        },
        {
          "type": "section",
          "title": "Uninstall",
          "content": "Uninstall Supabase Cron by disabling the `pg_cron` extension:\n\n```sql\ndrop extension if exists pg_cron;\n```\n\nDisabling the `pg_cron` extension will permanently delete all Jobs.",
          "level": 2
        }
      ],
      "wordCount": 80,
      "characterCount": 592
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:cron-quickstart",
      "identifier": "cron-quickstart",
      "name": "Quickstart",
      "description": "",
      "category": "cron",
      "url": "https://supabase.com/docs/guides/cron/quickstart",
      "dateModified": "2025-06-13T12:45:11.277727",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/cron/quickstart.mdx",
      "frontmatter": {
        "title": "Quickstart"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Job names are case sensitive and cannot be edited once created.\n\nAttempting to create a second Job with the same name (and case) will overwrite the first Job."
        },
        {
          "type": "section",
          "title": "Schedule a job",
          "content": "1. Go to the [Jobs](/dashboard/project/_/integrations/cron/jobs) section to schedule your first Job.\n2. Click on `Create job` button or navigate to the new Cron Job form [here](/dashboard/project/_/integrations/cron/jobs?dialog-shown=true).\n3. Name your Cron Job.\n4. Choose a schedule for your Job by inputting cron syntax (refer to the syntax chart in the form) or natural language.\n5. Input SQL snippet or select a Database function, HTTP request, or Supabase Edge Function.\n\n```sql\n-- Cron Job name cannot be edited\nselect cron.schedule('permanent-cron-job-name', '30 seconds', 'CALL do_something()');\n```\n\n ```\n ┌───────────── min (0 - 59)\n │ ┌────────────── hour (0 - 23)\n │ │ ┌─────────────── day of month (1 - 31)\n │ │ │ ┌──────────────── month (1 - 12)\n │ │ │ │ ┌───────────────── day of week (0 - 6) (0 to 6 are Sunday to\n │ │ │ │ │ Saturday, or use names; 7 is also Sunday)\n │ │ │ │ │\n │ │ │ │ │\n * * * * *\n ```\n\n You can use [1-59] seconds (e.g. `30 seconds`) as the cron syntax to schedule sub-minute Jobs.\n\nYou can input seconds for your Job schedule interval as long as you're on Postgres version 15.1.1.61 or later.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Edit a job",
          "content": "1. Go to the [Jobs](/dashboard/project/_/integrations/cron/jobs) section and find the Job you'd like to edit.\n2. Click on the three vertical dots menu on the right side of the Job and click `Edit cron job`.\n3. Make your changes and then click `Save cron job`.\n\n```sql\nselect cron.alter_job(\n job_id := (select jobid from cron.job where jobname = 'permanent-cron-job-name'),\n schedule := '*/5 * * * *'\n);\n```\n\nFull options for the `cron.alter_job()` function are:\n\n```sql\ncron.alter_job(\n job_id bigint,\n schedule text default null,\n command text default null,\n database text default null,\n username text default null,\n active boolean default null\n)\n```\n\nIt is also possible to modify a job by using the `cron.schedule()` function by inputting the same job name. This will replace the existing job via upsert.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Activate/Deactivate a job",
          "content": "1. Go to the [Jobs](/dashboard/project/_/integrations/cron/jobs) section and find the Job you'd like to unschedule.\n2. Toggle the `Active`/`Inactive` switch next to Job name.\n\n```sql\n-- Activate Job\nselect cron.alter_job(\n job_id := (select jobid from cron.job where jobname = 'permanent-cron-job-name'),\n active := true\n);\n\n-- Deactivate Job\nselect cron.alter_job(\n job_id := (select jobid from cron.job where jobname = 'permanent-cron-job-name'),\n active := false\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Unschedule a job",
          "content": "1. Go to the [Jobs](/dashboard/project/_/integrations/cron/jobs) section and find the Job you'd like to delete.\n2. Click on the three vertical dots menu on the right side of the Job and click `Delete cron job`.\n3. Confirm deletion by entering the Job name.\n\n```sql\nselect cron.unschedule('permanent-cron-job-name');\n```\n\nUnscheduling a Job will permanently delete the Job from `cron.job` table but its run history remain in `cron.job_run_details` table.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inspecting job runs",
          "content": "1. Go to the [Jobs](/dashboard/project/_/integrations/cron/jobs) section and find the Job you want to see the runs of.\n2. Click on the `History` button next to the Job name.\n\n```sql\nselect\n *\nfrom cron.job_run_details\nwhere jobid = (select jobid from cron.job where jobname = 'permanent-cron-job-name')\norder by start_time desc\nlimit 10;\n```\n\nThe records in the `cron.job_run_details` table are not cleaned up automatically. They are also not removed when jobs are unscheduled, which will take up disk space in your database.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Delete data every week",
          "content": "{/* */}\n\nDelete old data every Saturday at 3:30AM (GMT):\n\n{/* */}\n\n```sql\nselect cron.schedule (\n 'saturday-cleanup', -- name of the cron job\n '30 3 * * 6', -- Saturday at 3:30AM (GMT)\n $$ delete from events where event_time */}\n\nVacuum every day at 3:00AM (GMT):\n\n{/* */}\n\n```sql\nselect cron.schedule('nightly-vacuum', '0 3 * * *', 'VACUUM');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Call a database function every 5 minutes",
          "content": "Create a [`hello_world()`](/docs/guides/database/functions?language=sql#simple-functions) database function and then call it every 5 minutes:\n\n```sql\nselect cron.schedule('call-db-function', '*/5 * * * *', 'SELECT hello_world()');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Call a database stored procedure",
          "content": "To use a stored procedure, you can call it like this:\n\n```sql\nselect cron.schedule('call-db-procedure', '*/5 * * * *', 'CALL my_procedure()');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Invoke Supabase Edge Function every 30 seconds",
          "content": "Make a POST request to a Supabase Edge Function every 30 seconds:\n\n```sql\nselect\n cron.schedule(\n 'invoke-function-every-half-minute',\n '30 seconds',\n $$\n select\n net.http_post(\n url:='https://project-ref.supabase.co/functions/v1/function-name',\n headers:=jsonb_build_object('Content-Type','application/json', 'Authorization', 'Bearer ' || 'YOUR_ANON_KEY'),\n body:=jsonb_build_object('time', now() ),\n timeout_milliseconds:=5000\n ) as request_id;\n $$\n );\n```\n\nThis requires the [`pg_net` extension](/docs/guides/database/extensions/pg_net) to be enabled.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Caution: Scheduling system maintenance",
          "content": "Be extremely careful when setting up Jobs for system maintenance tasks as they can have unintended consequences.\n\nFor instance, scheduling a command to terminate idle connections with `pg_terminate_backend(pid)` can disrupt critical background processes like nightly backups. Often, there is an existing Postgres setting, such as `idle_session_timeout`, that can perform these common maintenance tasks without the risk.\n\nReach out to [Supabase Support](https://supabase.com/support) if you're unsure if that applies to your use case.",
          "level": 2
        }
      ],
      "wordCount": 829,
      "characterCount": 5709
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-arrays",
      "identifier": "database-arrays",
      "name": "Working With Arrays",
      "description": "How to use arrays in PostgreSQL and the Supabase API.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/arrays",
      "dateModified": "2025-06-13T12:45:11.278031",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/arrays.mdx",
      "frontmatter": {
        "id": "arrays",
        "title": "Working With Arrays",
        "description": "How to use arrays in PostgreSQL and the Supabase API."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres supports flexible [array types](https://www.postgresql.org/docs/12/arrays.html). These arrays are also supported in the Supabase Dashboard and in the JavaScript API."
        },
        {
          "type": "section",
          "title": "Create a table with an array column",
          "content": "Create a test table with a text array (an array of strings):\n\n1. Go to the [Table editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n1. Click **New Table** and create a table with the name `arraytest`.\n1. Click **Save**.\n1. Click **New Column** and create a column with the name `textarray`, type `text`, and select **Define as array**.\n1. Click **Save**.\n\n```sql\ncreate table arraytest (\n id integer not null,\n textarray text array\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Insert a record with an array value",
          "content": "1. Go to the [Table editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n1. Select the `arraytest` table.\n1. Click **Insert row** and add `[\"Harry\", \"Larry\", \"Moe\"]`.\n1. Click **Save.**\n\n```sql\nINSERT INTO arraytest (id, textarray) VALUES (1, ARRAY['Harry', 'Larry', 'Moe']);\n```\n\nInsert a record from the JavaScript client:\n\n```js\nconst { data, error } = await supabase\n .from('arraytest')\n .insert([{ id: 2, textarray: ['one', 'two', 'three', 'four'] }])\n```\n\nInsert a record from the Swift client:\n\n```swift\nstruct ArrayTest: Encodable {\n let id: Int\n let textarray: [String]\n}\n\ntry await supabase\n .from(\"arraytest\")\n .insert(\n [\n ArrayTest(\n id: 2,\n textarray: [\"one\", \"two\", \"three\", \"four\"]\n )\n ]\n )\n .execute()\n```\n\nInsert a record from the Python client:\n\n```python\nsupabase.from_('arraytest').insert(\n [\n {\n id: 2,\n textarray: [\"one\", \"two\", \"three\", \"four\"]\n }\n ]\n)\n.execute()\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "View the results",
          "content": "1. Go to the [Table editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n1. Select the `arraytest` table.\n\nYou should see:\n\n```\n| id | textarray |\n| --- | ----------------------- |\n| 1 | [\"Harry\",\"Larry\",\"Moe\"] |\n```\n\n```sql\nselect * from arraytest;\n```\n\nYou should see:\n\n```\n| id | textarray |\n| --- | ----------------------- |\n| 1 | [\"Harry\",\"Larry\",\"Moe\"] |\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Query array data",
          "content": "Postgres uses 1-based indexing (e.g., `textarray[1]` is the first item in the array).\n\nTo select the first item from the array and get the total length of the array:\n\n```js\nSELECT textarray[1], array_length(textarray, 1) FROM arraytest;\n```\n\nreturns:\n\n```\n| textarray | array_length |\n| --------- | ------------ |\n| Harry | 3 |\n```\n\nThis returns the entire array field:\n\n```js\nconst { data, error } = await supabase.from('arraytest').select('textarray')\nconsole.log(JSON.stringify(data, null, 2))\n```\n\nreturns:\n\n```json\n[\n {\n \"textarray\": [\"Harry\", \"Larry\", \"Moe\"]\n }\n]\n```\n\nThis returns the entire array field:\n\n```swift\nstruct Response: Decodable {\n let textarray: [String]\n}\n\nlet response: [Response] = try await supabase.from(\"arraytest\").select(\"textarray\").execute().value\ndump(response)\n```\n\nreturns:\n\n```\n[\n Response(\n textarray: [\"Harry\", \"Larry\", \"Moe\"],\n )\n]\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Supabase JS Client](https://github.com/supabase/supabase-js)\n- [Supabase - Get started for free](https://supabase.com)\n- [Postgres Arrays](https://www.postgresql.org/docs/15/arrays.html)",
          "level": 2
        }
      ],
      "wordCount": 440,
      "characterCount": 3159
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-beekeeper-studio",
      "identifier": "database-beekeeper-studio",
      "name": "Connecting with Beekeeper Studio",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/beekeeper-studio",
      "dateModified": "2025-06-13T12:45:11.278183",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/beekeeper-studio.mdx",
      "frontmatter": {
        "id": "Beekeeper Studio",
        "title": "Connecting with Beekeeper Studio",
        "breadcrumb": "GUI Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "[`Beekeeper Studio Community`](https://www.beekeeperstudio.io/get-community) is a free GUI tool for interacting with databases.\n\n In Beekeeper, create a new Postgres connection.\n\n ![Postgres connection](/docs/img/guides/database/connecting-to-postgres/beekeeper-studio/new-connection.png)\n\n Get your connection credentials from the [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database). You will need:\n - host\n - username\n - password\n - port\n\n Add your credentials to Beekeeper's connection form\n\n ![Credentials](/docs/img/guides/database/connecting-to-postgres/beekeeper-studio/beekeeper-credentials.png)\n\n Download your SSL certificate from the Dashboard's [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database)\n ![SSL](/docs/img/guides/database/connecting-to-postgres/beekeeper-studio/certificate.png)\n\n Add your SSL to the connection form\n ![SSL](/docs/img/guides/database/connecting-to-postgres/beekeeper-studio/certificate-beekeeper.png)\n\n Test your connection and then connect\n\n ![SSL](/docs/img/guides/database/connecting-to-postgres/beekeeper-studio/connect.png)"
        }
      ],
      "wordCount": 73,
      "characterCount": 1126
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-connecting-to-postgres",
      "identifier": "database-connecting-to-postgres",
      "name": "Connect to your database",
      "description": "Connect to Postgres from your frontend, backend, or serverless environment",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/connecting-to-postgres",
      "dateModified": "2025-06-13T12:45:11.278570",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/connecting-to-postgres.mdx",
      "frontmatter": {
        "title": "Connect to your database",
        "description": "Connect to Postgres from your frontend, backend, or serverless environment",
        "subtitle": "Supabase provides multiple methods to connect to your Postgres database, whether you’re working on the frontend, backend, or utilizing serverless functions."
      },
      "sections": [
        {
          "type": "section",
          "title": "How to connect to your Postgres databases",
          "content": "How you connect to your database depends on where you're connecting from:\n\n- For frontend applications, use the [Data API](#data-apis-and-client-libraries)\n- For Postgres clients, use a connection string\n - For single sessions (for example, database GUIs) or Postgres native commands (for example, using client applications like [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) or specifying connections for [replication](/docs/guides/database/postgres/setup-replication-external)) use the [direct connection string](#direct-connection) if your environment supports IPv6\n - For persistent clients, and support for both IPv4 and IPv6, use [Supavisor session mode](#supavisor-session-mode)\n - For temporary clients (for example, serverless or edge functions) use [Supavisor transaction mode](#supavisor-transaction-mode)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Quickstarts",
          "content": "{(data) =>\n data.items?.map((quickstart) => (\n \n div]:p-2 flex justify-center [&_p]:text-foreground-light\"\n />\n \n ))\n }\n\n {(data) =>\n data.items?.map((quickstart) => (\n \n div]:p-2 flex justify-center [&_p]:text-foreground-light\"\n />\n \n ))\n }",
          "level": 2
        },
        {
          "type": "section",
          "title": "Data APIs and client libraries",
          "content": "The Data APIs allow you to interact with your database using REST or GraphQL requests. You can use these APIs to fetch and insert data from the frontend, as long as you have [RLS](/docs/guides/database/postgres/row-level-security) enabled.\n\n- [REST](/docs/guides/api)\n- [GraphQL](/docs/guides/graphql/api)\n\nFor convenience, you can also use the [Supabase client libraries](/docs/reference), which wrap the Data APIs with a developer-friendly interface and automatically handle authentication:\n\n- [JavaScript](/docs/reference/javascript)\n- [Flutter](/docs/reference/dart)\n- [Swift](/docs/reference/swift)\n- [Python](/docs/reference/python)\n- [C#](/docs/reference/csharp)\n- [Kotlin](/docs/reference/kotlin)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Direct connection",
          "content": "The direct connection string connects directly to your Postgres instance. It is ideal for persistent servers, such as virtual machines (VMs) and long-lasting containers. Examples include AWS EC2 machines, Fly.io VMs, and DigitalOcean Droplets.\n\nDirect connections use IPv6 by default. If your environment doesn't support IPv6, use [Supavisor session mode](#supavisor-session-mode) or get the [IPv4 add-on](/docs/guides/platform/ipv4-address).\n\nThe connection string looks like this:\n\n```\npostgresql://postgres:[YOUR-PASSWORD]@db.apbkobhfnmcqqzqeeqss.supabase.co:5432/postgres\n```\n\nGet your project's direct connection string from your project dashboard by clicking [Connect](https://supabase.com/dashboard/project/_?showConnect=true).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Shared pooler",
          "content": "Every Supabase project includes a free, shared connection pooler. This is ideal for persistent servers when IPv6 is not supported.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Supavisor session mode",
          "content": "The session mode connection string connects to your Postgres instance via a proxy.\n\nThe connection string looks like this:\n\n```\npostgres://postgres.apbkobhfnmcqqzqeeqss:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres\n```\n\nGet your project's Session pooler connection string from your project dashboard by clicking [Connect](https://supabase.com/dashboard/project/_?showConnect=true).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Supavisor transaction mode",
          "content": "The transaction mode connection string connects to your Postgres instance via a proxy which serves as a connection pooler. This is ideal for serverless or edge functions, which require many transient connections.\n\nTransaction mode does not support [prepared statements](https://postgresql.org/docs/current/sql-prepare.html). To avoid errors, [turn off prepared statements](https://github.com/orgs/supabase/discussions/28239) for your connection library.\n\nThe connection string looks like this:\n\n```\npostgres://postgres.apbkobhfnmcqqzqeeqss:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:6543/postgres\n```\n\nGet your project's Transaction pooler connection string from your project dashboard by clicking [Connect](https://supabase.com/dashboard/project/_?showConnect=true).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Dedicated pooler",
          "content": "For paying customers, we provision a Dedicated Pooler ([PgBouncer](https://www.pgbouncer.org/)) that's co-located with your Postgres database. This will require you to connect with IPv6 or, if that's not an option, you can use the [IPv4 add-on](/docs/guides/platform/ipv4-address).\n\nThe Dedicated Pooler ensures best performance and latency, while using up more of your project's compute resources. If your network supports IPv6 or you have the IPv4 add-on, we encourage you to use the Dedicated Pooler over the Shared Pooler.\n\nGet your project's Dedicated pooler connection string from your project dashboard by clicking [Connect](https://supabase.com/dashboard/project/_?showConnect=true).\n\nPgBouncer always runs in Transaction mode and the current version does not support prepared statement (will be added in a few weeks).",
          "level": 2
        },
        {
          "type": "section",
          "title": "More about connection pooling",
          "content": "Connection pooling improves database performance by reusing existing connections between queries. This reduces the overhead of establishing connections and improves scalability.\n\nYou can use an application-side pooler or a server-side pooler (Supabase automatically provides one called Supavisor), depending on whether your backend is persistent or serverless.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Application-side poolers",
          "content": "Application-side poolers are built into connection libraries and API servers, such as Prisma, SQLAlchemy, and PostgREST. They maintain several active connections with Postgres or a server-side pooler, reducing the overhead of establishing connections between queries. When deploying to static architecture, such as long-standing containers or VMs, application-side poolers are satisfactory on their own.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Serverside poolers",
          "content": "Postgres connections are like a WebSocket. Once established, they are preserved until the client (application server) disconnects. A server might only make a single 10 ms query, but needlessly reserve its database connection for seconds or longer.\n\nServerside-poolers, such as Supabase's [Supavisor](https://github.com/supabase/supavisor) in transaction mode, sit between clients and the database and can be thought of as load balancers for Postgres connections.\n\nThey maintain hot connections with the database and intelligently share them with clients only when needed, maximizing the amount of queries a single connection can service. They're best used to manage queries from auto-scaling systems, such as edge and serverless functions.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Connecting with SSL",
          "content": "You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.\n\nYou can obtain your connection info and Server root certificate from your application's dashboard:\n\n![Connection Info and Certificate.](/docs/img/database/database-settings-ssl.png)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Connection management](/docs/guides/database/connection-management)\n- [Connecting with psql](/docs/guides/database/psql)\n- [Importing data into Supabase](/docs/guides/database/import-data)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Troubleshooting and Postgres connection string FAQs",
          "content": "Below are answers to common challenges and queries.",
          "level": 2
        },
        {
          "type": "section",
          "title": "What is a “connection refused” error?",
          "content": "A “Connection refused” error typically means your database isn’t reachable. Ensure your Supabase project is running, confirm your database’s connection string, check firewall settings, and validate network permissions.",
          "level": 3
        },
        {
          "type": "section",
          "title": "What is the “FATAL: Password authentication failed” error?",
          "content": "This error occurs when your credentials are incorrect. Double-check your username and password from the Supabase dashboard. If the problem persists, reset your database password from the project settings.",
          "level": 3
        },
        {
          "type": "section",
          "title": "How do you connect using IPv4?",
          "content": "Supabase’s default direct connection supports IPv6 only. To connect over IPv4, consider using the Supavisor session or transaction modes, or a connection pooler (shared or dedicated), which support both IPv4 and IPv6.",
          "level": 3
        },
        {
          "type": "section",
          "title": "How do you choose a connection method?",
          "content": "- Direct connection: Persistent backend services (IPv6 only)\n- Supavisor session mode: Persistent backend needing IPv4\n- Supavisor transaction mode: Serverless functions\n- Shared pooler: General-purpose connections with IPv4 and IPv6\n- Dedicated pooler: High-performance apps requiring dedicated resources (paid tier)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Where is the Postgres connection string in Supabase?",
          "content": "Your connection string is located in the Supabase Dashboard. Click the \"Connect\" button at the top of the page.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Can you use `psql` with a Supabase database?",
          "content": "Yes. Use the following command structure, replacing `your_connection_string` with the string from your Supabase dashboard:\n\n```\npsql \"your_connection_string\"\n```\n\nEnsure you have `psql` installed locally before running this command.",
          "level": 3
        }
      ],
      "wordCount": 1046,
      "characterCount": 8721
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-connecting-to-postgres-serverless-drivers",
      "identifier": "database-connecting-to-postgres-serverless-drivers",
      "name": "Serverless Drivers",
      "description": "Connecting to your Postgres database in serverless environments.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/connecting-to-postgres/serverless-drivers",
      "dateModified": "2025-06-13T12:45:11.278900",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/connecting-to-postgres/serverless-drivers.mdx",
      "frontmatter": {
        "id": "serverless-drivers",
        "title": "Serverless Drivers",
        "description": "Connecting to your Postgres database in serverless environments.",
        "subtitle": "Connecting to your Postgres database in serverless environments."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase provides several options for connecting to your Postgres database from serverless environments.\n\n[supabase-js](/docs/reference/javascript/introduction) is an isomorphic JavaScript client that uses the [auto-generated REST API](/docs/guides/api) and therefore works in any environment that supports HTTPS connections. This API has a built-in [connection pooler](/docs/guides/database/connecting-to-postgres#connection-pooler) and can serve thousands of simultaneous requests, and therefore is ideal for Serverless workloads."
        },
        {
          "type": "section",
          "title": "Vercel Edge Functions",
          "content": "Vercel's [Edge runtime](https://vercel.com/docs/functions/runtimes/edge-runtime) is built on top of the [V8 engine](https://v8.dev/), that provides a limited set of Web Standard APIs.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Quickstart",
          "content": "Choose one of these Vercel Deploy Templates which use our [Vercel Deploy Integration](https://vercel.com/integrations/supabase) to automatically configure your connection strings as environment variables on your Vercel project!\n\n {[\n {\n title: 'supabase-js',\n hasLightIcon: true,\n href: 'https://supabase.link/nextjs-with-supabase-starter',\n description: 'A Next.js App Router template configured with cookie-based auth using Supabase, TypeScript and Tailwind CSS.'\n },\n /* { TODO: Link the correct next.js template that uses drizzle ORM with supabase database.\n hasLightIcon: true,\n href: 'https://supabase.link/nextjs-supabase-drizzle',\n description: \"Simple Next.js template that uses Supabase as the database and Drizzle as the ORM.\",\n },*/\n {\n title: 'Kysely',\n hasLightIcon: true,\n href: 'https://supabase.link/nextjs-supabase-kysely',\n description: 'Simple Next.js template that uses Supabase as the database and Kysely as the query builder.',\n },\n /* { TODO: figure out how to get around Prisma accelerate requirement...\n title: 'Prisma',\n hasLightIcon: true,\n href: 'https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fvercel%2Fexamples%2Ftree%2Fmain%2Fstorage%2Fpostgres-prisma&project-name=postgres-prisma&repository-name=postgres-prisma&demo-title=Vercel%20Postgres%20%2B%20Prisma%20Next.js%20Starter&demo-description=Simple%20Next.js%20template%20that%20uses%20Vercel%20Postgres%20as%20the%20database%20and%20Prisma%20as%20the%20ORM.&demo-url=https%3A%2F%2Fpostgres-prisma.vercel.app%2F&demo-image=https%3A%2F%2Fpostgres-prisma.vercel.app%2Fopengraph-image.png&integration-ids=oac_VqOgBHqhEoFTPzGkPd7L0iH6',\n description: 'Simple Next.js template that uses Vercel Postgres as the database and Prisma as the ORM.',\n } */\n ].map((resource) => {\n return (\n\n {resource.description}\n\n )\n\n})}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Manual configuration",
          "content": "In your [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database), make sure `Use connection pooler` is checked and `Transaction` mode is selected, then copy the URI and save it as the `POSTGRES_URL` environment variable. Remember to replace the password placeholder with your actual database password and add the following suffix `?workaround=supabase-pooler.vercel`.\n\n```txt .env.local\nPOSTGRES_URL=\"postgres://postgres.cfcxynqnhdybqtbhjemm:[YOUR-PASSWORD]@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres?workaround=supabase-pooler.vercel\"\n```\n\n```ts lib/drizzle.ts\nimport { pgTable, serial, text, timestamp, uniqueIndex } from 'drizzle-orm/pg-core'\nimport { InferSelectModel, InferInsertModel } from 'drizzle-orm'\nimport { sql } from '@vercel/postgres'\nimport { drizzle } from 'drizzle-orm/vercel-postgres'\n\nexport const UsersTable = pgTable(\n 'users',\n {\n id: serial('id').primaryKey(),\n name: text('name').notNull(),\n email: text('email').notNull(),\n image: text('image').notNull(),\n createdAt: timestamp('createdAt').defaultNow().notNull(),\n },\n (users) => {\n return {\n uniqueIdx: uniqueIndex('unique_idx').on(users.email),\n }\n }\n)\n\nexport type User = InferSelectModel\nexport type NewUser = InferInsertModel\n\n// Connect to Vercel Postgres\nexport const db = drizzle(sql)\n```\n\n```ts lib/kysely.ts\nimport { Generated, ColumnType } from 'kysely'\nimport { createKysely } from '@vercel/postgres-kysely'\n\ninterface UserTable {\n // Columns that are generated by the database should be marked\n // using the `Generated` type. This way they are automatically\n // made optional in inserts and updates.\n id: Generated\n name: string\n email: string\n image: string\n\n // You can specify a different type for each operation (select, insert and\n // update) using the `ColumnType`\n // wrapper. Here we define a column `createdAt` that is selected as\n // a `Date`, can optionally be provided as a `string` in inserts and\n // can never be updated:\n createdAt: ColumnType\n}\n\n// Keys of this interface are table names.\nexport interface Database {\n users: UserTable\n}\n\nexport const db = createKysely()\nexport { sql } from 'kysely'\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Cloudflare Workers",
          "content": "Cloudflare's Workers runtime also uses the [V8 engine](https://v8.dev/) but provides polyfills for a subset of Node.js APIs and [TCP Sockets API](https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets/), giving you a couple of options:\n\n- [supabase-js](https://developers.cloudflare.com/workers/databases/native-integrations/supabase/)\n- [Postgres.js](https://github.com/porsager/postgres?tab=readme-ov-file#cloudflare-workers-support)\n- [node-postgres](https://developers.cloudflare.com/workers/tutorials/postgres/)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Supabase Edge Functions",
          "content": "Supabase Edge Functions uses the [Deno runtime](https://deno.com/) which has native support for TCP connections allowing you to choose your favorite client:\n\n- [supabase-js](/docs/guides/functions/connect-to-postgres#using-supabase-js)\n- [Deno Postgres driver](/docs/guides/functions/connect-to-postgres#using-a-postgres-client)\n- [Postgres.js](https://github.com/porsager/postgres)\n- [Drizzle](/docs/guides/functions/connect-to-postgres#using-drizzle)",
          "level": 2
        }
      ],
      "wordCount": 571,
      "characterCount": 5788
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-connection-management",
      "identifier": "database-connection-management",
      "name": "Connection management",
      "description": "Managing connections",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/connection-management",
      "dateModified": "2025-06-13T12:45:11.279127",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/connection-management.mdx",
      "frontmatter": {
        "id": "connection-management",
        "title": "Connection management",
        "description": "Managing connections",
        "subtitle": "Using your connections resourcefully"
      },
      "sections": [
        {
          "type": "section",
          "title": "Connections",
          "content": "Every [Compute Add-On](/docs/guides/platform/compute-add-ons) has a pre-configured direct connection count and Supavisor pool size. This guide discusses ways to observe and manage them resourcefully.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Configuring Supavisor's pool size",
          "content": "You can change how many database connections Supavisor can manage by altering the pool size in the \"Connection pooling configuration\" section of the [Database Settings](/dashboard/project/_/settings/database):\n\n![Connection Info and Certificate.](/docs/img/database/pool-size.png)\n\nThe general rule is that if you are heavily using the PostgREST database API, you should be conscientious about raising your pool size past 40%. Otherwise, you can commit 80% to the pool. This leaves adequate room for the Authentication server and other utilities.\n\nThese numbers are generalizations and depends on other Supabase products that you use and the extent of their usage. The actual values depend on your concurrent peak connection usage. For instance, if you were only using 80 connections in a week period and your database max connections is set to 500, then realistically you could allocate the difference of 420 (minus a reasonable buffer) to service more demand.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Capturing historical usage",
          "content": "Supabase offers a Grafana Dashboard that records and visualizes over 200 project metrics, including connections. For setup instructions, check the [metrics docs](/docs/guides/platform/metrics).\n\nIts \"Client Connections\" graph displays connections for both Supavisor and Postgres\n![client connection graph](/docs/img/database/grafana-connections.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Observing live connections",
          "content": "`pg_stat_activity` is a special view that keeps track of processes being run by your database, including live connections. It's particularly useful for determining if idle clients are hogging connection slots.\n\nQuery to get all live connections:\n\n```sql\nSELECT\n pg_stat_activity.pid as connection_id,\n ssl,\n datname as database,\n usename as connected_role,\n application_name,\n client_addr as IP,\n query,\n query_start,\n state,\n backend_start\nFROM pg_stat_ssl\nJOIN pg_stat_activity\nON pg_stat_ssl.pid = pg_stat_activity.pid;\n```\n\nInterpreting the query:\n\n| Column | Description |\n| ------------------ | --------------------------------------------------- |\n| `connection_id` | connection id |\n| `ssl` | Indicates if SSL is in use |\n| `database` | Name of the connected database (usually `postgres`) |\n| `usename` | Role of the connected user |\n| `application_name` | Name of the connecting application |\n| `client_addr` | IP address of the connecting server |\n| `query` | Last query executed by the connection |\n| `query_start` | Time when the last query was executed |\n| `state` | Querying state: active or idle |\n| `backend_start` | Timestamp of the connection's establishment |\n\nThe username can be used to identify the source:\n\n| Role | API/Tool |\n| ---------------------------- | ------------------------------------------------------------------------- |\n| `supabase_admin` | Used by Supabase for monitoring and by Realtime |\n| `authenticator` | Data API (PostgREST) |\n| `supabase_auth_admin` | Auth |\n| `supabase_storage_admin` | Storage |\n| `supabase_replication_admin` | Synchronizes Read Replicas |\n| `postgres` | Supabase Dashboard and External Tools (e.g., Prisma, SQLAlchemy, PSQL...) |\n| Custom roles defined by user | External Tools (e.g., Prisma, SQLAlchemy, PSQL...) |",
          "level": 3
        }
      ],
      "wordCount": 472,
      "characterCount": 3444
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-custom-postgres-config",
      "identifier": "database-custom-postgres-config",
      "name": "Customizing Postgres configs",
      "description": "Configuring Postgres for your Supabase project.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/custom-postgres-config",
      "dateModified": "2025-06-13T12:45:11.279494",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/custom-postgres-config.mdx",
      "frontmatter": {
        "id": "customizing-postgres-configs",
        "title": "Customizing Postgres configs",
        "description": "Configuring Postgres for your Supabase project."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Each Supabase project is a pre-configured Postgres cluster. You can override some configuration settings to suit your needs. This is an advanced topic, and we don't recommend touching these settings unless it is necessary.\n\nCustomizing Postgres configurations provides _advanced_ control over your database, but inappropriate settings can lead to severe performance degradation or project instability."
        },
        {
          "type": "section",
          "title": "Viewing settings",
          "content": "To list all Postgres settings and their descriptions, run:\n\n```sql\nselect * from pg_settings;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "User-context settings",
          "content": "The [`pg_settings`](https://www.postgresql.org/docs/current/view-pg-settings.html) table's `context` column specifies the requirements for changing a setting. By default, those with a `user` context can be changed at the `role` or `database` level with [SQL](https://supabase.com/dashboard/project/_/sql/).\n\nTo list all user-context settings, run:\n\n```sql\nselect * from pg_settings where context = 'user';\n```\n\nAs an example, the `statement_timeout` setting can be altered:\n\n```sql\nalter database \"postgres\" set \"statement_timeout\" TO '60s';\n```\n\nTo verify the change, execute:\n\n```sql\nshow \"statement_timeout\";\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Superuser settings",
          "content": "Some settings can only be modified by a superuser. Supabase pre-enables the [`supautils` extension](https://supabase.com/blog/roles-postgres-hooks#setting-up-the-supautils-extension), which allows the `postgres` role to retain certain superuser privileges. It enables modification of the below reserved configurations at the `role` level:\n\n| Setting | Description |\n| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `auto_explain.log_min_duration` | Logs query plans taking longer than this duration. |\n| `auto_explain.log_nested_statements` | Log nested statements' plans. |\n| `log_min_messages` | Minimum severity level of messages to log. |\n| `pg_net.ttl` | Sets how long the [pg_net extension](/docs/guides/database/extensions/pg_net) saves responses |\n| `pg_net.batch_size` | Sets how many requests the [pg_net extension](/docs/guides/database/extensions/pg_net) can make per second |\n| `pgaudit.*` | Configures the [PGAudit extension](https://supabase.com/docs/guides/database/extensions/pgaudit). The `log_parameter` is still restricted to protect secrets |\n| `pgrst.*` | [`PostgREST` settings](https://docs.postgrest.org/en/stable/references/configuration.html#db-aggregates-enabled) |\n| `plan_filter.*` | Configures the [pg_plan_filter extension](https://supabase.com/docs/guides/database/extensions/pg_plan_filter) |\n| `session_replication_role` | Sets the session's behavior for triggers and rewrite rules. |\n| `track_io_timing` | Collects timing statistics for database I/O activity. |\n\nFor example, to enable `log_nested_statements` for the `postgres` role, execute:\n\n```sql\nalter role \"postgres\" set \"auto_explain.log_nested_statements\" to 'on';\n```\n\nTo view the change:\n\n```sql\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere rolname = 'postgres';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "CLI configurable settings",
          "content": "While many Postgres parameters are configurable directly, some configurations can be changed with the Supabase CLI at the [`system`](https://www.postgresql.org/docs/current/config-setting.html#CONFIG-SETTING-SQL) level.\n\nCLI changes permanently overwrite default settings, so `reset all` and `set to default` commands won't revert to the original values.\n\nIn order to overwrite the default settings, you must have `Owner` or `Administrator` privileges within your organizations.",
          "level": 3
        },
        {
          "type": "section",
          "title": "CLI supported parameters",
          "content": "If a setting you need is not yet configurable, [share your use case with us](https://supabase.com/dashboard/support/new)! Let us know what setting you'd like to control, and we'll consider adding support in future updates.\n\nThe following parameters are available for overrides:\n\n1. [effective_cache_size](https://www.postgresql.org/docs/current/runtime-config-query.html#GUC-EFFECTIVE-CACHE-SIZE)\n1. [logical_decoding_work_mem](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-LOGICAL-DECODING-WORK-MEM) (CLI only)\n1. [maintenance_work_mem](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM)\n1. [max_connections](https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-MAX-CONNECTIONS) (CLI only)\n1. [max_locks_per_transaction](https://www.postgresql.org/docs/current/runtime-config-locks.html#GUC-MAX-LOCKS-PER-TRANSACTION) (CLI only)\n1. [max_parallel_maintenance_workers](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-PARALLEL-MAINTENANCE-WORKERS)\n1. [max_parallel_workers_per_gather](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-PARALLEL-WORKERS-PER-GATHER)\n1. [max_parallel_workers](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-PARALLEL-WORKERS)\n1. [max_replication_slots](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-REPLICATION-SLOTS) (CLI only)\n1. [max_slot_wal_keep_size](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-SLOT-WAL-KEEP-SIZE) (CLI only)\n1. [max_standby_archive_delay](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-STANDBY-ARCHIVE-DELAY) (CLI only)\n1. [max_standby_streaming_delay](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-STANDBY-STREAMING-DELAY) (CLI only)\n1. [max_wal_size](https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-MAX-WAL-SIZE) (CLI only)\n1. [max_wal_senders](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-MAX-WAL-SENDERS) (CLI only)\n1. [max_worker_processes](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-WORKER-PROCESSES) (CLI only)\n1. [session_replication_role](https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-SESSION-REPLICATION-ROLE)\n1. [shared_buffers](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-SHARED-BUFFERS) (CLI only)\n1. [statement_timeout](https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-STATEMENT-TIMEOUT)\n1. [track_activity_query_size](https://www.postgresql.org/docs/current/runtime-config-statistics.html#GUC-TRACK-ACTIVITY-QUERY-SIZE)\n1. [track_commit_timestamp](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-TRACK-COMMIT-TIMESTAMP)\n1. [wal_keep_size](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-WAL-KEEP-SIZE) (CLI only)\n1. [wal_sender_timeout](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-WAL-SENDER-TIMEOUT) (CLI only)\n1. [work_mem](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM)",
          "level": 4
        },
        {
          "type": "section",
          "title": "Managing Postgres configuration with the CLI",
          "content": "To start:\n\n1. [Install](/docs/guides/resources/supabase-cli) Supabase CLI 1.69.0+.\n1. [Log in](/docs/guides/cli/local-development#log-in-to-the-supabase-cli) to your Supabase account using the CLI.\n\nTo update Postgres configurations, use the [`postgres config`](/docs/reference/cli/supabase-postgres-config) command:\n\n```bash\nsupabase --experimental \\\n--project-ref \\\npostgres-config update --config shared_buffers=250MB\n```\n\nBy default, the CLI will merge any provided config overrides with any existing ones. The `--replace-existing-overrides` flag can be used to instead force all existing overrides to be replaced with the ones being provided:\n\n```bash\nsupabase --experimental \\\n--project-ref \\\npostgres-config update --config max_parallel_workers=3 \\\n--replace-existing-overrides\n```\n\nTo delete specific configuration overrides, use the `postgres-config delete` command:\n\n```bash\nsupabase --experimental \\\n--project-ref \\\npostgres-config delete --config shared_buffers,work_mem\n```\n\nBy default, changing the configuration, whether by updating or deleting, causes the database and all associated read replicas to restart. You can use the `--no-restart` flag to prevent this behavior, and attempt to reload the updated configuration without a restart. Refer to the Postgres documentation to determine if a given parameter can be reloaded without a restart.\n\nPostgres requires several parameters to be synchronized between the Primary cluster and [Read Replicas](/docs/guides/platform/read-replicas).\n\nBy default, Supabase ensures that this propagation is executed correctly. However, if the `--no-restart` behavior is used in conjunction with parameters that cannot be reloaded without a restart, the user is responsible for ensuring that both the primaries and the read replicas get restarted in a timely manner to ensure a stable running state. Leaving the configuration updated, but not utilized (via a restart) in such a case can result in read replica failure if the primary, or a read replica, restarts in isolation (e.g. due to an out-of-memory event, or hardware failure).\n\n```bash\nsupabase --experimental \\\n--project-ref \\\npostgres-config delete --config shared_buffers --no-restart\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Resetting to default config",
          "content": "To reset a setting to its default value at the database level:\n\n```sql\n-- reset a single setting at the database level\nalter database \"postgres\" set \"\" to default;\n\n-- reset all settings at the database level\nalter database \"postgres\" reset all;\n```\n\nFor `role` level configurations, you can run:\n\n```sql\nalter role \"\" set \"\" to default;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Considerations",
          "content": "1. Changes through the CLI might restart the database causing momentary disruption to existing database connections; in most cases this should not take more than a few seconds. However, you can use the --no-restart flag to bypass the restart and keep the connections intact. Keep in mind that this depends on the specific configuration changes you're making. if the change requires a restart, using the --no-restart flag will prevent the restart but you won't see those changes take effect until a restart is manually triggered. Additionally, some parameters are required to be the same on Primary and Read Replicas; not restarting in these cases can result in read replica failure if the Primary/Read Replicas restart in isolation.\n1. Custom Postgres Config will always override the default optimizations generated by Supabase. When changing compute add-ons, you should also review and update your custom Postgres Config to ensure they remain compatible and effective with the updated compute.\n1. Some parameters (e.g. `wal_keep_size`) can increase disk utilization, triggering disk expansion, which in turn can lead to [increases in your bill](/docs/guides/platform/compute-add-ons#disk-io).",
          "level": 3
        }
      ],
      "wordCount": 1054,
      "characterCount": 10761
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-dbeaver",
      "identifier": "database-dbeaver",
      "name": "Connecting with DBeaver",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/dbeaver",
      "dateModified": "2025-06-13T12:45:11.279688",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/dbeaver.mdx",
      "frontmatter": {
        "id": "dbeaver",
        "title": "Connecting with DBeaver",
        "breadcrumb": "GUI Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "If you do not have DBeaver, you can download it from its [website](https://dbeaver.io/download/).\n\n Create a new database connection\n\n ![new database connection](/docs/img/guides/database/connecting-to-postgres/dbeaver/new_database_connection.png)\n\n ![Selection Menu](/docs/img/guides/database/connecting-to-postgres/dbeaver/select_postgres.png)\n\n On your project dashboard, click [Connect](https://supabase.com/dashboard/project/_?showConnect=true), note your session pooler's:\n - host\n - username\n\n You will also need your database's password. If you forgot it, you can generate a new one in the settings.\n \n If you're in an [IPv6 environment](https://github.com/orgs/supabase/discussions/27034) or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.\n\n ![database credentials](/docs/img/guides/database/connecting-to-postgres/dbeaver/session_mode.png)\n\n In DBeaver's Main menu, add your host, username, and password\n\n ![filling out form](/docs/img/guides/database/connecting-to-postgres/dbeaver/filling_credentials.png)\n\n In the [Database Settings](https://supabase.com/dashboard/project/_/settings/database), download your SSL certificate.\n\n ![filling out form](/docs/img/guides/database/connecting-to-postgres/dbeaver/certificate.png)\n\n In DBeaver's SSL tab, add your SSL certificate\n\n ![filling out form](/docs/img/guides/database/connecting-to-postgres/dbeaver/ssl_tab.png)\n\n Test your connection and then click finish. You should now be able to interact with your database with DBeaver\n\n ![connected dashboard](/docs/img/guides/database/connecting-to-postgres/dbeaver/finished.png)"
        }
      ],
      "wordCount": 139,
      "characterCount": 1637
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-debugging-performance",
      "identifier": "database-debugging-performance",
      "name": "Debugging performance issues",
      "description": "Debug slow-running queries using the Postgres execution planner.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/debugging-performance",
      "dateModified": "2025-06-13T12:45:11.279865",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/debugging-performance.mdx",
      "frontmatter": {
        "id": "debugging-performance",
        "title": "Debugging performance issues",
        "description": "Debug slow-running queries using the Postgres execution planner.",
        "subtitle": "Debug slow-running queries using the Postgres execution planner."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "`explain()` is a method that provides the Postgres `EXPLAIN` execution plan of a query. It is a powerful tool for debugging slow queries and understanding how Postgres will execute a given query. This feature is applicable to any query, including those made through `rpc()` or write operations."
        },
        {
          "type": "section",
          "title": "Enabling `explain()`",
          "content": "`explain()` is disabled by default to protect sensitive information about your database structure and operations. We recommend using `explain()` in a non-production environment. Run the following SQL to enable `explain()`:\n\n{/* prettier-ignore */}\n```sql\n-- enable explain\nalter role authenticator\nset pgrst.db_plan_enabled to 'true';\n\n-- reload the config\nnotify pgrst, 'reload config';\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using `explain()`",
          "content": "To get the execution plan of a query, you can chain the `explain()` method to a Supabase query:\n\n{/* prettier-ignore */}\n```ts\nconst { data, error } = await supabase\n .from('instruments')\n .select()\n .explain()\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example data",
          "content": "To illustrate, consider the following setup of a `instruments` table:\n\n{/* prettier-ignore */}\n```sql\ncreate table instruments (\n id int8 primary key,\n name text\n);\n\ninsert into books\n (id, name)\nvalues\n (1, 'violin'),\n (2, 'viola'),\n (3, 'cello');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Expected response",
          "content": "The response would typically look like this:\n\n{/* prettier-ignore */}\n```markdown\nAggregate (cost=33.34..33.36 rows=1 width=112)\n -> Limit (cost=0.00..18.33 rows=1000 width=40)\n -> Seq Scan on instruments (cost=0.00..22.00 rows=1200 width=40)\n```\n\nBy default, the execution plan is returned in TEXT format. However, you can also retrieve it as JSON by specifying the `format` parameter.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Production use with pre-request protection",
          "content": "If you need to enable `explain()` in a production environment, ensure you protect your database by restricting access to the `explain()` feature. You can do so by using a pre-request function that filters requests based on the IP address:\n\n{/* prettier-ignore */}\n```sql\ncreate or replace function filter_plan_requests()\nreturns void as $$\ndeclare\n headers json := current_setting('request.headers', true)::json;\n client_ip text := coalesce(headers->>'cf-connecting-ip', '');\n accept text := coalesce(headers->>'accept', '');\n your_ip text := '123.123.123.123'; -- replace this with your IP\nbegin\n if accept like 'application/vnd.pgrst.plan%' and client_ip != your_ip then\n raise insufficient_privilege using\n message = 'Not allowed to use application/vnd.pgrst.plan';\n end if;\nend; $$ language plpgsql;\nalter role authenticator set pgrst.db_pre_request to 'filter_plan_requests';\nnotify pgrst, 'reload config';\n```\n\nReplace `'123.123.123.123'` with your actual IP address.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Disabling explain",
          "content": "To disable the `explain()` method after use, execute the following SQL commands:\n\n{/* prettier-ignore */}\n```sql\n-- disable explain\nalter role authenticator\nset pgrst.db_plan_enabled to 'false';\n\n-- if you used the above pre-request\nalter role authenticator\nset pgrst.db_pre_request to '';\n\n-- reload the config\nnotify pgrst, 'reload config';\n```",
          "level": 2
        }
      ],
      "wordCount": 417,
      "characterCount": 3025
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-drizzle",
      "identifier": "database-drizzle",
      "name": "Drizzle",
      "description": "Drizzle Quickstart",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/drizzle",
      "dateModified": "2025-06-13T12:45:11.280326",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/drizzle.mdx",
      "frontmatter": {
        "id": "drizzle",
        "title": "Drizzle",
        "description": "Drizzle Quickstart",
        "breadcrumb": "ORM Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "section",
          "title": "Connecting with Drizzle",
          "content": "[Drizzle ORM](https://github.com/drizzle-team/drizzle-orm) is a TypeScript ORM for SQL databases designed with maximum type safety in mind. You can use their ORM to connect to your database.\n\nIf you plan on solely using Drizzle instead of the Supabase Data API (PostgREST), you can turn off the latter in the [API Settings](https://supabase.com/dashboard/project/_/settings/api).\n\n Install Drizzle and related dependencies.\n\n ```shell\n npm i drizzle-orm postgres\n npm i -D drizzle-kit\n ```\n\n Create a `schema.ts` file and define your models.\n\n ```ts schema.ts\n import { pgTable, serial, text, varchar } from \"drizzle-orm/pg-core\";\n\n export const users = pgTable('users', {\n id: serial('id').primaryKey(),\n fullName: text('full_name'),\n phone: varchar('phone', { length: 256 }),\n });\n ```\n\n Connect to your database using the Connection Pooler.\n\n In your [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database), make sure `Use connection pooler` is checked, then copy the URI and save it as the `DATABASE_URL` environment variable. Remember to replace the password placeholder with your actual database password.\n\n ```ts db.ts\n import 'dotenv/config'\n\n import { drizzle } from 'drizzle-orm/postgres-js'\n import postgres from 'postgres'\n\n const connectionString = process.env.DATABASE_URL\n\n // Disable prefetch as it is not supported for \"Transaction\" pool mode\n export const client = postgres(connectionString, { prepare: false })\n export const db = drizzle(client);\n ```",
          "level": 3
        }
      ],
      "wordCount": 193,
      "characterCount": 1525
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions",
      "identifier": "database-extensions",
      "name": "Postgres Extensions Overview",
      "description": "Using Postgres extensions.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions",
      "dateModified": "2025-06-13T12:45:11.280444",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions.mdx",
      "frontmatter": {
        "id": "extensions",
        "title": "Postgres Extensions Overview",
        "description": "Using Postgres extensions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Extensions are exactly as they sound - they \"extend\" the database with functionality which isn't part of the Postgres core.\nSupabase has pre-installed some of the most useful open source extensions."
        },
        {
          "type": "section",
          "title": "Enable and disable extensions",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click **Extensions** in the sidebar.\n3. Enable or disable an extension.\n\n```sql\n -- Example: enable the \"pgtap\" extension and ensure it is installed\ncreate extension pgtap\nwith\n schema extensions;\n\n-- Example: disable the \"pgtap\" extension\ndrop\n extension pgtap;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension call `drop extension`.\n\nMost extensions are installed under the `extensions` schema, which is accessible to public by default. To avoid namespace pollution, we do not recommend creating other entities in the `extensions` schema.\n\nIf you need to restrict user access to tables managed by extensions, we recommend creating a separate schema for installing that specific extension.\n\nSome extensions can only be created under a specific schema, for example, `postgis_tiger_geocoder` extension creates a schema named `tiger`. Before enabling such extensions, make sure you have not created a conflicting schema with the same name.\n\nIn addition to the pre-configured extensions, you can also install your own SQL extensions directly in the database using Supabase's SQL editor. The SQL code for the extensions, including plpgsql extensions, can be added through the SQL editor.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Upgrade extensions",
          "content": "If a new version of an extension becomes available on Supabase, you need to initiate a software upgrade in the [Infrastructure Settings](https://supabase.com/dashboard/project/_/settings/infrastructure) to access it. Software upgrades can also be initiated by restarting your server in the [General Settings](https://supabase.com/dashboard/project/_/settings/general).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Full list of extensions",
          "content": "Supabase is pre-configured with over 50 extensions. You can also install your own SQL extensions directly in the database through our SQL editor.",
          "level": 3
        }
      ],
      "wordCount": 305,
      "characterCount": 2174
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-http",
      "identifier": "database-extensions-http",
      "name": "http: RESTful Client",
      "description": "An HTTP Client for PostgreSQL Functions.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/http",
      "dateModified": "2025-06-13T12:45:11.280558",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/http.mdx",
      "frontmatter": {
        "id": "http",
        "title": "http: RESTful Client",
        "description": "An HTTP Client for PostgreSQL Functions.",
        "video": "https://www.youtube.com/v/rARgrELRCwY"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The `http` extension allows you to call RESTful endpoints within Postgres."
        },
        {
          "type": "section",
          "title": "Overview",
          "content": "Let's cover some basic concepts:\n\n- REST: stands for REpresentational State Transfer. It's a way to request data from external services.\n- RESTful APIs are servers which accept HTTP \"calls\". The calls are typically:\n - `GET` − Read only access to a resource.\n - `POST` − Creates a new resource.\n - `DELETE` − Removes a resource.\n - `PUT` − Updates an existing resource or creates a new resource.\n\nYou can use the `http` extension to make these network requests from Postgres.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `http` and enable the extension.\n\n```sql\n-- Example: enable the \"http\" extension\ncreate extension http with schema extensions;\n\n-- Example: disable the \"http\" extension\ndrop extension if exists http;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension, call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Available functions",
          "content": "While the main usage is `http('http_request')`, there are 5 wrapper functions for specific functionality:\n\n- `http_get()`\n- `http_post()`\n- `http_put()`\n- `http_delete()`\n- `http_head()`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Returned values",
          "content": "A successful call to a web URL from the `http` extension returns a record with the following fields:\n\n- `status`: integer\n- `content_type`: character varying\n- `headers`: http_header[]\n- `content`: character varying. Typically you would want to cast this to `jsonb` using the format `content::jsonb`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Simple `GET` example",
          "content": "```sql\nselect\n \"status\", \"content\"::jsonb\nfrom\n http_get('https://jsonplaceholder.typicode.com/todos/1');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Simple `POST` example",
          "content": "```sql\nselect\n \"status\", \"content\"::jsonb\nfrom\n http_post(\n 'https://jsonplaceholder.typicode.com/posts',\n '{ \"title\": \"foo\", \"body\": \"bar\", \"userId\": 1 }',\n 'application/json'\n );\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`http` GitHub Repository](https://github.com/pramsey/pgsql-http)",
          "level": 2
        }
      ],
      "wordCount": 311,
      "characterCount": 2245
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-hypopg",
      "identifier": "database-extensions-hypopg",
      "name": "HypoPG: Hypothetical indexes",
      "description": "Quickly check if an index can be used without creating it.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/hypopg",
      "dateModified": "2025-06-13T12:45:11.280805",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/hypopg.mdx",
      "frontmatter": {
        "id": "hypopg",
        "title": "HypoPG: Hypothetical indexes",
        "description": "Quickly check if an index can be used without creating it."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "`HypoPG` is Postgres extension for creating hypothetical/virtual indexes. HypoPG allows users to rapidly create hypothetical/virtual indexes that have no resource cost (CPU, disk, memory) that are visible to the Postgres query planner.\n\nThe motivation for HypoPG is to allow users to quickly search for an index to improve a slow query without consuming server resources or waiting for them to build."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `hypopg` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"hypopg\" extension\ncreate extension hypopg with schema extensions;\n\n-- Disable the \"hypopg\" extension\ndrop extension if exists hypopg;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Speeding up a query",
          "content": "Given the following table and a simple query to select from the table by `id`:\n\n{/* prettier-ignore */}\n```sql\ncreate table account (\n id int,\n address text\n);\n\ninsert into account(id, address)\nselect\n id,\n id || ' main street'\nfrom\n generate_series(1, 10000) id;\n```\n\nWe can generate an explain plan for a description of how the Postgres query planner\nintends to execute the query.\n\n{/* prettier-ignore */}\n```sql\nexplain select * from account where id=1;\n\n QUERY PLAN\n-------------------------------------------------------\n Seq Scan on account (cost=0.00..180.00 rows=1 width=13)\n Filter: (id = 1)\n(2 rows)\n```\n\nUsing HypoPG, we can create a hypothetical index on the `account(id)` column to check if it would be useful to the query planner and then re-run the explain plan.\n\nNote that the virtual indexes created by HypoPG are only visible in the Postgres connection that they were created in. Supabase connects to Postgres through a connection pooler so the `hypopg_create_index` statement and the `explain` statement should be executed in a single query.\n\n{/* prettier-ignore */}\n```sql\nselect * from hypopg_create_index('create index on account(id)');\n\nexplain select * from account where id=1;\n\n QUERY PLAN\n------------------------------------------------------------------------------------\n Index Scan using btree_account_id on hypo (cost=0.29..8.30 rows=1 width=13)\n Index Cond: (id = 1)\n(2 rows)\n```\n\nThe query plan has changed from a `Seq Scan` to an `Index Scan` using the newly created virtual index, so we may choose to create a real version of the index to improve performance on the target query:\n\n{/* prettier-ignore */}\n```sql\ncreate index on account(id);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Functions",
          "content": "- [`hypo_create_index(text)`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#create-a-hypothetical-index): A function to create a hypothetical index.\n- [`hypopg_list_indexes`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A View that lists all hypothetical indexes that have been created.\n- [`hypopg()`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A function that lists all hypothetical indexes that have been created with the same format as `pg_index`.\n- [`hypopg_get_index_def(oid)`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A function to display the `create index` statement that would create the index.\n- [`hypopg_get_relation_size(oid)`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A function to estimate how large a hypothetical index would be.\n- [`hypopg_drop_index(oid)`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A function to remove a given hypothetical index by `oid`.\n- [`hypopg_reset()`](https://hypopg.readthedocs.io/en/rel1_stable/usage.html#manipulate-hypothetical-indexes): A function to remove all hypothetical indexes.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [HypoPG documentation](https://hypopg.readthedocs.io/en/rel1_stable/)",
          "level": 2
        }
      ],
      "wordCount": 511,
      "characterCount": 4183
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-index_advisor",
      "identifier": "database-extensions-index_advisor",
      "name": "index_advisor: query optimization",
      "description": "Automatically optimize SQL queries",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/index_advisor",
      "dateModified": "2025-06-13T12:45:11.281005",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/index_advisor.mdx",
      "frontmatter": {
        "id": "index-advisor",
        "title": "index_advisor: query optimization",
        "description": "Automatically optimize SQL queries"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[Index advisor](https://github.com/supabase/index_advisor) is a Postgres extension for recommending indexes to improve query performance.\n\nFeatures:\n\n- Supports generic parameters e.g. `$1`, `$2`\n- Supports materialized views\n- Identifies tables/columns obfuscated by views\n- Skips duplicate indexes\n\n`index_advisor` is accessible directly through Supabase Studio by navigating to the [Query Performance Report](/dashboard/project/_/advisors/query-performance) and selecting a query and then the \"indexes\" tab.\n\n![Supabase Studio index_advisor integration.](/docs/img/index_advisor_studio.png)\n\nAlternatively, you can use index_advisor directly via SQL.\n\nFor example:\n\n```sql\nselect\n *\nfrom\n index_advisor('select book.id from book where title = $1');\n\n startup_cost_before | startup_cost_after | total_cost_before | total_cost_after | index_statements | errors\n---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------\n 0.00 | 1.17 | 25.88 | 6.40 | {\"CREATE INDEX ON public.book USING btree (title)\"},| {}\n(1 row)\n```"
        },
        {
          "type": "section",
          "title": "Installation",
          "content": "To get started, enable index_advisor by running\n\n```sql\ncreate extension index_advisor;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "API",
          "content": "Index advisor exposes a single function `index_advisor(query text)` that accepts a query and searches for a set of SQL DDL `create index` statements that improve the query's execution time.\n\nThe function's signature is:\n\n```sql\nindex_advisor(query text)\nreturns\n table (\n startup_cost_before jsonb,\n startup_cost_after jsonb,\n total_cost_before jsonb,\n total_cost_after jsonb,\n index_statements text[],\n errors text[]\n )\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "As a minimal example, the `index_advisor` function can be given a single table query with a filter on an unindexed column.\n\n```sql\ncreate extension if not exists index_advisor cascade;\n\ncreate table book(\n id int primary key,\n title text not null\n);\n\nselect\n *\nfrom\n index_advisor('select book.id from book where title = $1');\n\n startup_cost_before | startup_cost_after | total_cost_before | total_cost_after | index_statements | errors\n---------------------+--------------------+-------------------+------------------+-----------------------------------------------------+--------\n 0.00 | 1.17 | 25.88 | 6.40 | {\"CREATE INDEX ON public.book USING btree (title)\"},| {}\n(1 row)\n```\n\nand will return a row recommending an index on the unindexed column.\n\nMore complex queries may generate additional suggested indexes:\n\n```sql\ncreate extension if not exists index_advisor cascade;\n\ncreate table author(\n id serial primary key,\n name text not null\n);\n\ncreate table publisher(\n id serial primary key,\n name text not null,\n corporate_address text\n);\n\ncreate table book(\n id serial primary key,\n author_id int not null references author(id),\n publisher_id int not null references publisher(id),\n title text\n);\n\ncreate table review(\n id serial primary key,\n book_id int references book(id),\n body text not null\n);\n\nselect\n *\nfrom\n index_advisor('\n select\n book.id,\n book.title,\n publisher.name as publisher_name,\n author.name as author_name,\n review.body review_body\n from\n book\n join publisher\n on book.publisher_id = publisher.id\n join author\n on book.author_id = author.id\n join review\n on book.id = review.book_id\n where\n author.id = $1\n and publisher.id = $2\n ');\n\n startup_cost_before | startup_cost_after | total_cost_before | total_cost_after | index_statements | errors\n---------------------+--------------------+-------------------+------------------+-----------------------------------------------------------+--------\n 27.26 | 12.77 | 68.48 | 42.37 | {\"CREATE INDEX ON public.book USING btree (author_id)\", | {}\n \"CREATE INDEX ON public.book USING btree (publisher_id)\",\n \"CREATE INDEX ON public.review USING btree (book_id)\"}\n(3 rows)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Limitations",
          "content": "- index_advisor will only recommend single column, B-tree indexes. More complex indexes will be supported in future releases.\n- when a generic argument's type is not discernible from context, an error is returned in the `errors` field. To resolve those errors, add explicit type casting to the argument. e.g. `$1::int`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [`index_advisor`](https://github.com/supabase/index_advisor) repo",
          "level": 2
        }
      ],
      "wordCount": 508,
      "characterCount": 4224
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_cron",
      "identifier": "database-extensions-pg_cron",
      "name": "pg_cron: Schedule Recurring Jobs with Cron Syntax in Postgres",
      "description": "pg_cron: schedule recurring Jobs with cron syntax in Postgres.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_cron",
      "dateModified": "2025-06-13T12:45:11.281066",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_cron.mdx",
      "frontmatter": {
        "id": "pg_cron",
        "title": "pg_cron: Schedule Recurring Jobs with Cron Syntax in Postgres",
        "description": "pg_cron: schedule recurring Jobs with cron syntax in Postgres."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "See the [Supabase Cron docs](/docs/guides/cron)."
        }
      ],
      "wordCount": 5,
      "characterCount": 48
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_graphql",
      "identifier": "database-extensions-pg_graphql",
      "name": "pg_graphql: GraphQL for PostgreSQL",
      "description": "A GraphQL Interface for PostgreSQL",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_graphql",
      "dateModified": "2025-06-13T12:45:11.281169",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_graphql.mdx",
      "frontmatter": {
        "id": "pg_graphql",
        "title": "pg_graphql: GraphQL for PostgreSQL",
        "description": "A GraphQL Interface for PostgreSQL"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[pg_graphql](https://supabase.github.io/pg_graphql/) is Postgres extension for interacting with the database using [GraphQL](https://graphql.org) instead of SQL.\n\nThe extension reflects a GraphQL schema from the existing SQL schema and exposes it through a SQL function, `graphql.resolve(...)`. This enables any programming language that can connect to Postgres to query the database via GraphQL with no additional servers, processes, or libraries.\n\nThe `pg_graphql` resolve method is designed to interop with [PostgREST](https://postgrest.org/en/stable/index.html), the tool that underpins the Supabase API, such that the `graphql.resolve` function can be called via RPC to safely and performantly expose the GraphQL API over HTTP/S.\n\nFor more information about how the SQL schema is reflected into a GraphQL schema, see the [pg_graphql API docs](https://supabase.github.io/pg_graphql/api/)."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"pg_graphql\" and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pg_graphql\" extension\ncreate extension pg_graphql;\n\n-- Disable the \"pg_graphql\" extension\ndrop extension if exists pg_graphql;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension you can call `drop extension`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "Given a table\n\n{/* prettier-ignore */}\n```sql\ncreate table \"Blog\"(\n id serial primary key,\n name text not null,\n description text\n);\n\ninsert into \"Blog\"(name)\nvalues ('My Blog');\n```\n\nThe reflected GraphQL schema can be queried immediately as\n\n{/* prettier-ignore */}\n```sql\nselect\n graphql.resolve($$\n {\n blogCollection(first: 1) {\n edges {\n node {\n id,\n name\n }\n }\n }\n }\n $$);\n```\n\nreturning the JSON\n\n{/* prettier-ignore */}\n```json\n{\n \"data\": {\n \"blogCollection\": {\n \"edges\": [\n {\n \"node\": {\n \"id\": 1\n \"name\": \"My Blog\"\n }\n }\n ]\n }\n }\n}\n```\n\nNote that `pg_graphql` fully supports schema introspection so you can connect any GraphQL IDE or schema inspection tool to see the full set of fields and arguments available in the API.",
          "level": 2
        },
        {
          "type": "section",
          "title": "API",
          "content": "- [`graphql.resolve`](https://supabase.github.io/pg_graphql/sql_interface/): A SQL function for executing GraphQL queries.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pg_graphql` documentation](https://github.com/supabase/pg_graphql)",
          "level": 2
        }
      ],
      "wordCount": 326,
      "characterCount": 2423
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_hashids",
      "identifier": "database-extensions-pg_hashids",
      "name": "pg_hashids: Short UIDs",
      "description": "Generate Short UIDs from Numbers",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_hashids",
      "dateModified": "2025-06-13T12:45:11.281382",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_hashids.mdx",
      "frontmatter": {
        "id": "pg_hashids",
        "title": "pg_hashids: Short UIDs",
        "description": "Generate Short UIDs from Numbers"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[pg_hashids](https://github.com/iCyberon/pg_hashids) provides a secure way to generate short, unique, non-sequential ids from numbers. The hashes are intended to be small, easy-to-remember identifiers that can be used to obfuscate data (optionally) with a password, alphabet, and salt. For example, you may wish to hide data like user IDs, order numbers, or tracking codes in favor of `pg_hashid`'s unique identifiers."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"pg_hashids\" and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pg_hashids\" extension\ncreate extension pg_hashids with schema extensions;\n\n-- Disable the \"pg_hashids\" extension\ndrop extension if exists pg_hashids;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep your `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "Suppose we have a table that stores order information, and we want to give customers a unique identifier without exposing the sequential `id` column. To do this, we can use `pg_hashid`'s `id_encode` function.\n\n```sql\ncreate table orders (\n id serial primary key,\n description text,\n price_cents bigint\n);\n\ninsert into orders (description, price_cents)\nvalues ('a book', 9095);\n\nselect\n id,\n id_encode(id) as short_id,\n description,\n price_cents\nfrom\n orders;\n\n id | short_id | description | price_cents\n----+----------+-------------+-------------\n 1 | jR | a book | 9095\n(1 row)\n```\n\nTo reverse the `short_id` back into an `id`, there is an equivalent function named `id_decode`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [pg_hashids documentation](https://github.com/iCyberon/pg_hashids)",
          "level": 2
        }
      ],
      "wordCount": 263,
      "characterCount": 1911
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_jsonschema",
      "identifier": "database-extensions-pg_jsonschema",
      "name": "pg_jsonschema: JSON Schema Validation",
      "description": "Validate json/jsonb with JSON Schema in PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_jsonschema",
      "dateModified": "2025-06-13T12:45:11.281603",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_jsonschema.mdx",
      "frontmatter": {
        "id": "pg_jsonschema",
        "title": "pg_jsonschema: JSON Schema Validation",
        "description": "Validate json/jsonb with JSON Schema in PostgreSQL.",
        "tocVideo": "amJo48ChLGs"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[JSON Schema](https://json-schema.org) is a language for annotating and validating JSON documents. [`pg_jsonschema`](https://github.com/supabase/pg_jsonschema) is a Postgres extension that adds the ability to validate PostgreSQL's built-in `json` and `jsonb` data types against JSON Schema documents."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pg_jsonschema` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pg_jsonschema\" extension\ncreate extension pg_jsonschema with schema extensions;\n\n-- Disable the \"pg_jsonschema\" extension\ndrop extension if exists pg_jsonschema;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Functions",
          "content": "- [`json_matches_schema(schema json, instance json)`](https://github.com/supabase/pg_jsonschema#api): Checks if a `json` _instance_ conforms to a JSON Schema _schema_.\n- [`jsonb_matches_schema(schema json, instance jsonb)`](https://github.com/supabase/pg_jsonschema#api): Checks if a `jsonb` _instance_ conforms to a JSON Schema _schema_.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "Since `pg_jsonschema` exposes its utilities as functions, we can execute them with a select statement:\n\n{/* prettier-ignore */}\n```sql\nselect\n extensions.json_matches_schema(\n schema := '{\"type\": \"object\"}',\n instance := '{}'\n );\n```\n\n`pg_jsonschema` is generally used in tandem with a [check constraint](https://www.postgresql.org/docs/current/ddl-constraints.html) as a way to constrain the contents of a json/b column to match a JSON Schema.\n\n{/* prettier-ignore */}\n```sql\ncreate table customer(\n id serial primary key,\n ...\n metadata json,\n\n check (\n json_matches_schema(\n '{\n \"type\": \"object\",\n \"properties\": {\n \"tags\": {\n \"type\": \"array\",\n \"items\": {\n \"type\": \"string\",\n \"maxLength\": 16\n }\n }\n }\n }',\n metadata\n )\n )\n);\n\n-- Example: Valid Payload\ninsert into customer(metadata)\nvalues ('{\"tags\": [\"vip\", \"darkmode-ui\"]}');\n-- Result:\n-- INSERT 0 1\n\n-- Example: Invalid Payload\ninsert into customer(metadata)\nvalues ('{\"tags\": [1, 3]}');\n-- Result:\n-- ERROR: new row for relation \"customer\" violates check constraint \"customer_metadata_check\"\n-- DETAIL: Failing row contains (2, {\"tags\": [1, 3]}).\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pg_jsonschema` documentation](https://github.com/supabase/pg_jsonschema)",
          "level": 2
        }
      ],
      "wordCount": 320,
      "characterCount": 2595
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_net",
      "identifier": "database-extensions-pg_net",
      "name": "pg_net: Async Networking",
      "description": "pg_net: an async networking extension for PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_net",
      "dateModified": "2025-06-13T12:45:11.282022",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_net.mdx",
      "frontmatter": {
        "id": "pg_net",
        "title": "pg_net: Async Networking",
        "description": "pg_net: an async networking extension for PostgreSQL."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The pg_net API is in beta. Functions signatures may change.\n\n[pg_net](https://github.com/supabase/pg_net/) enables Postgres to make asynchronous HTTP/HTTPS requests in SQL. It differs from the [`http`](https://supabase.com/docs/guides/database/extensions/http) extension in that it is asynchronous by default. This makes it useful in blocking functions (like triggers).\n\nIt eliminates the need for servers to continuously poll for database changes and instead allows the database to proactively notify external resources about significant events."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"pg_net\" and enable the extension.\n\n```sql\n-- Example: enable the \"pg_net\" extension.\ncreate extension pg_net;\n-- Note: The extension creates its own schema/namespace named \"net\" to avoid naming conflicts.\n\n-- Example: disable the \"pg_net\" extension\ndrop extension if exists pg_net;\ndrop schema net;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension, call `drop extension`.\n\nProcedural languages are automatically installed within `pg_catalog`, so you don't need to specify a schema.",
          "level": 2
        },
        {
          "type": "section",
          "title": "`http_get`",
          "content": "Creates an HTTP GET request returning the request's ID. HTTP requests are not started until the transaction is committed.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Signature [#get-signature]",
          "content": "This is a Postgres [SECURITY DEFINER](https://supabase.com/docs/guides/database/postgres/row-level-security#use-security-definer-functions) function.\n\n```sql\nnet.http_get(\n -- url for the request\n url text,\n -- key/value pairs to be url encoded and appended to the `url`\n params jsonb default '{}'::jsonb,\n -- key/values to be included in request headers\n headers jsonb default '{}'::jsonb,\n -- the maximum number of milliseconds the request may take before being canceled\n timeout_milliseconds int default 2000\n)\n -- request_id reference\n returns bigint\n\n strict\n volatile\n parallel safe\n language plpgsql\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Usage [#get-usage]",
          "content": "```sql\nselect\n net.http_get('https://news.ycombinator.com')\n as request_id;\nrequest_id\n----------\n 1\n(1 row)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`http_post`",
          "content": "Creates an HTTP POST request with a JSON body, returning the request's ID. HTTP requests are not started until the transaction is committed.\n\nThe body's character set encoding matches the database's `server_encoding` setting.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Signature [#post-signature]",
          "content": "This is a Postgres [SECURITY DEFINER](https://supabase.com/docs/guides/database/postgres/row-level-security#use-security-definer-functions) function\n\n```sql\nnet.http_post(\n -- url for the request\n url text,\n -- body of the POST request\n body jsonb default '{}'::jsonb,\n -- key/value pairs to be url encoded and appended to the `url`\n params jsonb default '{}'::jsonb,\n -- key/values to be included in request headers\n headers jsonb default '{\"Content-Type\": \"application/json\"}'::jsonb,\n -- the maximum number of milliseconds the request may take before being canceled\n timeout_milliseconds int default 2000\n)\n -- request_id reference\n returns bigint\n\n volatile\n parallel safe\n language plpgsql\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Usage [#post-usage]",
          "content": "```sql\nselect\n net.http_post(\n url:='https://httpbin.org/post',\n body:='{\"hello\": \"world\"}'::jsonb\n ) as request_id;\nrequest_id\n----------\n 1\n(1 row)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`http_delete`",
          "content": "Creates an HTTP DELETE request, returning the request's ID. HTTP requests are not started until the transaction is committed.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Signature [#post-signature]",
          "content": "This is a Postgres [SECURITY DEFINER](https://supabase.com/docs/guides/database/postgres/row-level-security#use-security-definer-functions) function\n\n```sql\nnet.http_delete(\n -- url for the request\n url text,\n -- key/value pairs to be url encoded and appended to the `url`\n params jsonb default '{}'::jsonb,\n -- key/values to be included in request headers\n headers jsonb default '{}'::jsonb,\n -- the maximum number of milliseconds the request may take before being canceled\n timeout_milliseconds int default 2000\n)\n -- request_id reference\n returns bigint\n\n strict\n volatile\n parallel safe\n language plpgsql\n security definer\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Usage [#delete-usage]",
          "content": "```sql\nselect\n net.http_delete(\n 'https://dummy.restapiexample.com/api/v1/delete/2'\n ) as request_id;\n----------\n 1\n(1 row)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Analyzing responses",
          "content": "Waiting requests are stored in the `net.http_request_queue` table. Upon execution, they are deleted.\n\n```sql\nCREATE UNLOGGED TABLE\n net.http_request_queue (\n id bigint NOT NULL DEFAULT nextval('net.http_request_queue_id_seq'::regclass),\n method text NOT NULL,\n url text NOT NULL,\n headers jsonb NOT NULL,\n body bytea NULL,\n timeout_milliseconds integer NOT NULL\n )\n```\n\nOnce a response is returned, by default, it is stored for 6 hours in the `net._http_response` table.\n\n```sql\nCREATE UNLOGGED TABLE\n net._http_response (\n id bigint NULL,\n status_code integer NULL,\n content_type text NULL,\n headers jsonb NULL,\n content text NULL,\n timed_out boolean NULL,\n error_msg text NULL,\n created timestamp with time zone NOT NULL DEFAULT now()\n )\n```\n\nThe responses can be observed with the following query:\n\n```sql\nselect * from net._http_response;\n```\n\nThe data can also be observed in the `net` schema with the [Supabase Dashboard's SQL Editor](https://supabase.com/dashboard/project/_/editor)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inspecting request data",
          "content": "The [Postman Echo API](https://documenter.getpostman.com/view/5025623/SWTG5aqV) returns a response with the same body and content\nas the request. It can be used to inspect the data being sent.\n\nSending a post request to the echo API\n\n```sql\nselect\n net.http_post(\n url := 'https://postman-echo.com/post',\n body := '{\"key1\": \"value\", \"key2\": 5}'::jsonb\n ) as request_id;\n```\n\nInspecting the echo API response content to ensure it contains the right body\n\n```sql\nselect\n \"content\"\nfrom net._http_response\nwhere id = \n-- returns information about the request\n-- including the body sent: {\"key\": \"value\", \"key\": 5}\n```\n\nAlternatively, by wrapping a request in a [database function](https://supabase.com/docs/guides/database/functions), sent row data can be logged or returned for inspection and debugging.\n\n```sql\ncreate or replace function debugging_example (row_id int)\nreturns jsonb as $$\ndeclare\n -- Store payload data\n row_data_var jsonb;\nbegin\n -- Retrieve row data and convert to JSON\n select to_jsonb(\"\".*) into row_data_var\n from \"\"\n where \"\".id = row_id;\n\n -- Initiate HTTP POST request to URL\n perform\n net.http_post(\n url := 'https://postman-echo.com/post',\n -- Use row data as payload\n body := row_data_var\n ) as request_id;\n\n -- Optionally Log row data or other data for inspection in Supabase Dashboard's Postgres Logs\n raise log 'Logging an entire row as JSON (%)', row_data_var;\n\n -- return row data to inspect\n return row_data_var;\n\n-- Handle exceptions here if needed\nexception\n when others then\n raise exception 'An error occurred: %', SQLERRM;\nend;\n$$ language plpgsql;\n\n-- calling function\nselect debugging_example();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Inspecting failed requests",
          "content": "Finds all failed requests\n\n```sql\nselect\n *\nfrom net._http_response\nwhere \"status_code\" >= 400 or \"error_msg\" is not null\norder by \"created\" desc;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Configuration",
          "content": "Supabase supports reconfiguring pg*net starting from v0.12.0+. For the latest release, initiate a Postgres upgrade in the [Infrastructure Settings](https://supabase.com/dashboard/project/*/settings/infrastructure).\n\nThe extension is configured to reliably execute up to 200 requests per second. The response messages are stored for only 6 hours to prevent needless buildup. The default behavior can be modified by rewriting config variables.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Get current settings",
          "content": "```sql\nselect\n \"name\",\n \"setting\"\nfrom pg_settings\nwhere \"name\" like 'pg_net%';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Alter settings",
          "content": "Change variables:\n\n```sql\nalter role \"postgres\" set pg_net.ttl to '24 hours';\nalter role \"postgres\" set pg_net.batch_size to 500;\n```\n\nThen reload the settings and restart the `pg_net` background worker with:\n\n```sql\nselect net.worker_restart();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Invoke a Supabase Edge Function",
          "content": "Make a POST request to a Supabase Edge Function with auth header and JSON body payload:\n\n```sql\nselect\n net.http_post(\n url:='https://project-ref.supabase.co/functions/v1/function-name',\n headers:='{\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \"}'::jsonb,\n body:='{\"name\": \"pg_net\"}'::jsonb\n ) as request_id;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Call an endpoint every minute with [pg_cron](https://supabase.com/docs/guides/database/extensions/pgcron)",
          "content": "The pg_cron extension enables Postgres to become its own cron server. With it you can schedule regular calls with up to a minute precision to endpoints.\n\n```sql\nselect cron.schedule(\n 'cron-job-name',\n '* * * * *', -- Executes every minute (cron syntax)\n $$\n -- SQL query\n select \"net\".\"http_post\"(\n -- URL of Edge function\n url:='https://project-ref.supabase.co/functions/v1/function-name',\n headers:='{\"Authorization\": \"Bearer \"}'::jsonb,\n body:='{\"name\": \"pg_net\"}'::jsonb\n ) as \"request_id\";\n $$\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Execute pg_net in a trigger",
          "content": "Make a call to an external endpoint when a trigger event occurs.\n\n```sql\n-- function called by trigger\ncreate or replace function ()\n returns trigger\n language plpgSQL\nas $$\nbegin\n -- calls pg_net function net.http_post\n -- sends request to postman API\n perform \"net\".\"http_post\"(\n 'https://postman-echo.com/post'::text,\n jsonb_build_object(\n 'old_row', to_jsonb(old.*),\n 'new_row', to_jsonb(new.*)\n ),\n headers:='{\"Content-Type\": \"application/json\"}'::jsonb\n ) as request_id;\n return new;\nEND $$;\n\n-- trigger for table update\ncreate trigger \n after update on \n for each row\n execute function ();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Send multiple table rows in one request",
          "content": "```sql\nwith \"selected_table_rows\" as (\n select\n -- Converts all the rows into a JSONB array\n jsonb_agg(to_jsonb(.*)) as JSON_payload\n from \n -- good practice to LIMIT the max amount of rows\n)\nselect\n net.http_post(\n url := 'https://postman-echo.com/post'::text,\n body := JSON_payload\n ) AS request_id\nFROM \"selected_table_rows\";\n```\n\nMore examples can be seen on the [Extension's GitHub page](https://github.com/supabase/pg_net/)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Limitations",
          "content": "- To improve speed and performance, the requests and responses are stored in [unlogged tables](https://pgpedia.info/u/unlogged-table.html), which are not preserved during a crash or unclean shutdown.\n- By default, response data is saved for only 6 hours\n- Can only make POST requests with JSON data. No other data formats are supported\n- Intended to handle at most 200 requests per second. Increasing the rate can introduce instability\n- Does not have support for PATCH/PUT requests\n- Can only work with one database at a time. It defaults to the `postgres` database.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Source code: [github.com/supabase/pg_net](https://github.com/supabase/pg_net/)\n- Official Docs: [github.com/supabase/pg_net](https://github.com/supabase/pg_net/)",
          "level": 2
        }
      ],
      "wordCount": 1405,
      "characterCount": 10948
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_plan_filter",
      "identifier": "database-extensions-pg_plan_filter",
      "name": "pg_plan_filter: Restrict Total Cost",
      "description": "Block queries over a total cost limit",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_plan_filter",
      "dateModified": "2025-06-13T12:45:11.282164",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_plan_filter.mdx",
      "frontmatter": {
        "id": "pg_plan_filter",
        "title": "pg_plan_filter: Restrict Total Cost",
        "description": "Block queries over a total cost limit"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[`pg_plan_filter`](https://github.com/pgexperts/pg_plan_filter) is Postgres extension to block execution of statements where query planner's estimate of the total cost exceeds a threshold. This is intended to give database administrators a way to restrict the contribution an individual query has on database load."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "The extension is already enabled by default via `shared_preload_libraries` setting.\n\nYou can follow the instructions below.",
          "level": 2
        },
        {
          "type": "section",
          "title": "API",
          "content": "`plan_filter.statement_cost_limit`: restricts the maximum total cost for executed statements\n`plan_filter.limit_select_only`: restricts to `select` statements\n\nNote that `limit_select_only = true` is not the same as read-only because `select` statements may modify data, for example, through a function call.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example",
          "content": "To demonstrate total cost filtering, we'll compare how `plan_filter.statement_cost_limit` treats queries that are under and over its cost limit. First, we set up a table with some data:\n\n{/* prettier-ignore */}\n```sql\ncreate table book(\n id int primary key\n);\n-- CREATE TABLE\n\ninsert into book(id) select * from generate_series(1, 10000);\n-- INSERT 0 10000\n```\n\nNext, we can review the explain plans for a single record select, and a whole table select.\n\n{/* prettier-ignore */}\n```sql\nexplain select * from book where id =1;\n QUERY PLAN\n---------------------------------------------------------------------------\n Index Only Scan using book_pkey on book (cost=0.28..2.49 rows=1 width=4)\n Index Cond: (id = 1)\n(2 rows)\n\nexplain select * from book;\n QUERY PLAN\n---------------------------------------------------------\n Seq Scan on book (cost=0.00..135.00 rows=10000 width=4)\n(1 row)\n```\n\nNow we can choose a `statement_cost_filter` value between the total cost for the single select (2.49) and the whole table select (135.0) so one statement will succeed and one will fail.\n\n{/* prettier-ignore */}\n```sql\nset plan_filter.statement_cost_limit = 50; -- between 2.49 and 135.0\n\nselect * from book where id = 1;\n id\n----\n 1\n(1 row)\n-- SUCCESS\n```\n\n{/* prettier-ignore */}\n```sql\nselect * from book;\n\nERROR: plan cost limit exceeded\nHINT: The plan for your query shows that it would probably have an excessive run time. This may be due to a logic error in the SQL, or it maybe just a very costly query. Rewrite your query or increase the configuration parameter \"plan_filter.statement_cost_limit\".\n-- FAILURE\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pg_plan_filter` documentation](https://github.com/pgexperts/pg_plan_filter)",
          "level": 2
        }
      ],
      "wordCount": 350,
      "characterCount": 2508
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_repack",
      "identifier": "database-extensions-pg_repack",
      "name": "pg_repack: Physical storage optimization and maintenance",
      "description": "A tool to remove bloat from tables and indexes and optimize physical data order and physical storage",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_repack",
      "dateModified": "2025-06-13T12:45:11.282296",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_repack.mdx",
      "frontmatter": {
        "id": "pg_repack",
        "title": "pg_repack: Physical storage optimization and maintenance",
        "description": "A tool to remove bloat from tables and indexes and optimize physical data order and physical storage"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[pg_repack](https://github.com/reorg/pg_repack) is a Postgres extension to remove bloat from tables and indexes, and optionally restore the physical order of clustered indexes. Unlike CLUSTER and VACUUM FULL, pg_repack runs \"online\" and does not hold a exclusive locks on the processed tables that could prevent ongoing database operations. pg_repack's efficiency is comparable to using CLUSTER directly.\n\npg_repack provides the following methods to optimize physical storage:\n\n- Online CLUSTER: ordering table data by cluster index in a non-blocking way\n- Ordering table data by specified columns\n- Online VACUUM FULL: packing rows only in a non-blocking way\n- Rebuild or relocate only the indexes of a table\n\npg_repack has 2 components, the database extension and a client-side CLI to control it."
        },
        {
          "type": "section",
          "title": "Requirements",
          "content": "- A target table must have a PRIMARY KEY, or a UNIQUE total index on a NOT NULL column.\n- Performing a full-table repack requires free disk space about twice as large as the target table and its indexes.\n\npg_repack requires the Postgres superuser role by default. That role is not available to users on the Supabase platform. To avoid that requirement, use the `-k` or `--no-superuser-check` flags on every `pg_repack` CLI command.\n\nThe first version of pg_repack with full support for non-superuser repacking is 1.5.2. You can check the version installed on your Supabase instance using\n\n```sql\nselect default_version\nfrom pg_available_extensions\nwhere name = 'pg_repack';\n```\n\nIf pg_repack is not present, or the version is < 1.5.2, [upgrade to the latest version](/docs/guides/platform/upgrading) of Supabase to gain access.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "Get started with pg_repack by enabling the extension in the Supabase Dashboard.\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"pg_repack\" and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Example: enable the \"pg_repack\" extension\ncreate extension pg_repack with schema extensions;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Install the CLI",
          "content": "Select an option from the pg_repack docs to [install the client CLI](https://reorg.github.io/pg_repack/#download).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Syntax",
          "content": "All pg_repack commands should include the `-k` flag to skip the client-side superuser check.\n\n{/* prettier-ignore */}\n```sh\npg_repack -k [OPTION]... [DBNAME]\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Example",
          "content": "Perform an online `VACUUM FULL` on the tables `public.foo` and `public.bar` in the database `postgres`:\n\n{/* prettier-ignore */}\n```sh\npg_repack -k -h db..supabase.co -p 5432 -U postgres -d postgres --no-order --table public.foo --table public.bar\n```\n\nSee the [official pg_repack documentation](https://reorg.github.io/pg_repack/) for the full list of options.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Limitations",
          "content": "- pg_repack cannot reorganize temporary tables.\n- pg_repack cannot cluster tables by GiST indexes.\n- You cannot perform DDL commands of the target tables except VACUUM or ANALYZE while pg_repack is working.\n pg_repack holds an ACCESS SHARE lock on the target table to enforce this restriction.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official pg_repack documentation](https://reorg.github.io/pg_repack/)",
          "level": 2
        }
      ],
      "wordCount": 450,
      "characterCount": 3180
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pg_stat_statements",
      "identifier": "database-extensions-pg_stat_statements",
      "name": "pg_stat_statements: Query Performance Monitoring",
      "description": "Track planning and execution statistics of all SQL statements executed on the database.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pg_stat_statements",
      "dateModified": "2025-06-13T12:45:11.282434",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pg_stat_statements.mdx",
      "frontmatter": {
        "id": "pg_stat_statements",
        "title": "pg_stat_statements: Query Performance Monitoring",
        "description": "Track planning and execution statistics of all SQL statements executed on the database."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "`pg_stat_statements` is a database extension that exposes a view, of the same name, to track statistics about SQL statements executed on the database. The following table shows some of the available statistics and metadata:\n\n| Column Name | Column Type | Description |\n| ----------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |\n| `userid` | `oid` (references `pg_authid.oid`) | OID of user who executed the statement |\n| `dbid` | `oid` (references `pg_database.oid`) | OID of database in which the statement was executed |\n| `toplevel` | `bool` | True if the query was executed as a top-level statement (always true if pg_stat_statements.track is set to top) |\n| `queryid` | `bigint` | Hash code to identify identical normalized queries. |\n| `query` | `text` | Text of a representative statement |\n| `plans` | `bigint` | Number of times the statement was planned (if pg_stat_statements.track_planning is enabled, otherwise zero) |\n| `total_plan_time` | `double precision` | Total time spent planning the statement, in milliseconds (if pg_stat_statements.track_planning is enabled, otherwise zero) |\n| `min_plan_time` | `double precision` | Minimum time spent planning the statement, in milliseconds (if pg_stat_statements.track_planning is enabled, otherwise zero) |\n\nA full list of statistics is available in the [pg_stat_statements docs](https://www.postgresql.org/docs/current/pgstatstatements.html).\n\nFor more information on query optimization, check out the [query performance guide](/docs/guides/platform/performance#examining-query-performance)."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"pg_stat_statements\" and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pg_stat_statements\" extension\ncreate extension pg_stat_statements with schema extensions;\n\n-- Disable the \"pg_stat_statements\" extension\ndrop extension if exists pg_stat_statements;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inspecting activity",
          "content": "A common use for `pg_stat_statements` is to track down expensive or slow queries. The `pg_stat_statements` view contains a row for each executed query with statistics inlined. For example, you can leverage the statistics to identify frequently executed and slow queries against a given table.\n\n{/* prettier-ignore */}\n```sql\nselect\n calls,\n mean_exec_time,\n max_exec_time,\n total_exec_time,\n stddev_exec_time,\n query\nfrom\n pg_stat_statements\nwhere\n calls > 50 -- at least 50 calls\n and mean_exec_time > 2.0 -- averaging at least 2ms/call\n and total_exec_time > 60000 -- at least one minute total server time spent\n and query ilike '%user_in_organization%' -- filter to queries that touch the user_in_organization table\norder by\n calls desc\n```\n\nFrom the results, we can make an informed decision about which queries to optimize or index.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [pg_stat_statements documentation](https://www.postgresql.org/docs/current/pgstatstatements.html)",
          "level": 2
        }
      ],
      "wordCount": 443,
      "characterCount": 3416
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgaudit",
      "identifier": "database-extensions-pgaudit",
      "name": "PGAudit: Postgres Auditing",
      "description": "Session and object auditing via PostgreSQL standard logging",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgaudit",
      "dateModified": "2025-06-13T12:45:11.282811",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgaudit.mdx",
      "frontmatter": {
        "id": "pgaudit",
        "title": "PGAudit: Postgres Auditing",
        "description": "Session and object auditing via PostgreSQL standard logging"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[PGAudit](https://www.pgaudit.org) extends Postgres's built-in logging abilities. It can be used to selectively track activities within your database.\n\nThis helps you with:\n\n- **Compliance**: Meeting audit requirements for regulations\n- **Security**: Detecting suspicious database activity\n- **Troubleshooting**: Identifying and fixing database issues"
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pgaudit` and enable the extension.\n\n```sql\n-- Enable the \"pgaudit\" extension\ncreate extension pgaudit;\n\n-- Disable the \"pgaudit\" extension\ndrop extension if exists pgaudit;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Configure the extension",
          "content": "PGAudit can be configured with different levels of precision.\n\n**PGAudit logging precision:**\n\n- **[Session](#session-logging):** Logs activity within a connection, such as a [psql](https://supabase.com/docs/guides/database/connecting-to-postgres#connecting-with-psql) connection.\n- **[User](#user-logging):** Logs activity by a particular database user (for example, `anon` or `postgres`).\n- **[Global](#global-logging):** Logs activity across the entire database.\n- **[Object](#object-logging):** Logs events related to specific database objects (for example, the auth.users table).\n\nAlthough Session, User, and Global modes differ in their precision, they're all considered variants of **Session Mode** and are configured with the same input categories.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Session mode categories",
          "content": "These modes can monitor predefined categories of database operations:\n\n| Category | What it Logs | Description |\n| ---------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------- |\n| `read` | Data retrieval (SELECT, COPY) | Tracks what data is being accessed. |\n| `write` | Data modification (INSERT, DELETE, UPDATE, TRUNCATE, COPY) | Tracks changes made to your database. |\n| `function` | FUNCTION, PROCEDURE, and DO/END block executions | Tracks routine/function executions |\n| `role` | User management actions (CREATE, DROP, ALTER on users and privileges) | Tracks changes to user permissions and access. |\n| `ddl` | Schema changes (CREATE, DROP, ALTER statements) | Monitors modifications to your database structure (tables, indexes, etc.). |\n| `misc` | Less common commands (FETCH, CHECKPOINT) | Captures obscure actions for deeper analysis if needed. |\n| `all` | Everything above | Comprehensive logging for complete audit trails. |\n\nBelow is a limited example of how to assign PGAudit to monitor specific categories.\n\n```sql\n-- log all CREATE, ALTER, and DROP events\n... pgaudit.log = 'ddl';\n\n-- log all CREATE, ALTER, DROP, and SELECT events\n... pgaudit.log = 'read, ddl';\n\n-- log nothing\n... pgaudit.log = 'none';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Session logging",
          "content": "When you are connecting in a session environment, such as a [psql](https://supabase.com/docs/guides/database/connecting-to-postgres#connecting-with-psql) connection, you can configure PGAudit to record events initiated within the session.\n\nThe [Dashboard](https://supabase.com/dashboard/project/_) is a transactional environment and won't sustain a session.\n\nInside a session, by default, PGAudit will log nothing:\n\n```sql\n-- returns 'none'\nshow pgaudit.log;\n```\n\nIn the session, you can `set` the `pgaudit.log` variable to record events:\n\n```sql\n-- log CREATE, ALTER, and DROP events\nset pgaudit.log = 'ddl';\n\n-- log all CREATE, ALTER, DROP, and SELECT events\nset pgaudit.log = 'read, ddl';\n\n-- log nothing\nset pgaudit.log = 'none';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "User logging",
          "content": "There are some cases where you may want to monitor a database user's actions. For instance, let's say you connected your database to [Zapier](https://supabase.com/partners/integrations/zapier) and created a custom role for it to use:\n\n```sql\ncreate user \"zapier\" with password '';\n```\n\nYou may want to log all actions initiated by `zapier`, which can be done with the following command:\n\n```sql\nalter role \"zapier\" set pgaudit.log to 'all';\n```\n\nTo remove the settings, execute the following code:\n\n```sql\n-- disables role's log\nalter role \"zapier\" set pgaudit.log to 'none';\n\n-- check to make sure the changes are finalized:\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere rolname = 'zapier';\n-- should return a rolconfig path with \"pgaudit.log=none\" present\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Global logging",
          "content": "Use global logging cautiously. It can generate many logs and make it difficult to find important events. Consider limiting the scope of what is logged by using session, user, or object logging where possible.\n\nThe below SQL configures PGAudit to record all events associated with the `postgres` role. Since it has extensive privileges, this effectively monitors all database activity.\n\n```sql\nalter role \"postgres\" set pgaudit.log to 'all';\n```\n\nTo check if the `postgres` role is auditing, execute the following command:\n\n```sql\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere rolname = 'postgres';\n-- should return a rolconfig path with \"pgaudit.log=all\" present\n```\n\nTo remove the settings, execute the following code:\n\n```sql\nalter role \"postgres\" set pgaudit.log to 'none';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Object logging",
          "content": "To fine-tune what object events PGAudit will record, you must create a custom database role with limited permissions:\n\n```sql\ncreate role \"some_audit_role\" noinherit;\n```\n\nNo other Postgres user can assume or login via this role. It solely exists to securely define what PGAudit will record.\n\nOnce the role is created, you can direct PGAudit to log by assigning it to the `pgaudit.role` variable:\n\n```sql\nalter role \"postgres\" set pgaudit.role to 'some_audit_role';\n```\n\nYou can then assign the role to monitor only approved object events, such as `select` statements that include a specific table:\n\n```sql\ngrant select on random_table to \"some_audit_role\";\n```\n\nWith this privilege granted, PGAudit will record all select statements that reference the `random_table`, regardless of _who_ or _what_ actually initiated the event. All assignable privileges can be viewed in the [Postgres documentation](https://www.postgresql.org/docs/current/ddl-priv.html).\n\nIf you would no longer like to use object logging, you will need to unassign the `pgaudit.role` variable:\n\n```sql\n-- change pgaudit.role to no longer reference some_audit_role\nalter role \"postgres\" set pgaudit.role to '';\n\n-- view if pgaudit.role changed with the following command:\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere rolname = 'postgres';\n-- should return a rolconfig path with \"pgaudit.role=\"\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Interpreting Audit Logs",
          "content": "PGAudit was designed for storing logs as CSV files with the following headers:\n\nReferenced from the [PGAudit official docs](https://github.com/pgaudit/pgaudit/blob/master/README.md#format)\n\n| header | Description |\n| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| AUDIT_TYPE | SESSION or OBJECT |\n| STATEMENT_ID | Unique statement ID for this session. Sequential even if some statements are not logged. |\n| SUBSTATEMENT_ID | Sequential ID for each sub-statement within the main statement. Continuous even if some are not logged. |\n| CLASS | ..., READ, ROLE (see pgaudit.log). |\n| COMMAND | ..., ALTER TABLE, SELECT. |\n| OBJECT_TYPE | TABLE, INDEX, VIEW, etc. Available for SELECT, DML, and most DDL statements. |\n| OBJECT_NAME | The fully qualified object name (for example, public.account). Available for SELECT, DML, and most DDL. |\n| STATEMENT | Statement executed on the backend. |\n| PARAMETER | If pgaudit.log_parameter is set, this field contains the statement parameters as quoted CSV, or \\. Otherwise, it's \\. |\n\nA log made from the following create statement:\n\n```sql\ncreate table account (\n id int primary key,\n name text,\n description text\n);\n```\n\nGenerates the following log in the [Dashboard's Postgres Logs](https://supabase.com/dashboard/project/_/logs/postgres-logs):\n\n```\n AUDIT: SESSION,1,1,DDL,CREATE TABLE,TABLE,public.account,create table account(\n id int,\n name text,\n description text\n); \n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Finding and filtering audit logs",
          "content": "Logs generated by PGAudit can be found in [Postgres Logs](https://supabase.com/dashboard/project/_/logs/postgres-logs?s=AUDIT). To find a specific log, you can use the log explorer. Below is a basic example to extract logs referencing `CREATE TABLE` events\n\n```sql\nselect\n cast(t.timestamp as datetime) as timestamp,\n event_message\nfrom\n postgres_logs as t\n cross join unnest(metadata) as m\n cross join unnest(m.parsed) as p\nwhere event_message like 'AUDIT%CREATE TABLE%'\norder by timestamp desc\nlimit 100;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Monitoring API events",
          "content": "API requests are already recorded in the [API Edge Network](https://supabase.com/dashboard/project/_/logs/edge-logs) logs.\n\nTo monitor all writes initiated by the PostgREST API roles:\n\n```sql\nalter role \"authenticator\" set pgaudit.log to 'write';\n\n-- the above is the practical equivalent to:\n-- alter role \"anon\" set pgaudit.log TO 'write';\n-- alter role \"authenticated\" set pgaudit.log TO 'write';\n-- alter role \"service_role\" set pgaudit.log TO 'write';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Monitoring the `auth.users` table",
          "content": "In the worst case scenario, where a privileged roles' password is exposed, you can use PGAudit to monitor if the `auth.users` table was targeted. It should be stated that API requests are already monitored in the [API Edge Network](https://supabase.com/dashboard/project/_/logs/edge-logs) and this is more about providing greater clarity about what is happening at the database level.\n\nLogging `auth.user` should be done in Object Mode and requires a custom role:\n\n```sql\n-- create logging role\ncreate role \"auth_auditor\" noinherit;\n\n-- give role permission to observe relevant table events\ngrant select on auth.users to \"auth_auditor\";\ngrant delete on auth.users to \"auth_auditor\";\n\n-- assign auth_auditor to pgaudit.role\nalter role \"postgres\" set pgaudit.role to 'auth_auditor';\n```\n\nWith the above code, any query involving reading or deleting from the auth.users table will be logged.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Disabling excess logging",
          "content": "PGAudit, if not configured mindfully, can log all database events, including background tasks. This can generate an undesirably large amount of logs in a few hours.\n\nThe first step to solve this problem is to identify which database users PGAudit is observing:\n\n```sql\n-- find all users monitored by pgaudit\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere\n exists (\n select\n 1\n from UNNEST(rolconfig) as c\n where c like '%pgaudit.role%' or c like '%pgaudit.log%'\n );\n```\n\nTo prevent PGAudit from monitoring the problematic roles, you'll want to change their `pgaudit.log` values to `none` and `pgaudit.role` values to `empty quotes ''`\n\n```sql\n -- Use to disable object level logging\n alter role \"\" set pgaudit.role to '';\n\n -- Use to disable global and user level logging\n alter role \"\" set pgaudit.log to 'none';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using PGAudit to debug database functions",
          "content": "Technically yes, but it is not the best approach. It is better to check out our [function debugging guide](https://supabase.com/docs/guides/database/functions#general-logging) instead.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Downloading database logs",
          "content": "In the [Logs Dashboard](https://supabase.com/dashboard/project/_/logs/postgres-logs) you can download logs as CSVs.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Logging observed table rows",
          "content": "By default, PGAudit records queries, but not the returned rows. You can modify this behavior with the `pgaudit.log_rows` variable:\n\n```sql\n--enable\nalter role \"postgres\" set pgaudit.log_rows to 'on';\n\n-- disable\nalter role \"postgres\" set pgaudit.log_rows to 'off';\n```\n\nYou should not do this unless you are _absolutely_ certain it is necessary for your use case. It can expose sensitive values to your logs that ideally should not be preserved. Furthermore, if done in excess, it can noticeably reduce database performance.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Logging function parameters",
          "content": "We don't currently support configuring `pgaudit.log_parameter` because it may log secrets in encrypted columns if you are using [pgsodium](https://supabase.com/docs/guides/database/extensions/pgsodium) or[Vault](https://supabase.com/docs/guides/database/vault).\n\nYou can upvote this [feature request](https://github.com/orgs/supabase/discussions/20183) with your use-case if you'd like this restriction lifted.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Does PGAudit support system wide configurations?",
          "content": "PGAudit allows settings to be applied to 3 different database scopes:\n\n| Scope | Description | Configuration File/Command |\n| -------- | ------------------ | -------------------------- |\n| System | Entire server | ALTER SYSTEM commands |\n| Database | Specific database | ALTER DATABASE commands |\n| Role | Specific user/role | ALTER ROLE commands |\n\nSupabase limits full privileges for file system and database variables, meaning PGAudit modifications can only occur at the role level. Assigning PGAudit to the `postgres` role grants it nearly complete visibility into the database, making role-level adjustments a practical alternative to configuring at the database or system level.\n\nPGAudit's [official documentation](https://www.pgaudit.org) focuses on system and database level configs, but its docs officially supports role level configs, too.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official `PGAudit` documentation](https://www.pgaudit.org)\n- [Database Function Logging](https://supabase.com/docs/guides/database/functions#general-logging)\n- [Supabase Logging](https://supabase.com/docs/guides/platform/logs)\n- [Self-Hosting Logs](https://supabase.com/docs/reference/self-hosting-analytics/introduction)",
          "level": 2
        }
      ],
      "wordCount": 1845,
      "characterCount": 13654
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgjwt",
      "identifier": "database-extensions-pgjwt",
      "name": "pgjwt: JSON Web Tokens",
      "description": "Encode and decode JWTs in PostgreSQL",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgjwt",
      "dateModified": "2025-06-13T12:45:11.282973",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgjwt.mdx",
      "frontmatter": {
        "id": "pgjwt",
        "title": "pgjwt: JSON Web Tokens",
        "description": "Encode and decode JWTs in PostgreSQL"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "{/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}\nThe [`pgjwt`](https://github.com/michelp/pgjwt) (PostgreSQL JSON Web Token) extension allows you to create and parse [JSON Web Tokens (JWTs)](https://en.wikipedia.org/wiki/JSON_Web_Token) within a PostgreSQL database. JWTs are commonly used for authentication and authorization in web applications and services."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pgjwt` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pgjwt\" extension\ncreate extension pgjwt schema extensions;\n\n-- Disable the \"pgjwt\" extension\ndrop extension if exists pgjwt;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "API",
          "content": "- [`sign(payload json, secret text, algorithm text default 'HSA256')`](https://github.com/michelp/pgjwt#usage): Signs a JWT containing _payload_ with _secret_ using _algorithm_.\n- [`verify(token text, secret text, algorithm text default 'HSA256')`](https://github.com/michelp/pgjwt#usage): Decodes a JWT _token_ that was signed with _secret_ using _algorithm_.\n\nWhere:\n\n- `payload` is an encrypted JWT represented as a string.\n- `secret` is the private/secret passcode which is used to sign the JWT and verify its integrity.\n- `algorithm` is the method used to sign the JWT using the secret.\n- `token` is an encrypted JWT represented as a string.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "Once the extension is installed, you can use its functions to create and parse JWTs. Here's an example of how you can use the `sign` function to create a JWT:\n\n{/* prettier-ignore */}\n```sql\nselect\n extensions.sign(\n payload := '{\"sub\":\"1234567890\",\"name\":\"John Doe\",\"iat\":1516239022}',\n secret := 'secret',\n algorithm := 'HS256'\n );\n```\n\nThe `pgjwt_encode` function returns a string that represents the JWT, which can then be safely transmitted between parties.\n\n{/* prettier-ignore */}\n```\n sign\n---------------------------------\n eyJhbGciOiJIUzI1NiIsInR5cCI6IkpX\n VCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiw\n ibmFtZSI6IkpvaG4gRG9lIiwiaWF0Ijo\n xNTE2MjM5MDIyfQ.XbPfbIHMI6arZ3Y9\n 22BhjWgQzWXcXNrz0ogtVhfEd2o\n(1 row)\n```\n\nTo parse a JWT and extract its claims, you can use the `verify` function. Here's an example:\n\n{/* prettier-ignore */}\n```sql\nselect\n extensions.verify(\n token := 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiRm9vIn0.Q8hKjuadCEhnCPuqIj9bfLhTh_9QSxshTRsA5Aq4IuM',\n secret := 'secret',\n algorithm := 'HS256'\n );\n```\n\nWhich returns the decoded contents and some associated metadata.\n\n{/* prettier-ignore */}\n```sql\n header | payload | valid\n-----------------------------+----------------+-------\n {\"alg\":\"HS256\",\"typ\":\"JWT\"} | {\"name\":\"Foo\"} | t\n(1 row)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pgjwt` documentation](https://github.com/michelp/pgjwt)",
          "level": 2
        }
      ],
      "wordCount": 375,
      "characterCount": 3066
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgmq",
      "identifier": "database-extensions-pgmq",
      "name": "pgmq: Queues",
      "description": "pgmq: Managed queues in Postgres",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgmq",
      "dateModified": "2025-06-13T12:45:11.283048",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgmq.mdx",
      "frontmatter": {
        "id": "pgmq",
        "title": "pgmq: Queues",
        "description": "pgmq: Managed queues in Postgres"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "See the [Supabase Queues docs](/docs/guides/queues)."
        }
      ],
      "wordCount": 5,
      "characterCount": 52
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgroonga",
      "identifier": "database-extensions-pgroonga",
      "name": "PGroonga: Multilingual Full Text Search",
      "description": "Full Text Search for multiple languages in PostgreSQL",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgroonga",
      "dateModified": "2025-06-13T12:45:11.283233",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgroonga.mdx",
      "frontmatter": {
        "id": "pgroonga",
        "title": "PGroonga: Multilingual Full Text Search",
        "description": "Full Text Search for multiple languages in PostgreSQL",
        "tocVideo": "Mmmv9g_MiBA"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "`PGroonga` is a Postgres extension adding a full text search indexing method based on [Groonga](https://groonga.org). While native Postgres supports full text indexing, it is limited to alphabet and digit based languages. `PGroonga` offers a wider range of character support making it viable for a superset of languages supported by Postgres including Japanese, Chinese, etc."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pgroonga` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pgroonga\" extension\ncreate extension pgroonga with schema extensions;\n\n-- Disable the \"pgroonga\" extension\ndrop extension if exists pgroonga;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating a full text search index",
          "content": "Given a table with a `text` column:\n\n{/* prettier-ignore */}\n```sql\ncreate table memos (\n id serial primary key,\n content text\n);\n```\n\nWe can index the column for full text search with a `pgroonga` index:\n\n{/* prettier-ignore */}\n```sql\ncreate index ix_memos_content ON memos USING pgroonga(content);\n```\n\nTo test the full text index, we'll add some data.\n\n{/* prettier-ignore */}\n```sql\ninsert into memos(content)\nvalues\n ('PostgreSQL is a relational database management system.'),\n ('Groonga is a fast full text search engine that supports all languages.'),\n ('PGroonga is a PostgreSQL extension that uses Groonga as index.'),\n ('There is groonga command.');\n```\n\nThe Postgres query planner is smart enough to know that, for extremely small tables, it's faster to scan the whole table rather than loading an index. To force the index to be used, we can disable sequential scans:\n\n{/* prettier-ignore */}\n```sql\n-- For testing only. Don't do this in production\nset enable_seqscan = off;\n```\n\nNow if we run an explain plan on a query filtering on `memos.content`:\n\n{/* prettier-ignore */}\n```sql\nexplain select * from memos where content like '%engine%';\n\n QUERY PLAN\n-----------------------------------------------------------------------------\nIndex Scan using ix_memos_content on memos (cost=0.00..1.11 rows=1 width=36)\n Index Cond: (content ~~ '%engine%'::text)\n(2 rows)\n```\n\nThe `pgroonga` index is used to retrieve the result set:\n\n```markdown\n| id | content |\n| --- | ------------------------------------------------------------------------ |\n| 2 | 'Groonga is a fast full text search engine that supports all languages.' |\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Full text search",
          "content": "The `&@~` operator performs full text search. It returns any matching results. Unlike `LIKE` operator, `pgroonga` can search any text that contains the keyword case insensitive.\n\nTake the following example:\n\n{/* prettier-ignore */}\n```sql\nselect * from memos where content &@~ 'groonga';\n```\n\nAnd the result:\n\n```markdown\nid | content \n----+------------------------------------------------------------------------\n2 | Groonga is a fast full text search engine that supports all languages.\n3 | PGroonga is a PostgreSQL extension that uses Groonga as index.\n4 | There is groonga command.\n(3 rows)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Match all search words",
          "content": "To find all memos where content contains BOTH of the words `postgres` and `pgroonga`, we can just use space to separate each words:\n\n{/* prettier-ignore */}\n```sql\nselect * from memos where content &@~ 'postgres pgroonga';\n```\n\nAnd the result:\n\n```markdown\nid | content \n----+----------------------------------------------------------------\n3 | PGroonga is a PostgreSQL extension that uses Groonga as index.\n(1 row)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Match any search words",
          "content": "To find all memos where content contain ANY of the words `postgres` or `pgroonga`, use the upper case `OR`:\n\n{/* prettier-ignore */}\n```sql\nselect * from memos where content &@~ 'postgres OR pgroonga';\n```\n\nAnd the result:\n\n```markdown\nid | content \n----+----------------------------------------------------------------\n1 | PostgreSQL is a relational database management system.\n3 | PGroonga is a PostgreSQL extension that uses Groonga as index.\n(2 rows)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Search that matches words with negation",
          "content": "To find all memos where content contain the word `postgres` but not `pgroonga`, use `-` symbol:\n\n{/* prettier-ignore */}\n```sql\nselect * from memos where content &@~ 'postgres -pgroonga';\n```\n\nAnd the result:\n\n```markdown\nid | content \n----+--------------------------------------------------------\n1 | PostgreSQL is a relational database management system.\n(1 row)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [PGroonga documentation](https://pgroonga.github.io/tutorial/)",
          "level": 2
        }
      ],
      "wordCount": 669,
      "characterCount": 4683
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgrouting",
      "identifier": "database-extensions-pgrouting",
      "name": "pgrouting: Geospatial Routing",
      "description": "Extends PostGIS with Geospatial Routing",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgrouting",
      "dateModified": "2025-06-13T12:45:11.283411",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgrouting.mdx",
      "frontmatter": {
        "id": "pgrouting",
        "title": "pgrouting: Geospatial Routing",
        "description": "Extends PostGIS with Geospatial Routing"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[`pgRouting`](http://pgrouting.org) is Postgres and [PostGIS](http://postgis.net) extension adding geospatial routing functionality.\n\nThe core functionality of `pgRouting` is a set of path finding algorithms including:\n\n- All Pairs Shortest Path, Johnson’s Algorithm\n- All Pairs Shortest Path, Floyd-Warshall Algorithm\n- Shortest Path A\\*\n- Bi-directional Dijkstra Shortest Path\n- Bi-directional A\\* Shortest Path\n- Shortest Path Dijkstra\n- Driving Distance\n- K-Shortest Path, Multiple Alternative Paths\n- K-Dijkstra, One to Many Shortest Path\n- Traveling Sales Person\n- Turn Restriction Shortest Path (TRSP)"
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pgrouting` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"pgRouting\" extension\ncreate extension pgrouting cascade;\n\n-- Disable the \"pgRouting\" extension\ndrop extension if exists pgRouting;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example",
          "content": "As an example, we'll solve the [traveling salesperson problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) using the `pgRouting`'s `pgr_TSPeuclidean` function from some PostGIS coordinates.\n\nA summary of the traveling salesperson problem is, given a set of city coordinates, solve for a path that goes through each city and minimizes the total distance traveled.\n\nFirst we populate a table with some X, Y coordinates\n\n{/* prettier-ignore */}\n```sql\ncreate table wi29 (\n id bigint,\n x float,\n y float,\n geom geometry\n);\n\ninsert into wi29 (id, x, y)\nvalues\n (1,20833.3333,17100.0000),\n (2,20900.0000,17066.6667),\n (3,21300.0000,13016.6667),\n (4,21600.0000,14150.0000),\n (5,21600.0000,14966.6667),\n (6,21600.0000,16500.0000),\n (7,22183.3333,13133.3333),\n (8,22583.3333,14300.0000),\n (9,22683.3333,12716.6667),\n (10,23616.6667,15866.6667),\n (11,23700.0000,15933.3333),\n (12,23883.3333,14533.3333),\n (13,24166.6667,13250.0000),\n (14,25149.1667,12365.8333),\n (15,26133.3333,14500.0000),\n (16,26150.0000,10550.0000),\n (17,26283.3333,12766.6667),\n (18,26433.3333,13433.3333),\n (19,26550.0000,13850.0000),\n (20,26733.3333,11683.3333),\n (21,27026.1111,13051.9444),\n (22,27096.1111,13415.8333),\n (23,27153.6111,13203.3333),\n (24,27166.6667,9833.3333),\n (25,27233.3333,10450.0000),\n (26,27233.3333,11783.3333),\n (27,27266.6667,10383.3333),\n (28,27433.3333,12400.0000),\n (29,27462.5000,12992.2222);\n```\n\nNext we use the `pgr_TSPeuclidean` function to find the best path.\n\n{/* prettier-ignore */}\n```sql\nselect\n *\nfrom\n pgr_TSPeuclidean($$select * from wi29$$)\n```\n\n{/* prettier-ignore */}\n```sql\n seq | node | cost | agg_cost \n-----+------+------------------+------------------\n 1 | 1 | 0 | 0\n 2 | 2 | 74.535614157127 | 74.535614157127\n 3 | 6 | 900.617093380362 | 975.152707537489\n 4 | 10 | 2113.77757765045 | 3088.93028518793\n 5 | 11 | 106.718669615254 | 3195.64895480319\n 6 | 12 | 1411.95293791574 | 4607.60189271893\n 7 | 13 | 1314.23824873744 | 5921.84014145637\n 8 | 14 | 1321.76283931305 | 7243.60298076942\n 9 | 17 | 1202.91366735569 | 8446.5166481251\n 10 | 18 | 683.333268292684 | 9129.84991641779\n 11 | 15 | 1108.05137466134 | 10237.9012910791\n 12 | 19 | 772.082339448903 | 11009.983630528\n 13 | 22 | 697.666150054665 | 11707.6497805827\n 14 | 23 | 220.141999627513 | 11927.7917802102\n 15 | 21 | 197.926372783442 | 12125.7181529937\n 16 | 29 | 440.456596290771 | 12566.1747492844\n 17 | 28 | 592.939989005405 | 13159.1147382898\n 18 | 26 | 648.288376333318 | 13807.4031146231\n 19 | 20 | 509.901951359278 | 14317.3050659824\n 20 | 25 | 1330.83095428717 | 15648.1360202696\n 21 | 27 | 74.535658878487 | 15722.6716791481\n 22 | 24 | 559.016994374947 | 16281.688673523\n 23 | 16 | 1243.87392358622 | 17525.5625971092\n 24 | 9 | 4088.0585364911 | 21613.6211336004\n 25 | 7 | 650.85409697993 | 22264.4752305803\n 26 | 3 | 891.004385199336 | 23155.4796157796\n 27 | 4 | 1172.36699411442 | 24327.846609894\n 28 | 8 | 994.708187806297 | 25322.5547977003\n 29 | 5 | 1188.01888359478 | 26510.5736812951\n 30 | 1 | 2266.91173136004 | 28777.4854126552\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pgRouting` documentation](https://docs.pgrouting.org/latest/en/index.html)",
          "level": 2
        }
      ],
      "wordCount": 523,
      "characterCount": 4322
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgsodium",
      "identifier": "database-extensions-pgsodium",
      "name": "pgsodium (pending deprecation): Encryption Features",
      "description": "Encryption library for PostgreSQL",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgsodium",
      "dateModified": "2025-06-13T12:45:11.283527",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgsodium.mdx",
      "frontmatter": {
        "id": "pgsodium",
        "title": "pgsodium (pending deprecation): Encryption Features",
        "description": "Encryption library for PostgreSQL"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase DOES NOT RECOMMEND any new usage of [`pgsodium`](https://github.com/michelp/pgsodium).\n\nThe [`pgsodium`](https://github.com/michelp/pgsodium) extension is expected to go through a deprecation cycle in the near future. We will reach out to owners of impacted projects to assist with migrations away from [`pgsodium`](https://github.com/michelp/pgsodium) once the deprecation process begins.\n\nThe [Vault extension](/docs/guides/database/vault) won’t be impacted. Its internal implementation will shift away from pgsodium, but the interface and API will remain unchanged.\n\n[`pgsodium`](https://github.com/michelp/pgsodium) is a Postgres extension which provides SQL access to [`libsodium`'s](https://doc.libsodium.org/) high-level cryptographic algorithms.\n\nSupabase previously documented two features derived from pgsodium. Namely [Server Key Management](https://github.com/michelp/pgsodium#server-key-management) and [Transparent Column Encryption](https://github.com/michelp/pgsodium#transparent-column-encryption). At this time, we do not recommend using either on the Supabase platform due to their high level of operational complexity and misconfiguration risk.\n\nNote that Supabase projects are encrypted at rest by default which likely is sufficient for your compliance needs e.g. SOC2 & HIPAA."
        },
        {
          "type": "section",
          "title": "Get the root encryption key for your Supabase project",
          "content": "Encryption requires keys. Keeping the keys in the same database as the encrypted data would be unsafe. For more information about managing the `pgsodium` root encryption key on your Supabase project see **[encryption key location](/docs/guides/database/vault#encryption-key-location)**. This key is required to decrypt values stored in [Supabase Vault](/docs/guides/database/vault) and data encrypted with Transparent Column Encryption.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Supabase Vault](/docs/guides/database/vault)\n- Read more about Supabase Vault in the [blog post](https://supabase.com/blog/vault-now-in-beta)\n- [Supabase Vault on GitHub](https://github.com/supabase/vault)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pgsodium` documentation](https://github.com/michelp/pgsodium)",
          "level": 2
        }
      ],
      "wordCount": 232,
      "characterCount": 2117
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgtap",
      "identifier": "database-extensions-pgtap",
      "name": "pgTAP: Unit Testing",
      "description": "Unit testing in PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgtap",
      "dateModified": "2025-06-13T12:45:11.283688",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgtap.mdx",
      "frontmatter": {
        "id": "pgtap",
        "title": "pgTAP: Unit Testing",
        "description": "Unit testing in PostgreSQL."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "`pgTAP` is a unit testing extension for Postgres."
        },
        {
          "type": "section",
          "title": "Overview",
          "content": "Let's cover some basic concepts:\n\n- Unit tests: allow you to test small parts of a system (like a database table!).\n- TAP: stands for [Test Anything Protocol](http://testanything.org/). It is an framework which aims to simplify the error reporting during testing.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `pgtap` and enable the extension.\n\n```sql\n-- Enable the \"pgtap\" extension\ncreate extension pgtap with schema extensions;\n\n-- Disable the \"pgtap\" extension\ndrop extension if exists pgtap;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Testing tables",
          "content": "```sql\nbegin;\nselect plan( 1 );\n\nselect has_table( 'profiles' );\n\nselect * from finish();\nrollback;\n```\n\nAPI:\n\n- [`has_table()`](https://pgtap.org/documentation.html#has_table): Tests whether or not a table exists in the database\n- [`has_index()`](https://pgtap.org/documentation.html#has_index): Checks for the existence of a named index associated with the named table.\n- [`has_relation()`](https://pgtap.org/documentation.html#has_relation): Tests whether or not a relation exists in the database.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Testing columns",
          "content": "```sql\nbegin;\nselect plan( 2 );\n\nselect has_column( 'profiles', 'id' ); -- test that the \"id\" column exists in the \"profiles\" table\nselect col_is_pk( 'profiles', 'id' ); -- test that the \"id\" column is a primary key\n\nselect * from finish();\nrollback;\n```\n\nAPI:\n\n- [`has_column()`](https://pgtap.org/documentation.html#has_column): Tests whether or not a column exists in a given table, view, materialized view or composite type.\n- [`col_is_pk()`](https://pgtap.org/documentation.html#col_is_pk): Tests whether the specified column or columns in a table is/are the primary key for that table.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Testing RLS policies",
          "content": "```sql\nbegin;\nselect plan( 1 );\n\nselect policies_are(\n 'public',\n 'profiles',\n ARRAY [\n 'Profiles are public', -- Test that there is a policy called \"Profiles are public\" on the \"profiles\" table.\n 'Profiles can only be updated by the owner' -- Test that there is a policy called \"Profiles can only be updated by the owner\" on the \"profiles\" table.\n ]\n);\n\nselect * from finish();\nrollback;\n```\n\nAPI:\n\n- [`policies_are()`](https://pgtap.org/documentation.html#policies_are): Tests that all of the policies on the named table are only the policies that should be on that table.\n- [`policy_roles_are()`](https://pgtap.org/documentation.html#policy_roles_are): Tests whether the roles to which policy applies are only the roles that should be on that policy.\n- [`policy_cmd_is()`](https://pgtap.org/documentation.html#policy_cmd_is): Tests whether the command to which policy applies is same as command that is given in function arguments.\n\nYou can also use the `results_eq()` method to test that a Policy returns the correct data:\n\n```sql\nbegin;\nselect plan( 1 );\n\nselect results_eq(\n 'select * from profiles()',\n $$VALUES ( 1, 'Anna'), (2, 'Bruce'), (3, 'Caryn')$$,\n 'profiles() should return all users'\n);\n\nselect * from finish();\nrollback;\n```\n\nAPI:\n\n- [`results_eq()`](https://pgtap.org/documentation.html#results_eq)\n- [`results_ne()`](https://pgtap.org/documentation.html#results_ne)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Testing functions",
          "content": "```sql\nprepare hello_expr as select 'hello'\n\nbegin;\nselect plan(3);\n-- You'll need to create a hello_world and is_even function\nselect function_returns( 'hello_world', 'text' ); -- test if the function \"hello_world\" returns text\nselect function_returns( 'is_even', ARRAY['integer'], 'boolean' ); -- test if the function \"is_even\" returns a boolean\nselect results_eq('select * from hello_world()', 'hello_expr'); -- test if the function \"hello_world\" returns \"hello\"\n\nselect * from finish();\nrollback;\n```\n\nAPI:\n\n- [`function_returns()`](https://pgtap.org/documentation.html#function_returns): Tests that a particular function returns a particular data type\n- [`is_definer()`](https://pgtap.org/documentation.html#is_definer): Tests that a function is a security definer (that is, a `setuid` function).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`pgTAP` documentation](https://pgtap.org/)",
          "level": 2
        }
      ],
      "wordCount": 580,
      "characterCount": 4425
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-pgvector",
      "identifier": "database-extensions-pgvector",
      "name": "pgvector: Embeddings and vector similarity",
      "description": "pgvector: a PostgreSQL extension for storing embeddings and performing vector similarity search.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/pgvector",
      "dateModified": "2025-06-13T12:45:11.283870",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/pgvector.mdx",
      "frontmatter": {
        "id": "pgvector",
        "title": "pgvector: Embeddings and vector similarity",
        "description": "pgvector: a PostgreSQL extension for storing embeddings and performing vector similarity search."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[pgvector](https://github.com/pgvector/pgvector/) is a Postgres extension for vector similarity search. It can also be used for storing [embeddings](https://supabase.com/blog/openai-embeddings-postgres-vector).\n\nThe name of pgvector's Postgres extension is [vector](https://github.com/pgvector/pgvector/blob/258eaf58fdaff1843617ff59ea855e0768243fe9/README.md?plain=1#L64).\n\nLearn more about Supabase's [AI & Vector](/docs/guides/ai) offering."
        },
        {
          "type": "section",
          "title": "Vector similarity",
          "content": "Vector similarity refers to a measure of the similarity between two related items. For example, if you have a list of products, you can use vector similarity to find similar products. To do this, you need to convert each product into a \"vector\" of numbers, using a mathematical model. You can use a similar model for text, images, and other types of data. Once all of these vectors are stored in the database, you can use vector similarity to find similar items.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Embeddings",
          "content": "This is particularly useful if you're building on top of OpenAI's [GPT-3](https://openai.com/blog/gpt-3-apps/). You can create and store [embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings) for retrieval augmented generation.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"vector\" and enable the extension.\n\n```sql\n -- Example: enable the \"vector\" extension.\ncreate extension vector\nwith\n schema extensions;\n\n-- Example: disable the \"vector\" extension\ndrop\n extension if exists vector;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension, call `drop extension`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a table to store vectors",
          "content": "```sql\ncreate table posts (\n id serial primary key,\n title text not null,\n body text not null,\n embedding vector(384)\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Storing a vector / embedding",
          "content": "In this example we'll generate a vector using Transformer.js, then store it in the database using the Supabase client.\n\n```js\nimport { pipeline } from '@xenova/transformers'\nconst generateEmbedding = await pipeline('feature-extraction', 'Supabase/gte-small')\n\nconst title = 'First post!'\nconst body = 'Hello world!'\n\n// Generate a vector using Transformers.js\nconst output = await generateEmbedding(body, {\n pooling: 'mean',\n normalize: true,\n})\n\n// Extract the embedding output\nconst embedding = Array.from(output.data)\n\n// Store the vector in Postgres\nconst { data, error } = await supabase.from('posts').insert({\n title,\n body,\n embedding,\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Queries with filtering",
          "content": "If you use an IVFFlat or HNSW index and naively filter the results based on the value of another column, you may get fewer rows returned than requested.\n\nFor example, the following query may return fewer than 5 rows, even if 5 corresponding rows exist in the database. This is because the embedding index may not return 5 rows matching the filter.\n\n```\nSELECT * FROM items WHERE category_id = 123 ORDER BY embedding '[3,1,2]' LIMIT 5;\n```\n\nTo get the exact number of requested rows, use [iterative search](https://github.com/pgvector/pgvector/?tab=readme-ov-file#iterative-index-scans) to continue scanning the index until enough results are found.",
          "level": 3
        },
        {
          "type": "section",
          "title": "More pgvector and Supabase resources",
          "content": "- [Supabase Clippy: ChatGPT for Supabase Docs](https://supabase.com/blog/chatgpt-supabase-docs)\n- [Storing OpenAI embeddings in Postgres with pgvector](https://supabase.com/blog/openai-embeddings-postgres-vector)\n- [A ChatGPT Plugins Template built with Supabase Edge Runtime](https://supabase.com/blog/building-chatgpt-plugins-template)\n- [Template for building your own custom ChatGPT style doc search](https://github.com/supabase-community/nextjs-openai-doc-search)",
          "level": 2
        }
      ],
      "wordCount": 494,
      "characterCount": 3837
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-plpgsql_check",
      "identifier": "database-extensions-plpgsql_check",
      "name": "plpgsql_check: PL/pgSQL Linter",
      "description": "Lint PL/pgSQL code",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/plpgsql_check",
      "dateModified": "2025-06-13T12:45:11.283984",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/plpgsql_check.mdx",
      "frontmatter": {
        "id": "plpgsql_check",
        "title": "plpgsql_check: PL/pgSQL Linter",
        "description": "Lint PL/pgSQL code"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[plpgsql_check](https://github.com/okbob/plpgsql_check) is a Postgres extension that lints plpgsql for syntax, semantic and other related issues. The tool helps developers to identify and correct errors before executing the code. plpgsql_check is most useful for developers who are working with large or complex SQL codebases, as it can help identify and resolve issues early in the development cycle."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"plpgsql_check\" and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"plpgsql_check\" extension\ncreate extension plpgsql_check;\n\n-- Disable the \"plpgsql_check\" extension\ndrop extension if exists plpgsql_check;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension you can call `drop extension`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "API",
          "content": "- [`plpgsql_check_function( ... )`](https://github.com/okbob/plpgsql_check#active-mode): Scans a function for errors.\n\n`plpgsql_check_function` is highly customizable. For a complete list of available arguments see [the docs](https://github.com/okbob/plpgsql_check#arguments)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "To demonstrate `plpgsql_check` we can create a function with a known error. In this case we create a function `some_func`, that references a non-existent column `place.created_at`.\n\n{/* prettier-ignore */}\n```sql\ncreate table place(\n x float,\n y float\n);\n\ncreate or replace function public.some_func()\n returns void\n language plpgsql\nas $$\ndeclare\n rec record;\nbegin\n for rec in select * from place\n loop\n -- Bug: There is no column `created_at` on table `place`\n raise notice '%', rec.created_at;\n end loop;\nend;\n$$;\n```\n\nNote that executing the function would not catch the invalid reference error because the `loop` does not execute if no rows are present in the table.\n\n{/* prettier-ignore */}\n```sql\nselect public.some_func();\n some_func\n ───────────\n\n (1 row)\n```\n\nNow we can use plpgsql_check's `plpgsql_check_function` function to identify the known error.\n\n{/* prettier-ignore */}\n```sql\nselect plpgsql_check_function('public.some_func()');\n\n plpgsql_check_function\n------------------------------------------------------------\n error:42703:8:RAISE:record \"rec\" has no field \"created_at\"\n Context: SQL expression \"rec.created_at\"\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`plpgsql_check` documentation](https://github.com/okbob/plpgsql_check)",
          "level": 2
        }
      ],
      "wordCount": 313,
      "characterCount": 2513
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-plv8",
      "identifier": "database-extensions-plv8",
      "name": "plv8: JavaScript Language",
      "description": "JavaScript language for PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/plv8",
      "dateModified": "2025-06-13T12:45:11.284123",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/plv8.mdx",
      "frontmatter": {
        "id": "plv8",
        "title": "plv8: JavaScript Language",
        "description": "JavaScript language for PostgreSQL."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The `plv8` extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See the [Upgrading to Postgres 17 notes](/docs/guides/platform/upgrading#upgrading-to-postgres-17) for more information.\n\nThe `plv8` extension allows you use JavaScript within Postgres."
        },
        {
          "type": "section",
          "title": "Overview",
          "content": "While Postgres natively runs SQL, it can also run other procedural languages.\n`plv8` allows you to run JavaScript code - specifically any code that runs on the [V8 JavaScript engine](https://v8.dev).\n\nIt can be used for database functions, triggers, queries and more.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"plv8\" and enable the extension.\n\n```sql\n-- Example: enable the \"plv8\" extension\ncreate extension plv8;\n\n-- Example: disable the \"plv8\" extension\ndrop extension if exists plv8;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of enabling the extension.\nTo disable an extension, call `drop extension`.\n\nProcedural languages are automatically installed within `pg_catalog`, so you don't need to specify a schema.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create `plv8` functions",
          "content": "Functions written in `plv8` are written just like any other Postgres functions, only\nwith the `language` identifier set to `plv8`.\n\n```sql\ncreate or replace function function_name()\nreturns void as $$\n // V8 JavaScript\n // code\n // here\n$$ language plv8;\n```\n\nYou can call `plv8` functions like any other Postgres function:\n\n```sql\nselect function_name();\n```\n\n```js\nconst { data, error } = supabase.rpc('function_name')\n```\n\n```kotlin\nval data = supabase.postgrest.rpc(\"function_name\")\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Scalar functions",
          "content": "A [scalar function](https://plv8.github.io/#scalar-function-calls) is anything that takes in some user input and returns a single result.\n\n```sql\ncreate or replace function hello_world(name text)\nreturns text as $$\n\n let output = `Hello, ${name}!`;\n return output;\n\n$$ language plv8;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Executing SQL",
          "content": "You can execute SQL within `plv8` code using the [`plv8.execute` function](https://plv8.github.io/#plv8-execute).\n\n```sql\ncreate or replace function update_user(id bigint, first_name text)\nreturns smallint as $$\n\n var num_affected = plv8.execute(\n 'update profiles set first_name = $1 where id = $2',\n [first_name, id]\n );\n\n return num_affected;\n$$ language plv8;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set-returning functions",
          "content": "A [set-returning function](https://plv8.github.io/#set-returning-function-calls) is anything that returns a full set of results - for example, rows in a table.\n\n```sql\ncreate or replace function get_messages()\nreturns setof messages as $$\n\n var json_result = plv8.execute(\n 'select * from messages'\n );\n\n return json_result;\n$$ language plv8;\n\nselect * from get_messages();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`plv8` documentation](https://plv8.github.io/)\n- [plv8 GitHub Repository](https://github.com/plv8/plv8)",
          "level": 2
        }
      ],
      "wordCount": 412,
      "characterCount": 3066
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-postgis",
      "identifier": "database-extensions-postgis",
      "name": "PostGIS: Geo queries",
      "description": "Working with geo-spatial data in Postgres",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/postgis",
      "dateModified": "2025-06-13T12:45:11.284614",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/postgis.mdx",
      "frontmatter": {
        "id": "postgis",
        "title": "PostGIS: Geo queries",
        "description": "Working with geo-spatial data in Postgres",
        "tocVideo": "agFsGDJxjwA"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[PostGIS](https://postgis.net/) is a Postgres extension that allows you to interact with Geo data within Postgres. You can sort your data by geographic location, get data within certain geographic boundaries, and do much more with it."
        },
        {
          "type": "section",
          "title": "Overview",
          "content": "While you may be able to store simple lat/long geographic coordinates as a set of decimals, it does not scale very well when you try to query through a large data set. PostGIS comes with special data types that are efficient, and indexable for high scalability.\n\nThe additional data types that PostGIS provides include [Point](https://postgis.net/docs/using_postgis_dbmanagement.html#Point), [Polygon](https://postgis.net/docs/using_postgis_dbmanagement.html#Polygon), [LineString](https://postgis.net/docs/using_postgis_dbmanagement.html#LineString), and many more to represent different types of geographical data. In this guide, we will mainly focus on how to interact with `Point` type, which represents a single set of latitude and longitude. If you are interested in digging deeper, you can learn more about different data types on the [data management section of PostGIS docs](https://postgis.net/docs/using_postgis_dbmanagement.html).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "You can get started with PostGIS by enabling the PostGIS extension in your Supabase dashboard.\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `postgis` and enable the extension.\n4. In the confirmation prompt select \"Create a new schema\" and name it `gis` for example.\n\n```sql\n-- Create a dedicated separate schema\ncreate schema if not exists \"gis\";\n\n-- Example: enable the \"postgis\" extension\ncreate extension postgis with schema \"gis\";\n\n-- Example: disable the \"postgis\" extension\ndrop extension if exists postgis;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Examples",
          "content": "Now that we are ready to get started with PostGIS, let’s create a table and see how we can utilize PostGIS for some typical use cases. Let’s imagine we are creating a simple restaurant-searching app.\n\nLet’s create our table. Each row represents a restaurant with its location stored in `location` column as a `Point` type.\n\n```sql\ncreate table if not exists public.restaurants (\n id int generated by default as identity primary key,\n name text not null,\n location gis.geography(POINT) not null\n);\n```\n\nWe can then set a [spatial index](https://postgis.net/docs/using_postgis_dbmanagement.html#build-indexes) on the `location` column of this table.\n\n```sql\ncreate index restaurants_geo_index\n on public.restaurants\n using GIST (location);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inserting data",
          "content": "You can insert geographical data through SQL or through our API.\n\nRestaurants\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | name | location |\n| --- | ----------- | -------------------------------- |\n| 1 | Supa Burger | lat: 40.807416, long: -73.946823 |\n| 2 | Supa Pizza | lat: 40.807475, long: -73.94581 |\n| 3 | Supa Taco | lat: 40.80629, long: -73.945826 |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\n```sql\ninsert into public.restaurants\n (name, location)\nvalues\n ('Supa Burger', gis.st_point(-73.946823, 40.807416)),\n ('Supa Pizza', gis.st_point(-73.94581, 40.807475)),\n ('Supa Taco', gis.st_point(-73.945826, 40.80629));\n```\n\n```js\nconst { error } = await supabase.from('restaurants').insert([\n {\n name: 'Supa Burger',\n location: 'POINT(-73.946823 40.807416)',\n },\n {\n name: 'Supa Pizza',\n location: 'POINT(-73.94581 40.807475)',\n },\n {\n name: 'Supa Taco',\n location: 'POINT(-73.945826 40.80629)',\n },\n])\n```\n\n```dart\nawait supabase.from('restaurants').insert([\n {\n 'name': 'Supa Burger',\n 'location': 'POINT(-73.946823 40.807416)',\n },\n {\n 'name': 'Supa Pizza',\n 'location': 'POINT(-73.94581 40.807475)',\n },\n {\n 'name': 'Supa Taco',\n 'location': 'POINT(-73.945826 40.80629)',\n },\n]);\n```\n\n```swift\nstruct Restaurant: Codable {\n let name: String\n let location: String // You could also use a custom type with a custom `Encodable` conformance for convenience.\n}\n\ntry await supabase.from(\"restaurants\")\n .insert(\n [\n Restaurant(name: \"Supa Burger\", location: \"POINT(-73.946823 40.807416)\"),\n Restaurant(name: \"Supa Pizza\", location: \"POINT(-73.94581 40.807475)\"),\n Restaurant(name: \"Supa Taco\", location: \"POINT(-73.945826 40.80629)\"),\n ]\n )\n .execute()\n```\n\n```kotlin\n@Serializable\ndata class Restaurant(\n val name: String,\n val location: String //you could also use a custom type with a custom serializer for more type safety\n)\n```\n\n```kotlin\nval data = supabase.from(\"restaurants\").insert(listOf(\n Restaurant(\"Supa Burger\", \"POINT(-73.946823 40.807416)\"),\n Restaurant(\"Supa Pizza\", \"POINT(-73.94581 40.807475)\"),\n Restaurant(\"Supa Taco\", \"POINT(-73.945826 40.80629)\"),\n))\n```\n\nNotice the order in which you pass the latitude and longitude. Longitude comes first, and is because longitude represents the x-axis of the location. Another thing to watch for is when inserting data from the client library, there is no comma between the two values, just a single space.\n\nAt this point, if you go into your Supabase dashboard and look at the data, you will notice that the value of the `location` column looks something like this.\n\n```\n0101000020E6100000A4DFBE0E9C91614044FAEDEBC0494240\n```\n\nWe can query the `restaurants` table directly, but it will return the `location` column in the format you see above.\nWe will create [database functions](https://supabase.com/docs/guides/database/functions) so that we can use the [st_y()](https://postgis.net/docs/ST_Y.html) and [st_x()](https://postgis.net/docs/ST_X.html) function to convert it back to lat and long floating values.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Order by distance",
          "content": "Sorting datasets from closest to farthest, sometimes called nearest-neighbor sort, is a very common use case in Geo-queries. PostGIS can handle it with the use of the [``](https://postgis.net/docs/geometry_distance_knn.html) operator. `` operator returns the two-dimensional distance between two geometries and will utilize the spatial index when used within `order by` clause. You can create the following database function to sort the restaurants from closest to farthest by passing the current locations as parameters.\n\n```sql\ncreate or replace function nearby_restaurants(lat float, long float)\nreturns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float, dist_meters float)\nset search_path = ''\nlanguage sql\nas $$\n select id, name, gis.st_y(location::gis.geometry) as lat, gis.st_x(location::gis.geometry) as long, gis.st_distance(location, gis.st_point(long, lat)::gis.geography) as dist_meters\n from public.restaurants\n order by location operator(gis.) gis.st_point(long, lat)::gis.geography;\n$$;\n```\n\nBefore being able to call this function from our client we need to grant access to our `gis` schema:\n\n```sql\ngrant usage on schema gis to anon, authenticated;\n```\n\nNow you can call this function from your client using `rpc()` like this:\n\n```js\nconst { data, error } = await supabase.rpc('nearby_restaurants', {\n lat: 40.807313,\n long: -73.946713,\n})\n```\n\n```dart\nfinal data = await supabase.rpc('nearby_restaurants',params: {\n 'lat': 40.807313,\n 'long': -73.946713,\n});\n```\n\n```swift\nstruct Response: Codable {\n let id: Int\n let name: String\n let lat: Double\n let long: Double\n let distance: Double\n\n enum CodingKeys: String, CodingKey {\n case id, name, lat, long\n case distance = \"dist_meters\"\n }\n}\n\nlet response: Response = try await supabase.rpc(\n \"nearby_restaurants\",\n params: [\n \"lat\": 40.807313,\n \"long\": -73.946713\n ]\n)\n.execute()\n.value\n```\n\n```kotlin\nval data = supabase.postgrest.rpc(\n function = \"nearby_restaurants\",\n parameters = buildJsonObject { //You can put here any serializable object including your own classes\n put(\"lat\", 40.807313)\n put(\"lon\", -73.946713)\n }\n)\n```\n\n```json\n[\n {\n \"id\": 1,\n \"name\": \"Supa Burger\",\n \"lat\": 40.807416,\n \"long\": -73.946823,\n \"dist_meters\": 14.73033739\n },\n {\n \"id\": 2,\n \"name\": \"Supa Pizza\",\n \"lat\": 40.807475,\n \"long\": -73.94581,\n \"dist_meters\": 78.28980007\n },\n {\n \"id\": 3,\n \"name\": \"Supa Taco\",\n \"lat\": 40.80629,\n \"long\": -73.945826,\n \"dist_meters\": 136.04329002\n }\n]\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Finding all data points within a bounding box",
          "content": "![Searching within a bounding box of a map](/docs/img/guides/database/extensions/postgis/map.png)\n\nWhen you are working on a map-based application where the user scrolls through your map, you might want to load the data that lies within the bounding box of the map every time your users scroll. PostGIS can return the rows that are within the bounding box just by supplying the bottom left and the top right coordinates. Let’s look at what the function would look like:\n\n```sql\ncreate or replace function restaurants_in_view(min_lat float, min_long float, max_lat float, max_long float)\nreturns table (id public.restaurants.id%TYPE, name public.restaurants.name%TYPE, lat float, long float)\nset search_path to ''\nlanguage sql\nas $$\n select id, name, gis.st_y(location::gis.geometry) as lat, gis.st_x(location::gis.geometry) as long\n from public.restaurants\n where location operator(gis.&&) gis.ST_SetSRID(gis.ST_MakeBox2D(gis.ST_Point(min_long, min_lat), gis.ST_Point(max_long, max_lat)), 4326)\n$$;\n```\n\nThe [`&&`](https://postgis.net/docs/geometry_overlaps.html) operator used in the `where` statement here returns a boolean of whether the bounding box of the two geometries intersect or not. We are basically creating a bounding box from the two points and finding those points that fall under the bounding box. We are also utilizing a few different PostGIS functions:\n\n- [ST_MakeBox2D](https://postgis.net/docs/ST_MakeBox2D.html): Creates a 2-dimensional box from two points.\n- [ST_SetSRID](https://postgis.net/docs/ST_SetSRID.html): Sets the [SRID](https://postgis.net/docs/manual-dev/using_postgis_dbmanagement.html#spatial_ref_sys), which is an identifier of what coordinate system to use for the geometry. 4326 is the standard longitude and latitude coordinate system.\n\nYou can call this function from your client using `rpc()` like this:\n\n```js\nconst { data, error } = await supabase.rpc('restaurants_in_view', {\n min_lat: 40.807,\n min_long: -73.946,\n max_lat: 40.808,\n max_long: -73.945,\n})\n```\n\n```dart\nfinal data = await supabase.rpc('restaurants_in_view', params: {\n 'min_lat': 40.807,\n 'min_long': -73.946,\n 'max_lat': 40.808,\n 'max_long': -73.945,\n});\n```\n\n```swift\nstruct Response: Codable {\n let id: Int\n let name: String\n let lat: Double\n let long: Double\n}\n\nlet response: Response = try await supabase.rpc(\n \"restaurants_in_view\",\n params: [\n \"min_lat\": 40.807,\n \"min_long\": -73.946,\n \"max_long\": -73.945,\n \"max_lat\": 40.808,\n ]\n)\n.execute()\n.value\n```\n\n```kotlin\nval data = supabase.postgrest.rpc(\n function = \"restaurants_in_view\",\n parameters = buildJsonObject { //You can put here any serializable object including your own classes\n put(\"min_lat\", 40.807)\n put(\"min_lon\", -73.946)\n put(\"max_lat\", 40.808)\n put(\"max_lon\", -73.945)\n }\n)\n```\n\n```json\n[\n {\n \"id\": 2,\n \"name\": \"Supa Pizza\",\n \"lat\": 40.807475,\n \"long\": -73.94581\n }\n]\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Troubleshooting",
          "content": "The [official PostGIS documentation](https://postgis.net/documentation/tips/tip-move-postgis-schema/) for relocating the schema will cause issues for Supabase projects. These issues might not be apparent immediately but will eventually surface. To relocate your schema, use the following steps instead.\n\nAs of PostGIS 2.3 or newer, the PostGIS extension is no longer relocatable from one schema to another. If you need to move it from one schema to another for any reason (e.g. from the public schema to the extensions schema for security reasons), you would normally run a ALTER EXTENSION to relocate the schema. However, you will now to do the following steps:\n\n1. Backup your Database to prevent data loss - You can do this through the [CLI](https://supabase.com/docs/reference/cli/supabase-db-dump) or Postgres backup tools such as [pg_dumpall](https://www.postgresql.org/docs/current/backup-dump.html#BACKUP-DUMP-ALL)\n\n2. Drop all dependencies you created and the PostGIS extension - `DROP EXTENSION postgis CASCADE;`\n\n3. Enable PostGIS extension in the new schema - `CREATE EXTENSION postgis SCHEMA extensions;`\n\n4. Restore dropped data via the Backup if necessary from step 1 with your tool of choice.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official PostGIS documentation](https://postgis.net/documentation/)",
          "level": 2
        }
      ],
      "wordCount": 1603,
      "characterCount": 12345
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-postgres_fdw",
      "identifier": "database-extensions-postgres_fdw",
      "name": "postgres_fdw",
      "description": "Query Postgres server from another",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/postgres_fdw",
      "dateModified": "2025-06-13T12:45:11.284784",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/postgres_fdw.mdx",
      "frontmatter": {
        "id": "postgres_fdw",
        "title": "postgres_fdw",
        "description": "Query Postgres server from another"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The extension enables Postgres to query tables and views on a remote Postgres server."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](/dashboard/project/_/database/tables) page in the Dashboard.\n 2. Click on **Extensions** in the sidebar.\n 3. Search for \"postgres_fdw\" and enable the extension.\n\n ```sql\n -- Example: enable the \"postgres_fdw\" extension\n create extension if not exists postgres_fdw;\n\n -- Example: disable the \"postgres_fdw\" extension\n drop extension if exists postgres_fdw;\n ```\n\n Procedural languages are automatically installed within `pg_catalog`, so you don't need to specify a schema.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create a connection to another database",
          "content": "Define the remote database address\n\n ```sql\n create server \"\"\n foreign data wrapper postgres_fdw\n options (\n host '',\n port '',\n dbname ''\n );\n ```\n\n Set the user credentials for the remote server\n\n ```sql\n create user mapping for \"\"\n server \"\"\n options (\n user '',\n password ''\n );\n ```\n\n Import tables from the foreign database\n\n Example: Import all tables from a schema\n ```sql\n import foreign schema \"\"\n from server \"\"\n into \"\";\n ```\n\n Example: Import specific tables\n ```sql\n import foreign schema \"\"\n limit to (\n \"\",\n \"\"\n )\n from server \"\"\n into \"\";\n ```\n\n ```sql\n select * from \"\"\n ```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Fetch_size",
          "content": "Maximum rows fetched per operation. For example, fetching 200 rows with `fetch_size` set to 100 requires 2 requests.\n\n```sql\nalter server \"\"\noptions (fetch_size '10000');\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Batch_size",
          "content": "Maximum rows inserted per cycle. For example, inserting 200 rows with `batch_size` set to 100 requires 2 requests.\n\n```sql\nalter server \"\"\noptions (batch_size '1000');\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Extensions",
          "content": "Lists shared extensions. Without them, queries involving unlisted extension functions or operators may fail or omit references.\n\n```sql\nalter server \"\"\noptions (extensions 'vector, postgis');\n```\n\nFor more server options, check the extension's [official documentation](https://www.postgresql.org/docs/current/postgres-fdw.html#POSTGRES-FDW)",
          "level": 4
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`postgres_fdw` documentation](https://www.postgresql.org/docs/current/postgres-fdw.html#POSTGRES-FDW)",
          "level": 2
        }
      ],
      "wordCount": 290,
      "characterCount": 2153
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-rum",
      "identifier": "database-extensions-rum",
      "name": "RUM: improved inverted index for full-text search based on GIN index",
      "description": "A GIN-like index with additional tree-organized data for each index entry",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/rum",
      "dateModified": "2025-06-13T12:45:11.284950",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/rum.mdx",
      "frontmatter": {
        "id": "rum",
        "title": "RUM: improved inverted index for full-text search based on GIN index",
        "description": "A GIN-like index with additional tree-organized data for each index entry"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[RUM](https://github.com/postgrespro/rum) is an extension which adds a RUM index to Postgres.\n\nRUM index is based on GIN that stores additional per-entry information in a posting tree. For example, positional information of lexemes or timestamps. In comparison to GIN it can use this information to make faster index-only scans for:\n\n- Phrase search\n- Text search with ranking by text distance operator\n- Text `SELECT`s with ordering by some non-indexed additional column e.g. by timestamp.\n\nRUM works best in scenarios when the possible keys are highly repeatable. I.e. all texts are composed of a\nlimited amount of words, so per-lexeme indexing gives significant speed-up in searching texts containing word\ncombinations or phrases.\n\nMain operators for ordering are:\n\n`tsvector` `` `tsquery` | `float4` | Distance between `tsvector` and `tsquery`.\nvalue `` value | `float8` | Distance between two values.\n\nWhere value is `timestamp`, `timestamptz`, `int2`, `int4`, `int8`, `float4`, `float8`, `money` and `oid`"
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "You can get started with rum by enabling the extension in your Supabase dashboard.\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"rum\" and enable the extension.\n\n```sql\n-- Example: enable the \"rum\" extension\ncreate extension rum with schema extensions;\n\n-- Example: disable the \"rum\" extension\ndrop extension if exists rum;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "For type: `tsvector`",
          "content": "To understand the following you may need first to see [Official Postgres documentation on text\nsearch](https://www.postgresql.org/docs/current/functions-textsearch.html)\n\n`rum_tsvector_ops`\n\n```sql\nCREATE TABLE test_rum(t text, a tsvector);\n\nCREATE TRIGGER tsvectorupdate\nBEFORE UPDATE OR INSERT ON test_rum\nFOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger('a', 'pg_catalog.english', 't');\n\nINSERT INTO test_rum(t) VALUES ('The situation is most beautiful');\nINSERT INTO test_rum(t) VALUES ('It is a beautiful');\nINSERT INTO test_rum(t) VALUES ('It looks like a beautiful place');\n\nCREATE INDEX rumidx ON test_rum USING rum (a rum_tsvector_ops);\n```\n\nAnd we can execute `tsvector` selects with ordering by text distance operator:\n\n```sql\nSELECT t, a `` to_tsquery('english', 'beautiful | place') AS rank\n FROM test_rum\n WHERE a @@ to_tsquery('english', 'beautiful | place')\n ORDER BY a `` to_tsquery('english', 'beautiful | place');\n t | rank\n---------------------------------+---------\n It looks like a beautiful place | 8.22467\n The situation is most beautiful | 16.4493\n It is a beautiful | 16.4493\n(3 rows)\n```\n\n`rum_tsvector_addon_ops`\n\n```sql\nCREATE TABLE tsts (id int, t tsvector, d timestamp);\nCREATE INDEX tsts_idx ON tsts USING rum (t rum_tsvector_addon_ops, d)\n WITH (attach = 'd', to = 't');\n```\n\nNow we can execute the selects with ordering distance operator on attached column:\n\n```sql\nSELECT id, d, d `` '2016-05-16 14:21:25' FROM tsts WHERE t @@ 'wr&qh' ORDER BY d `` '2016-05-16 14:21:25' LIMIT 5;\n id | d | ?column?\n-----+---------------------------------+---------------\n 355 | Mon May 16 14:21:22.326724 2016 | 2.673276\n 354 | Mon May 16 13:21:22.326724 2016 | 3602.673276\n 371 | Tue May 17 06:21:22.326724 2016 | 57597.326724\n 406 | Wed May 18 17:21:22.326724 2016 | 183597.326724\n 415 | Thu May 19 02:21:22.326724 2016 | 215997.326724\n(5 rows)\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "For type: `anyarray`",
          "content": "`rum_anyarray_ops`\n\nThis operator class stores `anyarray` elements with length of the array. It supports operators `&&`, `@>`, `` operator.\n\n```sql\nCREATE TABLE test_array (i int2[]);\nINSERT INTO test_array VALUES ('{}'), ('{0}'), ('{1,2,3,4}'), ('{1,2,3}'), ('{1,2}'), ('{1}');\nCREATE INDEX idx_array ON test_array USING rum (i rum_anyarray_ops);\n```\n\nNow we can execute the query using index scan:\n\n```sql\nSELECT * FROM test_array WHERE i && '{1}' ORDER BY i `` '{1}' ASC;\n i\n-----------\n {1}\n {1,2}\n {1,2,3}\n {1,2,3,4}\n(4 rows)\n```\n\n`rum_anyarray_addon_ops`\n\nThe does the same with `anyarray` index as `rum_tsvector_addon_ops` i.e. allows to order select results using distance\noperator by attached column.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Limitations",
          "content": "`RUM` has slower build and insert times than `GIN` due to:\n\n1. It is bigger due to the additional attributes stored in the index.\n2. It uses generic WAL records.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official RUM documentation](https://github.com/postgrespro/rum)",
          "level": 2
        }
      ],
      "wordCount": 631,
      "characterCount": 4408
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-timescaledb",
      "identifier": "database-extensions-timescaledb",
      "name": "timescaledb: Time-Series data",
      "description": "Scalable time-series data storage and analysis",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/timescaledb",
      "dateModified": "2025-06-13T12:45:11.285104",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/timescaledb.mdx",
      "frontmatter": {
        "id": "timescaledb",
        "title": "timescaledb: Time-Series data",
        "description": "Scalable time-series data storage and analysis"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The `timescaledb` extension is deprecated in projects using Postgres 17. It continues to be supported in projects using Postgres 15, but will need to dropped before those projects are upgraded to Postgres 17. See the [Upgrading to Postgres 17 notes](/guides/platform/upgrading#upgrading-to-postgres-17) for more information.\n\n[`timescaledb`](https://docs.timescale.com/timescaledb/latest/) is a Postgres extension designed for improved handling of time-series data. It provides a scalable, high-performance solution for storing and querying time-series data on top of a standard Postgres database.\n\n`timescaledb` uses a time-series-aware storage model and indexing techniques to improve performance of Postgres in working with time-series data. The extension divides data into chunks based on time intervals, allowing it to scale efficiently, especially for large data sets. The data is then compressed, optimized for write-heavy workloads, and partitioned for parallel processing. `timescaledb` also includes a set of functions, operators, and indexes that work with time-series data to reduce query times, and make data easier to work with.\n\nSupabase projects come with [TimescaleDB Apache 2 Edition](https://docs.timescale.com/about/latest/timescaledb-editions/#timescaledb-apache-2-edition). Functionality only available under the Community Edition is not available."
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `timescaledb` and enable the extension.\n\n{/* prettier-ignore */}\n```sql\n-- Enable the \"timescaledb\" extension\ncreate extension timescaledb with schema extensions;\n\n-- Disable the \"timescaledb\" extension\ndrop extension if exists timescaledb;\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\". To disable an extension you can call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep your `public` schema clean.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "To demonstrate how `timescaledb` works, let's consider a simple example where we have a table that stores temperature data from different sensors. We will create a table named \"temperatures\" and store data for two sensors.\n\nFirst we create a hypertable, which is a virtual table that is partitioned into chunks based on time intervals. The hypertable acts as a proxy for the actual table and makes it easy to query and manage time-series data.\n\n{/* prettier-ignore */}\n```sql\ncreate table temperatures (\n time timestamptz not null,\n sensor_id int not null,\n temperature double precision not null\n);\n\nselect create_hypertable('temperatures', 'time');\n```\n\nNext, we can populate some values\n\n{/* prettier-ignore */}\n```sql\ninsert into temperatures (time, sensor_id, temperature)\nvalues\n ('2023-02-14 09:00:00', 1, 23.5),\n ('2023-02-14 09:00:00', 2, 21.2),\n ('2023-02-14 09:05:00', 1, 24.5),\n ('2023-02-14 09:05:00', 2, 22.3),\n ('2023-02-14 09:10:00', 1, 25.1),\n ('2023-02-14 09:10:00', 2, 23.9),\n ('2023-02-14 09:15:00', 1, 24.9),\n ('2023-02-14 09:15:00', 2, 22.7),\n ('2023-02-14 09:20:00', 1, 24.7),\n ('2023-02-14 09:20:00', 2, 23.5);\n```\n\nAnd finally we can query the table using `timescaledb`'s `time_bucket` function to divide the time-series into intervals of the specified size (in this case, 1 hour) averaging the `temperature` reading within each group.\n\n{/* prettier-ignore */}\n```sql\nselect\n time_bucket('1 hour', time) AS hour,\n avg(temperature) AS average_temperature\nfrom\n temperatures\nwhere\n sensor_id = 1\n and time > NOW() - interval '1 hour'\ngroup by\n hour;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`timescaledb` documentation](https://docs.timescale.com/timescaledb/latest/)",
          "level": 2
        }
      ],
      "wordCount": 499,
      "characterCount": 3775
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-uuid-ossp",
      "identifier": "database-extensions-uuid-ossp",
      "name": "uuid-ossp: Unique Identifiers",
      "description": "A UUID generator for PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/uuid-ossp",
      "dateModified": "2025-06-13T12:45:11.285221",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/uuid-ossp.mdx",
      "frontmatter": {
        "id": "uuid-ossp",
        "title": "uuid-ossp: Unique Identifiers",
        "description": "A UUID generator for PostgreSQL."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The `uuid-ossp` extension can be used to generate a `UUID`."
        },
        {
          "type": "section",
          "title": "Overview",
          "content": "A `UUID` is a \"Universally Unique Identifier\" and it is, for practical purposes, unique.\nThis makes them particularly well suited as Primary Keys. It is occasionally referred to as a `GUID`, which stands for \"Globally Unique Identifier\".",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "**Note**:\nCurrently `uuid-ossp` extension is enabled by default and cannot be disabled.\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for `uuid-ossp` and enable the extension.\n\n```sql\n-- Example: enable the \"uuid-ossp\" extension\ncreate extension \"uuid-ossp\" with schema extensions;\n\n-- Example: disable the \"uuid-ossp\" extension\ndrop extension if exists \"uuid-ossp\";\n```\n\nEven though the SQL code is `create extension`, this is the equivalent of \"enabling the extension\".\nTo disable an extension, call `drop extension`.\n\nIt's good practice to create the extension within a separate schema (like `extensions`) to keep the `public` schema clean.\n\n**Note**:\nCurrently `uuid-ossp` extension is enabled by default and cannot be disabled.",
          "level": 2
        },
        {
          "type": "section",
          "title": "The `uuid` type",
          "content": "Once the extension is enabled, you now have access to a `uuid` type.",
          "level": 2
        },
        {
          "type": "section",
          "title": "`uuid_generate_v1()`",
          "content": "Creates a UUID value based on the combination of computer’s MAC address, current timestamp, and a random value.\n\nUUIDv1 leaks identifiable details, which might make it unsuitable for certain security-sensitive applications.",
          "level": 2
        },
        {
          "type": "section",
          "title": "`uuid_generate_v4()`",
          "content": "Creates UUID values based solely on random numbers. You can also use Postgres's built-in [`gen_random_uuid()`](https://www.postgresql.org/docs/current/functions-uuid.html) function to generate a UUIDv4.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Within a query",
          "content": "```sql\nselect uuid_generate_v4();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "As a primary key",
          "content": "Automatically create a unique, random ID in a table:\n\n```sql\ncreate table contacts (\n id uuid default uuid_generate_v4(),\n first_name text,\n last_name text,\n primary key (id)\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Choosing a Postgres Primary Key](https://supabase.com/blog/choosing-a-postgres-primary-key)\n- [The Basics Of Postgres `UUID` Data Type](https://www.postgresqltutorial.com/postgresql-uuid/)",
          "level": 2
        }
      ],
      "wordCount": 298,
      "characterCount": 2236
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-extensions-wrappers-overview",
      "identifier": "database-extensions-wrappers-overview",
      "name": "Foreign Data Wrappers",
      "description": "Connecting to external systems using Postgres Foreign Data Wrappers.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/extensions/wrappers/overview",
      "dateModified": "2025-06-13T12:45:11.285494",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/extensions/wrappers/overview.mdx",
      "frontmatter": {
        "id": "foreign-data-wrappers",
        "title": "Foreign Data Wrappers",
        "description": "Connecting to external systems using Postgres Foreign Data Wrappers.",
        "subtitle": "Connecting to external systems using Postgres Foreign Data Wrappers."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Foreign Data Wrappers (FDW) are a core feature of Postgres that allow you to access and query data stored in external data sources as if they were native Postgres tables.\n\nPostgres includes several built-in foreign data wrappers, such as [`postgres_fdw`](https://www.postgresql.org/docs/current/postgres-fdw.html) for accessing other Postgres databases, and [`file_fdw`](https://www.postgresql.org/docs/current/file-fdw.html) for reading data from files. Supabase extends this feature to query other databases or any other external systems. We do this with our open source [Wrappers](https://github.com/supabase/wrappers) framework. In these guides we'll refer to them as \"Wrappers\", Foreign Data Wrappers, or FDWs. They are conceptually the same thing."
        },
        {
          "type": "section",
          "title": "Concepts",
          "content": "Wrappers introduce some new terminology and different workflows.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Remote servers",
          "content": "A Remote Server is an external database, API, or any system containing data that you want to query from your Postgres database. Examples include:\n\n- An external database, like Postgres or Firebase.\n- A remote data warehouse, like ClickHouse, BigQuery, or Snowflake.\n- An API, like Stripe or GitHub.\n\nIt's possible to connect to multiple remote servers of the same type. For example, you can connect to two different Firebase projects within the same Supabase database.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Foreign tables",
          "content": "A table in your database which maps to some data inside a Remote Server.\n\nExamples:\n\n- An `analytics` table which maps to a table inside your data warehouse.\n- A `subscriptions` table which maps to your Stripe subscriptions.\n- A `collections` table which maps to a Firebase collection.\n\nAlthough a foreign table behaves like any other table, the data is not stored inside your database. The data remains inside the Remote Server.",
          "level": 3
        },
        {
          "type": "section",
          "title": "ETL with Wrappers",
          "content": "ETL stands for Extract, Transform, Load. It's an established process for moving data from one system to another. For example, it's common to move data from a production database to a data warehouse.\n\nThere are many popular ETL tools, such as [Fivetran](https://fivetran.com/) and [Airbyte](https://airbyte.io/).\n\nWrappers provide an alternative to these tools. You can use SQL to move data from one table to another:\n\n```sql\n-- Copy data from your production database to your\n-- data warehouse for the last 24 hours:\n\ninsert into warehouse.analytics\nselect * from public.analytics\nwhere ts > (now() - interval '1 DAY');\n```\n\nThis approach provides several benefits:\n\n1. **Simplicity:** the Wrappers API is just SQL, so data engineers don't need to learn new tools and languages.\n1. **Save on time:** avoid setting up additional data pipelines.\n1. **Save on Data Engineering costs:** less infrastructure to be managed.\n\nOne disadvantage is that Wrappers are not as feature-rich as ETL tools. They also couple the ETL process to your database.",
          "level": 3
        },
        {
          "type": "section",
          "title": "On-demand ETL with Wrappers",
          "content": "Supabase extends the ETL concept with real-time data access. Instead of moving gigabytes of data from one system to another before you can query it, you can instead query the data directly from the remote server. This additional option, \"Query\", extends the ETL process and is called [QETL](https://www.sciencedirect.com/science/article/abs/pii/S0169023X1730438X) (pronounced \"kettle\"): Query, Extract, Transform, Load.\n\n{/* prettier-ignore */}\n```sql\n-- Get all purchases for a user from your data warehouse:\nselect\n auth.users.id as user_id,\n warehouse.orders.id as order_id\nfrom\n warehouse.orders\njoin \n auth.users on auth.users.id = warehouse.orders.user_id\nwhere \n auth.users.id = '';\n```\n\nThis approach has several benefits:\n\n1. **On-demand:** analytical data is immediately available within your application with no additional infrastructure.\n1. **Always in sync:** since the data is queried directly from the remote server, it's always up-to-date.\n1. **Integrated:** large datasets are available within your application, and can be joined with your operational/transactional data.\n1. **Save on bandwidth:** only extract/load what you need.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Batch ETL with Wrappers",
          "content": "A common use case for Wrappers is to extract data from a production database and load it into a data warehouse. This can be done within your database using [pg_cron](/docs/guides/database/extensions/pg_cron). For example, you can schedule a job to run every night to extract data from your production database and load it into your data warehouse.\n\n```sql\n-- Every day at 3am, copy data from your\n-- production database to your data warehouse:\nselect cron.schedule(\n 'nightly-etl',\n '0 3 * * *',\n $$\n insert into warehouse.analytics\n select * from public.analytics\n where ts > (now() - interval '1 DAY');\n $$\n);\n```\n\nThis process can be taxing on your database if you are moving large amounts of data. Often, it's better to use an external tool for batch ETL, such as [Fivetran](https://fivetran.com/) or [Airbyte](https://airbyte.io/).",
          "level": 3
        },
        {
          "type": "section",
          "title": "WebAssembly Wrappers",
          "content": "WebAssembly (Wasm) is a binary instruction format that enables high-performance execution of code on the web. Wrappers now includes a Wasm runtime, which provides a sandboxed execution environment, to run Wasm foreign data wrappers. Combined Wrappers with Wasm, developing and distributing new FDW becomes much easier and you can even build your own Wasm FDW and use it on Supabase platform.\n\nTo learn more about Wasm FDW, visit [Wrappers official documentation](https://supabase.github.io/wrappers/).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Security",
          "content": "Foreign Data Wrappers do not provide Row Level Security, thus it is not advised to expose them via your API. Wrappers should _always_ be stored in a private schema. For example, if you are connecting to your Stripe account, you should create a `stripe` schema to store all of your foreign tables inside. This schema should _not_ be added to the “Additional Schemas” setting in the API section.\n\nIf you want to expose any of the foreign table columns to your public API, you can create a [Database Function with security definer](https://supabase.com/docs/guides/database/functions#security-definer-vs-invoker) in the `public` schema, and then you can interact with your foreign table through API. For better access control, the function should have appropriate filters on the foreign table to apply security rules based on your business needs.\n\nAs an example, go to [SQL Editor](https://supabase.com/dashboard/project/_/sql/new) and then follow below steps,\n\n1. Create a Stripe Products foreign table:\n\n ```sql\n create foreign table stripe.stripe_products (\n id text,\n name text,\n active bool,\n default_price text,\n description text,\n created timestamp,\n updated timestamp,\n attrs jsonb\n )\n server stripe_fdw_server\n options (\n object 'products',\n rowid_column 'id'\n );\n ```\n\n2. Create a security definer function that queries the foreign table and filters on the name prefix parameter:\n\n ```sql\n create function public.get_stripe_products(name_prefix text)\n returns table (\n id text,\n name text,\n active boolean,\n default_price text,\n description text\n )\n language plpgsql\n security definer set search_path = ''\n as $$\n begin\n return query\n select\n t.id,\n t.name,\n t.active,\n t.default_price,\n t.description\n from\n stripe.stripe_products t\n where\n t.name like name_prefix || '%'\n ;\n end;\n $$;\n ```\n\n3. Restrict the function execution to a specific role only, for example, the authenticated users:\n\n By default, the function created can be executed by any roles like `anon`, that means the\n foreign table is public accessible. Always limit the function execution permission to\n appropriate roles.\n\n ```sql\n -- revoke public execute permission\n revoke execute on function public.get_stripe_products from public;\n revoke execute on function public.get_stripe_products from anon;\n\n -- grant execute permission to a specific role only\n grant execute on function public.get_stripe_products to authenticated;\n ```\n\nOnce the preceding steps are finished, the function can be invoked from Supabase client to query the foreign table:\n\n```js\nconst { data, error } = await supabase\n .rpc('get_stripe_products', { name_prefix: 'Test' })\n .select('*')\nif (error) console.error(error)\nelse console.log(data)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official [`supabase/wrappers` documentation](https://supabase.github.io/wrappers/)",
          "level": 2
        }
      ],
      "wordCount": 1172,
      "characterCount": 8229
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-full-text-search",
      "identifier": "database-full-text-search",
      "name": "Full Text Search",
      "description": "How to use full text search in PostgreSQL.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/full-text-search",
      "dateModified": "2025-06-13T12:45:11.286136",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/full-text-search.mdx",
      "frontmatter": {
        "id": "full-text-search",
        "title": "Full Text Search",
        "description": "How to use full text search in PostgreSQL.",
        "subtitle": "How to use full text search in PostgreSQL.",
        "tocVideo": "b-mgca_2Oe4"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres has built-in functions to handle `Full Text Search` queries. This is like a \"search engine\" within Postgres."
        },
        {
          "type": "section",
          "title": "Preparation",
          "content": "For this guide we'll use the following example data:\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description |\n| --- | ----------------------------------- | ---------------------- | ------------------------------------------------------------------ |\n| 1 | The Poky Little Puppy | Janette Sebring Lowrey | Puppy is slower than other, bigger animals. |\n| 2 | The Tale of Peter Rabbit | Beatrix Potter | Rabbit eats some vegetables. |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. |\n| 4 | Green Eggs and Ham | Dr. Seuss | Sam has changing food preferences and eats unusually colored food. |\n| 5 | Harry Potter and the Goblet of Fire | J.K. Rowling | Fourth year of school starts, big drama ensues. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\n```sql\ncreate table books (\n id serial primary key,\n title text,\n author text,\n description text\n);\n\ninsert into books\n (title, author, description)\nvalues\n (\n 'The Poky Little Puppy',\n 'Janette Sebring Lowrey',\n 'Puppy is slower than other, bigger animals.'\n ),\n ('The Tale of Peter Rabbit', 'Beatrix Potter', 'Rabbit eats some vegetables.'),\n ('Tootle', 'Gertrude Crampton', 'Little toy train has big dreams.'),\n (\n 'Green Eggs and Ham',\n 'Dr. Seuss',\n 'Sam has changing food preferences and eats unusually colored food.'\n ),\n (\n 'Harry Potter and the Goblet of Fire',\n 'J.K. Rowling',\n 'Fourth year of school starts, big drama ensues.'\n );\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Usage",
          "content": "The functions we'll cover in this guide are:",
          "level": 2
        },
        {
          "type": "section",
          "title": "`to_tsvector()` [#to-tsvector]",
          "content": "Converts your data into searchable tokens. `to_tsvector()` stands for \"to text search vector.\" For example:\n\n```sql\nselect to_tsvector('green eggs and ham');\n-- Returns 'egg':2 'green':1 'ham':4\n```\n\nCollectively these tokens are called a \"document\" which Postgres can use for comparisons.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`to_tsquery()` [#to-tsquery]",
          "content": "Converts a query string into tokens to match. `to_tsquery()` stands for \"to text search query.\"\n\nThis conversion step is important because we will want to \"fuzzy match\" on keywords.\nFor example if a user searches for `eggs`, and a column has the value `egg`, we probably still want to return a match.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Match: `@@` [#match]",
          "content": "The `@@` symbol is the \"match\" symbol for Full Text Search. It returns any matches between a `to_tsvector` result and a `to_tsquery` result.\n\nTake the following example:\n\n```sql\nselect *\nfrom books\nwhere title = 'Harry';\n```\n\n```js\nconst { data, error } = await supabase.from('books').select().eq('title', 'Harry')\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .eq('title', 'Harry');\n```\n\n```swift\nlet response = try await supabase.from(\"books\")\n .select()\n .eq(\"title\", value: \"Harry\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n eq(\"title\", \"Harry\")\n }\n}\n```\n\n```python\ndata = supabase.from_('books').select().eq('title', 'Harry').execute()\n```\n\nThe equality symbol above (`=`) is very \"strict\" on what it matches. In a full text search context, we might want to find all \"Harry Potter\" books and so we can rewrite the\nexample above:\n\n```sql\nselect *\nfrom books\nwhere to_tsvector(title) @@ to_tsquery('Harry');\n```\n\n```js\nconst { data, error } = await supabase.from('books').select().textSearch('title', `'Harry'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('title', \"'Harry'\");\n```\n\n```swift\nlet response = try await supabase.from(\"books\")\n .select()\n .textSearch(\"title\", value: \"'Harry'\")\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"title\", \"'Harry'\", TextSearchType.NONE)\n }\n}\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Search a single column",
          "content": "To find all `books` where the `description` contain the word `big`:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description)\n @@ to_tsquery('big');\n```\n\n```js\nconst { data, error } = await supabase.from('books').select().textSearch('description', `'big'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'big'\");\n```\n\n```swift\nlet response = await client.from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'big'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'big'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = supabase.from_('books').select().text_search('description', \"'big'\").execute()\n```\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description |\n| --- | ----------------------------------- | ----------------- | ----------------------------------------------- |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. |\n| 5 | Harry Potter and the Goblet of Fire | J.K. Rowling | Fourth year of school starts, big drama ensues. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Search multiple columns",
          "content": "Right now there is no direct way to use JavaScript or Dart to search through multiple columns but you can do it by creating [computed columns](https://postgrest.org/en/stable/api.html#computed-virtual-columns) on the database.\n\nTo find all `books` where `description` or `title` contain the word `little`:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description || ' ' || title) -- concat columns, but be sure to include a space to separate them!\n @@ to_tsquery('little');\n```\n\n```sql\ncreate function title_description(books) returns text as $$\n select $1.title || ' ' || $1.description;\n$$ language sql immutable;\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('title_description', `little`)\n```\n\n```sql\ncreate function title_description(books) returns text as $$\n select $1.title || ' ' || $1.description;\n$$ language sql immutable;\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('title_description', \"little\")\n```\n\n```sql\ncreate function title_description(books) returns text as $$\n select $1.title || ' ' || $1.description;\n$$ language sql immutable;\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"title_description\", value: \"little\")\n .execute()\n```\n\n```sql\ncreate function title_description(books) returns text as $$\n select $1.title || ' ' || $1.description;\n$$ language sql immutable;\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"title_description\", \"title\", TextSearchType.NONE)\n }\n}\n```\n\n```sql\ncreate function title_description(books) returns text as $$\n select $1.title || ' ' || $1.description;\n$$ language sql immutable;\n```\n\n```python\ndata = supabase.from_('books').select().text_search('title_description', \"little\").execute()\n```\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description |\n| --- | --------------------- | ---------------------- | ------------------------------------------- |\n| 1 | The Poky Little Puppy | Janette Sebring Lowrey | Puppy is slower than other, bigger animals. |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Match all search words",
          "content": "To find all `books` where `description` contains BOTH of the words `little` and `big`, we can use the `&` symbol:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description)\n @@ to_tsquery('little & big'); -- use & for AND in the search query\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('description', `'little' & 'big'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'little' & 'big'\");\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'little' & 'big'\");\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'title' & 'big'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = supabase.from_('books').select().text_search('description', \"'little' & 'big'\").execute()\n```\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description |\n| --- | ------ | ----------------- | -------------------------------- |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Match any search words",
          "content": "To find all `books` where `description` contain ANY of the words `little` or `big`, use the `|` symbol:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description)\n @@ to_tsquery('little | big'); -- use | for OR in the search query\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('description', `'little' | 'big'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'little' | 'big'\");\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'little' | 'big'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'title' | 'big'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\nresponse = client.from_('books').select().text_search('description', \"'little' | 'big'\").execute()\n```\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description |\n| --- | --------------------- | ---------------------- | ------------------------------------------- |\n| 1 | The Poky Little Puppy | Janette Sebring Lowrey | Puppy is slower than other, bigger animals. |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\nNotice how searching for `big` includes results with the word `bigger` (or `biggest`, etc).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Partial search",
          "content": "Partial search is particularly useful when you want to find matches on substrings within your data.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Implementing partial search",
          "content": "You can use the `:*` syntax with `to_tsquery()`. Here's an example that searches for any book titles beginning with \"Lit\":\n\n```sql\nselect title from books where to_tsvector(title) @@ to_tsquery('Lit:*');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Extending functionality with RPC",
          "content": "To make the partial search functionality accessible through the API, you can wrap the search logic in a stored procedure.\n\nAfter creating this function, you can invoke it from your application using the SDK for your platform. Here's an example:\n\n```sql\ncreate or replace function search_books_by_title_prefix(prefix text)\nreturns setof books AS $$\nbegin\n return query\n select * from books where to_tsvector('english', title) @@ to_tsquery(prefix || ':*');\nend;\n$$ language plpgsql;\n```\n\n```js\nconst { data, error } = await supabase.rpc('search_books_by_title_prefix', { prefix: 'Lit' })\n```\n\n```dart\nfinal data = await supabase.rpc('search_books_by_title_prefix', params: { 'prefix': 'Lit' });\n```\n\n```swift\nlet response = try await supabase.rpc(\n \"search_books_by_title_prefix\",\n params: [\"prefix\": \"Lit\"]\n)\n.execute()\n```\n\n```kotlin\nval rpcParams = mapOf(\"prefix\" to \"Lit\")\nval result = supabase.postgrest.rpc(\"search_books_by_title_prefix\", rpcParams)\n```\n\n```python\ndata = client.rpc('search_books_by_title_prefix', { 'prefix': 'Lit' }).execute()\n```\n\nThis function takes a prefix parameter and returns all books where the title contains a word starting with that prefix. The `:*` operator is used to denote a prefix match in the `to_tsquery()` function.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Handling spaces in queries",
          "content": "When you want the search term to include a phrase or multiple words, you can concatenate words using a `+` as a placeholder for space:\n\n```sql\nselect * from search_books_by_title_prefix('Little+Puppy');\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating indexes",
          "content": "Now that we have Full Text Search working, let's create an `index`. This will allow Postgres to \"build\" the documents preemptively so that they\ndon't need to be created at the time we execute the query. This will make our queries much faster.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Searchable columns",
          "content": "Let's create a new column `fts` inside the `books` table to store the searchable index of the `title` and `description` columns.\n\nWe can use a special feature of Postgres called\n[Generated Columns](https://www.postgresql.org/docs/current/ddl-generated-columns.html)\nto ensure that the index is updated any time the values in the `title` and `description` columns change.\n\n```sql\nalter table\n books\nadd column\n fts tsvector generated always as (to_tsvector('english', description || ' ' || title)) stored;\n\ncreate index books_fts on books using gin (fts); -- generate the index\n\nselect id, fts\nfrom books;\n```\n\n```\n| id | fts |\n| --- | --------------------------------------------------------------------------------------------------------------- |\n| 1 | 'anim':7 'bigger':6 'littl':10 'poki':9 'puppi':1,11 'slower':3 |\n| 2 | 'eat':2 'peter':8 'rabbit':1,9 'tale':6 'veget':4 |\n| 3 | 'big':5 'dream':6 'littl':1 'tootl':7 'toy':2 'train':3 |\n| 4 | 'chang':3 'color':9 'eat':7 'egg':12 'food':4,10 'green':11 'ham':14 'prefer':5 'sam':1 'unus':8 |\n| 5 | 'big':6 'drama':7 'ensu':8 'fire':15 'fourth':1 'goblet':13 'harri':9 'potter':10 'school':4 'start':5 'year':2 |\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Search using the new column",
          "content": "Now that we've created and populated our index, we can search it using the same techniques as before:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n fts @@ to_tsquery('little & big');\n```\n\n```js\nconst { data, error } = await supabase.from('books').select().textSearch('fts', `'little' & 'big'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('fts', \"'little' & 'big'\");\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"fts\", value: \"'little' & 'big'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"fts\", \"'title' & 'big'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = client.from_('books').select().text_search('fts', \"'little' & 'big'\").execute()\n```\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | description | fts |\n| --- | ------ | ----------------- | -------------------------------- | ------------------------------------------------------- |\n| 3 | Tootle | Gertrude Crampton | Little toy train has big dreams. | 'big':5 'dream':6 'littl':1 'tootl':7 'toy':2 'train':3 |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Query operators",
          "content": "Visit [Postgres: Text Search Functions and Operators](https://www.postgresql.org/docs/current/functions-textsearch.html)\nto learn about additional query operators you can use to do more advanced `full text queries`, such as:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Proximity: `` [#proximity]",
          "content": "The proximity symbol is useful for searching for terms that are a certain \"distance\" apart.\nFor example, to find the phrase `big dreams`, where the a match for \"big\" is followed immediately by a match for \"dreams\":\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description) @@ to_tsquery('big dreams');\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('description', `'big' 'dreams'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'big' 'dreams'\");\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'big' 'dreams'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'big' 'dreams'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = client.from_('books').select().text_search('description', \"'big' 'dreams'\").execute()\n```\n\nWe can also use the `` to find words within a certain distance of each other. For example to find `year` and `school` within 2 words of each other:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description) @@ to_tsquery('year school');\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('description', `'year' 'school'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'year' 'school'\");\n```\n\n```swift\nlet response = try await supabase\n .from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'year' 'school'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'year' 'school'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = client.from_('books').select().text_search('description', \"'year' 'school'\").execute()\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Negation: `!` [#negation]",
          "content": "The negation symbol can be used to find phrases which _don't_ contain a search term.\nFor example, to find records that have the word `big` but not `little`:\n\n```sql\nselect\n *\nfrom\n books\nwhere\n to_tsvector(description) @@ to_tsquery('big & !little');\n```\n\n```js\nconst { data, error } = await supabase\n .from('books')\n .select()\n .textSearch('description', `'big' & !'little'`)\n```\n\n```dart\nfinal result = await client\n .from('books')\n .select()\n .textSearch('description', \"'big' & !'little'\");\n```\n\n```swift\nlet response = try await client\n .from(\"books\")\n .select()\n .textSearch(\"description\", value: \"'big' & !'little'\")\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select {\n filter {\n textSearch(\"description\", \"'big' & !'little'\", TextSearchType.NONE)\n }\n}\n```\n\n```python\ndata = client.from_('books').select().text_search('description', \"'big' & !'little'\").execute()\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Postgres: Text Search Functions and Operators](https://www.postgresql.org/docs/12/functions-textsearch.html)",
          "level": 2
        }
      ],
      "wordCount": 2322,
      "characterCount": 17477
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-functions",
      "identifier": "database-functions",
      "name": "Database Functions",
      "description": "Creating and using Postgres functions.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/functions",
      "dateModified": "2025-06-13T12:45:11.286684",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/functions.mdx",
      "frontmatter": {
        "id": "functions",
        "title": "Database Functions",
        "description": "Creating and using Postgres functions.",
        "video": "https://www.youtube.com/v/MJZCCpCYEqk"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres has built-in support for [SQL functions](https://www.postgresql.org/docs/current/sql-createfunction.html).\nThese functions live inside your database, and they can be [used with the API](../../reference/javascript/rpc)."
        },
        {
          "type": "section",
          "title": "Getting started",
          "content": "Supabase provides several options for creating database functions. You can use the Dashboard or create them directly using SQL.\nWe provide a SQL editor within the Dashboard, or you can [connect](../../guides/database/connecting-to-postgres) to your database\nand run the SQL queries yourself.\n\n1. Go to the \"SQL editor\" section.\n2. Click \"New Query\".\n3. Enter the SQL to create or replace your Database function.\n4. Click \"Run\" or cmd+enter (ctrl+enter).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Simple functions",
          "content": "Let's create a basic Database Function which returns a string \"hello world\".\n\n```sql\ncreate or replace function hello_world() -- 1\nreturns text -- 2\nlanguage sql -- 3\nas $$ -- 4\n select 'hello world'; -- 5\n$$; --6\n\n```\n\nShow/Hide Details\n\nAt it's most basic a function has the following parts:\n\n1. `create or replace function hello_world()`: The function declaration, where `hello_world` is the name of the function. You can use either `create` when creating a new function or `replace` when replacing an existing function. Or you can use `create or replace` together to handle either.\n2. `returns text`: The type of data that the function returns. If it returns nothing, you can `returns void`.\n3. `language sql`: The language used inside the function body. This can also be a procedural language: `plpgsql`, `plpython`, etc.\n4. `as $$`: The function wrapper. Anything enclosed inside the `$$` symbols will be part of the function body.\n5. `select 'hello world';`: A simple function body. The final `select` statement inside a function body will be returned if there are no statements following it.\n6. `$$;`: The closing symbols of the function wrapper.\n\nWhen naming your functions, make the name of the function unique as overloaded functions are not supported.\n\nAfter the Function is created, we have several ways of \"executing\" the function - either directly inside the database using SQL, or with one of the client libraries.\n\n```sql\nselect hello_world();\n```\n\n```js\nconst { data, error } = await supabase.rpc('hello_world')\n```\n\nReference: [`rpc()`](../../reference/javascript/rpc)\n\n```dart\nfinal data = await supabase\n .rpc('hello_world');\n```\n\nReference: [`rpc()`](../../reference/dart/rpc)\n\n```swift\ntry await supabase.rpc(\"hello_world\").execute()\n```\n\nReference: [`rpc()`](../../reference/swift/rpc)\n\n```kotlin\nval data = supabase.postgrest.rpc(\"hello_world\")\n```\n\nReference: [`rpc()`](../../reference/kotlin/rpc)\n\n```python\ndata = supabase.rpc('hello_world').execute()\n```\n\nReference: [`rpc()`](../../reference/python/rpc)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Returning data sets",
          "content": "Database Functions can also return data sets from [Tables](../../guides/database/tables) or Views.\n\nFor example, if we had a database with some Star Wars data inside:\n\nPlanets\n\n```\n| id | name |\n| --- | -------- |\n| 1 | Tatooine |\n| 2 | Alderaan |\n| 3 | Kashyyyk |\n```\n\nPeople\n\n```\n| id | name | planet_id |\n| --- | ---------------- | --------- |\n| 1 | Anakin Skywalker | 1 |\n| 2 | Luke Skywalker | 1 |\n| 3 | Princess Leia | 2 |\n| 4 | Chewbacca | 3 |\n```\n\n```sql\ncreate table planets (\n id serial primary key,\n name text\n);\n\ninsert into planets\n (id, name)\nvalues\n (1, 'Tattoine'),\n (2, 'Alderaan'),\n (3, 'Kashyyyk');\n\ncreate table people (\n id serial primary key,\n name text,\n planet_id bigint references planets\n);\n\ninsert into people\n (id, name, planet_id)\nvalues\n (1, 'Anakin Skywalker', 1),\n (2, 'Luke Skywalker', 1),\n (3, 'Princess Leia', 2),\n (4, 'Chewbacca', 3);\n```\n\nWe could create a function which returns all the planets:\n\n```sql\ncreate or replace function get_planets()\nreturns setof planets\nlanguage sql\nas $$\n select * from planets;\n$$;\n```\n\nBecause this function returns a table set, we can also apply filters and selectors. For example, if we only wanted the first planet:\n\n```sql\nselect *\nfrom get_planets()\nwhere id = 1;\n```\n\n```js\nconst { data, error } = supabase.rpc('get_planets').eq('id', 1)\n```\n\n```dart\nfinal data = await supabase\n .rpc('get_planets')\n .eq('id', 1);\n```\n\n```swift\nlet response = try await supabase.rpc(\"get_planets\").eq(\"id\", value: 1).execute()\n```\n\n```kotlin\nval data = supabase.postgrest.rpc(\"get_planets\") {\n filter {\n eq(\"id\", 1)\n }\n}\n```\n\n```python\ndata = supabase.rpc('get_planets').eq('id', 1).execute()\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Passing parameters",
          "content": "Let's create a Function to insert a new planet into the `planets` table and return the new ID. Note that this time we're using the `plpgsql` language.\n\n```sql\ncreate or replace function add_planet(name text)\nreturns bigint\nlanguage plpgsql\nas $$\ndeclare\n new_row bigint;\nbegin\n insert into planets(name)\n values (add_planet.name)\n returning id into new_row;\n\n return new_row;\nend;\n$$;\n```\n\nOnce again, you can execute this function either inside your database using a `select` query, or with the client libraries:\n\n```sql\nselect * from add_planet('Jakku');\n```\n\n```js\nconst { data, error } = await supabase.rpc('add_planet', { name: 'Jakku' })\n```\n\n```dart\nfinal data = await supabase\n .rpc('add_planet', params: { 'name': 'Jakku' });\n```\n\nUsing `Encodable` type:\n\n```swift\nstruct Planet: Encodable {\n let name: String\n}\n\ntry await supabase.rpc(\n \"add_planet\",\n params: Planet(name: \"Jakku\")\n)\n.execute()\n```\n\nUsing `AnyJSON` convenience` type:\n\n```swift\ntry await supabase.rpc(\n \"add_planet\",\n params: [\"name\": AnyJSON.string(\"Jakku\")]\n)\n.execute()\n```\n\n```kotlin\nval data = supabase.postgrest.rpc(\n function = \"add_planet\",\n parameters = buildJsonObject { //You can put here any serializable object including your own classes\n put(\"name\", \"Jakku\")\n }\n)\n```\n\n```python\ndata = supabase.rpc('add_planet', params={'name': 'Jakku'}).execute()\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Database Functions vs Edge Functions",
          "content": "For data-intensive operations, use Database Functions, which are executed within your database\nand can be called remotely using the [REST and GraphQL API](../api).\n\nFor use-cases which require low-latency, use [Edge Functions](../../guides/functions), which are globally-distributed and can be written in Typescript.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Security `definer` vs `invoker`",
          "content": "Postgres allows you to specify whether you want the function to be executed as the user _calling_ the function (`invoker`), or as the _creator_ of the function (`definer`). For example:\n\n```sql\ncreate function hello_world()\nreturns text\nlanguage plpgsql\nsecurity definer set search_path = ''\nas $$\nbegin\n select 'hello world';\nend;\n$$;\n```\n\nIt is best practice to use `security invoker` (which is also the default). If you ever use `security definer`, you _must_ set the `search_path`.\nIf you use an empty search path (`search_path = ''`), you must explicitly state the schema for every relation in the function body (e.g. `from public.table`).\nThis limits the potential damage if you allow access to schemas which the user executing the function should not have.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Function privileges",
          "content": "By default, database functions can be executed by any role. There are two main ways to restrict this:\n\n1. On a case-by-case basis. Specifically revoke permissions for functions you want to protect. Execution needs to be revoked for both `public` and the role you're restricting:\n\n ```sql\n revoke execute on function public.hello_world from public;\n revoke execute on function public.hello_world from anon;\n ```\n\n1. Restrict function execution by default. Specifically _grant_ access when you want a function to be executable by a specific role.\n\n To restrict all existing functions, revoke execution permissions from both `public` _and_ the role you want to restrict:\n\n ```sql\n revoke execute on all functions in schema public from public;\n revoke execute on all functions in schema public from anon, authenticated;\n ```\n\n To restrict all new functions, change the default privileges for both `public` _and_ the role you want to restrict:\n\n ```sql\n alter default privileges in schema public revoke execute on functions from public;\n alter default privileges in schema public revoke execute on functions from anon, authenticated;\n ```\n\n You can then regrant permissions for a specific function to a specific role:\n\n ```sql\n grant execute on function public.hello_world to authenticated;\n ```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Debugging functions",
          "content": "You can add logs to help you debug functions. This is especially recommended for complex functions.\n\nGood targets to log include:\n\n- Values of (non-sensitive) variables\n- Returned results from queries",
          "level": 3
        },
        {
          "type": "section",
          "title": "General logging",
          "content": "To create custom logs in the [Dashboard's Postgres Logs](https://supabase.com/dashboard/project/_/logs/postgres-logs), you can use the `raise` keyword. By default, there are 3 observed severity levels:\n\n- `log`\n- `warning`\n- `exception` (error level)\n\n```sql\ncreate function logging_example(\n log_message text,\n warning_message text,\n error_message text\n)\nreturns void\nlanguage plpgsql\nas $$\nbegin\n raise log 'logging message: %', log_message;\n raise warning 'logging warning: %', warning_message;\n\n -- immediately ends function and reverts transaction\n raise exception 'logging error: %', error_message;\nend;\n$$;\n\nselect logging_example('LOGGED MESSAGE', 'WARNING MESSAGE', 'ERROR MESSAGE');\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Error handling",
          "content": "You can create custom errors with the `raise exception` keywords.\n\nA common pattern is to throw an error when a variable doesn't meet a condition:\n\n```sql\ncreate or replace function error_if_null(some_val text)\nreturns text\nlanguage plpgsql\nas $$\nbegin\n -- error if some_val is null\n if some_val is null then\n raise exception 'some_val should not be NULL';\n end if;\n -- return some_val if it is not null\n return some_val;\nend;\n$$;\n\nselect error_if_null(null);\n```\n\nValue checking is common, so Postgres provides a shorthand: the `assert` keyword. It uses the following format:\n\n```sql\n-- throw error when condition is false\nassert , 'message';\n```\n\nBelow is an example\n\n```sql\ncreate function assert_example(name text)\nreturns uuid\nlanguage plpgsql\nas $$\ndeclare\n student_id uuid;\nbegin\n -- save a user's id into the user_id variable\n select\n id into student_id\n from attendance_table\n where student = name;\n\n -- throw an error if the student_id is null\n assert student_id is not null, 'assert_example() ERROR: student not found';\n\n -- otherwise, return the user's id\n return student_id;\nend;\n$$;\n\nselect assert_example('Harry Potter');\n```\n\nError messages can also be captured and modified with the `exception` keyword:\n\n```sql\ncreate function error_example()\nreturns void\nlanguage plpgsql\nas $$\nbegin\n -- fails: cannot read from nonexistent table\n select * from table_that_does_not_exist;\n\n exception\n when others then\n raise exception 'An error occurred in function : %', sqlerrm;\nend;\n$$;\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Advanced logging",
          "content": "For more complex functions or complicated debugging, try logging:\n\n- Formatted variables\n- Individual rows\n- Start and end of function calls\n\n```sql\ncreate or replace function advanced_example(num int default 10)\nreturns text\nlanguage plpgsql\nas $$\ndeclare\n var1 int := 20;\n var2 text;\nbegin\n -- Logging start of function\n raise log 'logging start of function call: (%)', (select now());\n\n -- Logging a variable from a SELECT query\n select\n col_1 into var1\n from some_table\n limit 1;\n raise log 'logging a variable (%)', var1;\n\n -- It is also possible to avoid using variables, by returning the values of your query to the log\n raise log 'logging a query with a single return value(%)', (select col_1 from some_table limit 1);\n\n -- If necessary, you can even log an entire row as JSON\n raise log 'logging an entire row as JSON (%)', (select to_jsonb(some_table.*) from some_table limit 1);\n\n -- When using INSERT or UPDATE, the new value(s) can be returned\n -- into a variable.\n -- When using DELETE, the deleted value(s) can be returned.\n -- All three operations use \"RETURNING value(s) INTO variable(s)\" syntax\n insert into some_table (col_2)\n values ('new val')\n returning col_2 into var2;\n\n raise log 'logging a value from an INSERT (%)', var2;\n\n return var1 || ',' || var2;\nexception\n -- Handle exceptions here if needed\n when others then\n raise exception 'An error occurred in function : %', sqlerrm;\nend;\n$$;\n\nselect advanced_example();\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official Client libraries: [JavaScript](../../reference/javascript/rpc) and [Flutter](../../reference/dart/rpc)\n- Community client libraries: [github.com/supabase-community](https://github.com/supabase-community)\n- Postgres Official Docs: [Chapter 9. Functions and Operators](https://www.postgresql.org/docs/current/functions.html)\n- Postgres Reference: [CREATE FUNCTION](https://www.postgresql.org/docs/9.1/sql-createfunction.html)",
          "level": 2
        }
      ],
      "wordCount": 1849,
      "characterCount": 12852
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-hardening-data-api",
      "identifier": "database-hardening-data-api",
      "name": "Hardening the Data API",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/hardening-data-api",
      "dateModified": "2025-06-13T12:45:11.286884",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/hardening-data-api.mdx",
      "frontmatter": {
        "title": "Hardening the Data API"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Your database's auto-generated Data API exposes the `public` schema by default. You can change this to any schema in your database, or even disable the Data API completely.\n\nAny tables that are accessible through the Data API _must_ have [Row Level Security](/docs/guides/database/postgres/row-level-security) enabled. Row Level Security (RLS) is enabled by default when you create tables from the Supabase Dashboard. If you create a table using the SQL editor or your own SQL client or migration runner, you*must* enable RLS yourself."
        },
        {
          "type": "section",
          "title": "Shared responsibility",
          "content": "Your application's security is your responsibility as a developer. This includes RLS, falling under the [Shared Responsibility](/docs/guides/deployment/shared-responsibility-model) model. To help you:\n\n- Supabase sends daily emails warning of any tables that are exposed to the Data API which do not have RLS enabled.\n- Supabase provides a Security Advisor and other tools in the Supabase Dashboard to fix any issues.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Private schemas",
          "content": "We highly recommend creating a `private` schema for storing tables that you do not want to expose via the Data API. These tables can be accessed via Supabase Edge Functions or any other serverside tool. In this model, you should implement your security model in your serverside code. Although it's not required, we _still_ recommend enabling RLS for private tables and then connecting to your database using a Postgres role with `bypassrls` privileges.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Managing the public schema",
          "content": "If your `public` schema is used by other tools as a default space, you might want to lock down this schema. This helps prevent accidental exposure of data that's automatically added to `public`.\n\nThere are two levels of security hardening for the Data API:\n\n- Disabling the Data API entirely. This is recommended if you _never_ need to access your database via Supabase client libraries or the REST and GraphQL endpoints.\n- Removing the `public` schema from the Data API and replacing it with a custom schema (such as `api`).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Disabling the Data API",
          "content": "You can disable the Data API entirely if you never intend to use the Supabase client libraries or the REST and GraphQL data endpoints. For example, if you only access your database via a direct connection on the server, disabling the Data API gives you the greatest layer of protection.\n\n1. Go to [API Settings](/dashboard/project/_/settings/api) in the Supabase Dashboard.\n1. Under **Data API Settings**, toggle **Enable Data API** off.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Exposing a custom schema instead of `public`",
          "content": "If you want to use the Data API but with increased security, you can expose a custom schema instead of `public`. By not using `public`, which is often used as a default space and has laxer default permissions, you get more conscious control over your exposed data.\n\nAny data, views, or functions that should be exposed need to be deliberately put within your custom schema (which we will call `api`), rather than ending up there by default.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Step 1: Remove `public` from exposed schemas",
          "content": "1. Go to [**API Settings**](/dashboard/project/_/settings/api) in the Supabase Dashboard.\n1. Under **Data API Settings**, remove `public` from **Exposed schemas**. Also remove `public` from **Extra search path**.\n1. Click **Save**.\n1. Go to [**Database Extensions**](/dashboard/project/_/database/extensions) and disable the `pg_graphql` extension.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Step 2: Create an `api` schema and expose it",
          "content": "1. Connect to your database. You can use `psql`, the [Supabase SQL Editor](/dashboard/project/_/sql), or the Postgres client of your choice.\n\n1. Create a new schema named `api`:\n\n ```sql\n create schema if not exists api;\n ```\n\n1. Grant the `anon` and `authenticated` roles usage on this schema.\n\n ```sql\n grant usage on schema api to anon, authenticated;\n ```\n\n1. Go to [API Settings](/dashboard/project/_/settings/api) in the Supabase Dashboard.\n\n1. Under **Data API Settings**, add `api` to **Exposed schemas**. Make sure it is the first schema in the list, so that it will be searched first by default.\n\n1. Under these new settings, `anon` and `authenticated` can execute functions defined in the `api` schema, but they have no automatic permissions on any tables. On a table-by-table basis, you can grant them permissions. For example:\n\n ```sql\n grant select on table api. to anon;\n grant select, insert, update, delete on table api. to authenticated;\n ```",
          "level": 3
        }
      ],
      "wordCount": 679,
      "characterCount": 4381
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-import-data",
      "identifier": "database-import-data",
      "name": "Import data into Supabase",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/import-data",
      "dateModified": "2025-06-13T12:45:11.287120",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/import-data.mdx",
      "frontmatter": {
        "id": "database",
        "title": "Import data into Supabase"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You can import data into Supabase in multiple ways. The best method depends on your data size and app requirements.\n\nIf you're working with small datasets in development, you can experiment quickly using CSV import in the Supabase dashboard. If you're working with a large dataset in production, you should plan your data import to minimize app latency and ensure data integrity."
        },
        {
          "type": "section",
          "title": "How to import data into Supabase",
          "content": "You have multiple options for importing your data into Supabase:\n\n1. [CSV import via the Supabase dashboard](#option-1-csv-import-via-supabase-dashboard)\n2. [Bulk import using `pgloader`](#option-2-bulk-import-using-pgloader)\n3. [Using the Postgres `COPY` command](#option-3-using-postgres-copy-command)\n4. [Using the Supabase API](#option-4-using-the-supabase-api)\n\nIf you're importing a large dataset or importing data into production, plan ahead and [prepare your database](#preparing-to-import-data).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Option 1: CSV import via Supabase dashboard",
          "content": "Supabase dashboard provides a user-friendly way to import data. However, for very large datasets, this method may not be the most efficient choice, given the size limit is 100MB. It's generally better suited for smaller datasets and quick data imports. Consider using alternative methods like pgloader for large-scale data imports.\n\n1. Navigate to the relevant table in the [Table Editor.](/dashboard/project/_/editor)\n2. Click on “Insert” then choose \"Import Data from CSV\" and follow the on-screen instructions to upload your CSV file.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Option 2: Bulk import using pgloader",
          "content": "[pgloader](https://pgloader.io/) is a powerful tool for efficiently importing data into a Postgres database that supports a wide range of source database engines, including MySQL and MS SQL.\n\nYou can use it in conjunction with Supabase by following these steps:\n\n1. Install pgloader on your local machine or a server. For more info, you can refer to the [official pgloader installation page](https://pgloader.readthedocs.io/en/latest/install.html).\n\n ```bash\n $ apt-get install pgloader\n ```\n\n2. Create a configuration file that specifies the source data and the target Supabase database (e.g., config.load).\n Here's an example configuration file:\n\n ```sql\n LOAD DATABASE\n FROM sourcedb://USER:PASSWORD@HOST/SOURCE_DB\n INTO postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres\n ALTER SCHEMA 'public' OWNER TO 'postgres';\n set wal_buffers = '64MB', max_wal_senders = 0, statement_timeout = 0, work_mem to '2GB';\n ```\n\n Customize the source and Supabase database URL and options to fit your specific use case:\n\n - `wal_buffers`: This parameter is set to '64MB' to allocate 64 megabytes of memory for write-ahead logging buffers. A larger value can help improve write performance by caching more data in memory before writing it to disk. This can be useful during data import operations to speed up the writing of transaction logs.\n - `max_wal_senders`: It is set to 0, to disable replication connections. This is done during the data import process to prevent replication-related conflicts and issues.\n - `statement_timeout`: The value is set to 0, which means it's disabled, allowing SQL statements to run without a time limit.\n - `work_mem`: It is set to '2GB', allocating 2 GB of memory for query operations. This enhances the performance of complex queries by allowing larger in-memory datasets.\n\n3. Run pgloader with the configuration file.\n ```jsx\n pgloader config.load\n ```\n\nFor databases using the Postgres engine, we recommend using the [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) and [psql](https://www.postgresql.org/docs/current/app-psql.html) command line tools.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Option 3: Using Postgres copy command",
          "content": "Read more about [Bulk data loading.](/docs/guides/database/tables#bulk-data-loading)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Option 4: Using the Supabase API",
          "content": "The Supabase API allows you to programmatically import data into your tables. You can use various client libraries to interact with the API and perform data import operations. This approach is useful when you need to automate data imports, and it gives you fine-grained control over the process. Refer to our [API guide](/docs/guides/api) for more details.\n\nWhen importing data via the Supabase API, it's advisable to refrain from bulk imports. This helps ensure a smooth data transfer process and prevents any potential disruptions.\n\nRead more about [Rate Limiting, Resource Allocation, & Abuse Prevention.](/docs/guides/platform/going-into-prod#rate-limiting-resource-allocation--abuse-prevention)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Preparing to import data",
          "content": "Large data imports can affect your database performance. Failed imports can also cause data corruption. Importing data is a safe and common operation, but you should plan ahead if you're importing a lot of data, or if you're working in a production environment.",
          "level": 2
        },
        {
          "type": "section",
          "title": "1. Back up your data",
          "content": "Backups help you restore your data if something goes wrong. Databases on Pro, Team and Enterprise Plans are automatically backed up on schedule, but you can also take your own backup. See [Database Backups](/docs/guides/platform/backups) for more information.",
          "level": 3
        },
        {
          "type": "section",
          "title": "2. Increase statement timeouts",
          "content": "By default, Supabase enforces query statement timeouts to ensure fair resource allocation and prevent long-running queries from affecting the overall system. When importing large datasets, you may encounter timeouts. To address this:\n\n- **Increase the Statement Timeout**: You can adjust the statement timeout for your session or connection to accommodate longer-running queries. Be cautious when doing this, as excessively long queries can negatively impact system performance. Read more about [Statement Timeouts](/docs/guides/database/postgres/configuration).",
          "level": 3
        },
        {
          "type": "section",
          "title": "3. Estimate your required disk size",
          "content": "Large datasets consume disk space. Ensure your Supabase project has sufficient disk capacity to accommodate the imported data. If you know how big your database is going to be, you can manually increase the size in your [projects database settings](/dashboard/project/_/settings/database).\n\nRead more about [disk management](/docs/guides/platform/database-size#disk-management).",
          "level": 3
        },
        {
          "type": "section",
          "title": "4. Disable triggers",
          "content": "When importing large datasets, it's often beneficial to disable triggers temporarily. Triggers can significantly slow down the import process, especially if they involve complex logic or referential integrity checks. After the import, you can re-enable the triggers.\n\nTo disable triggers, use the following SQL commands:\n\n```sql\n-- Disable triggers on a specific table\nALTER TABLE table_name DISABLE TRIGGER ALL;\n\n-- To re-enable triggers\nALTER TABLE table_name ENABLE TRIGGER ALL;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "5. Rebuild indices after data import is complete",
          "content": "Indexing is crucial for query performance, but building indices while importing a large dataset can be time-consuming. Consider building or rebuilding indices after the data import is complete. This approach can significantly speed up the import process and reduce the overall time required.\n\nTo build an index after the data import:\n\n```sql\n-- Create an index on a table\ncreate index index_name on table_name (column_name);\n```\n\nRead more about [Managing Indexes in Postgres](/docs/guides/database/postgres/indexes).",
          "level": 3
        }
      ],
      "wordCount": 986,
      "characterCount": 7223
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-inspect",
      "identifier": "database-inspect",
      "name": "Debugging and monitoring",
      "description": "Inspecting your Postgres database for common issues around disk, query performance, index, locks, and more using the terminal.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/inspect",
      "dateModified": "2025-06-13T12:45:11.287532",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/inspect.mdx",
      "frontmatter": {
        "title": "Debugging and monitoring",
        "description": "Inspecting your Postgres database for common issues around disk, query performance, index, locks, and more using the terminal."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Database performance is a large topic and many factors can contribute. Some of the most common causes of poor performance include:\n\n- An inefficiently designed schema\n- Inefficiently designed queries\n- A lack of indexes causing slower than required queries over large tables\n- Unused indexes causing slow `INSERT`, `UPDATE` and `DELETE` operations\n- Not enough compute resources, such as memory, causing your database to go to disk for results too often\n- Lock contention from multiple queries operating on highly utilized tables\n- Large amount of bloat on your tables causing poor query planning\n\nYou can examine your database and queries for these issues using either the [Supabase CLI](/docs/guides/local-development/cli/getting-started) or SQL."
        },
        {
          "type": "section",
          "title": "Using the CLI",
          "content": "The Supabase CLI comes with a range of tools to help inspect your Postgres instances for potential issues. The CLI gets the information from Postgres internals. Therefore, most tools provided are compatible with any Postgres databases regardless if they are a Supabase project or not.\n\nYou can find installation instructions for the the Supabase CLI here.",
          "level": 2
        },
        {
          "type": "section",
          "title": "The `inspect db` command",
          "content": "The inspection tools for your Postgres database are under then `inspect db` command. You can get a full list of available commands by running `supabase inspect db help`.\n\n```\n$ supabase inspect db help\nTools to inspect your Supabase database\n\nUsage:\n supabase inspect db [command]\n\nAvailable Commands:\n bloat Estimates space allocated to a relation that is full of dead tuples\n blocking Show queries that are holding locks and the queries that are waiting for them to be released\n cache-hit Show cache hit rates for tables and indices\n\n...\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Connect to any Postgres database",
          "content": "Most inspection commands are Postgres agnostic. You can run inspection routines on any Postgres database even if it is not a Supabase project by providing a connection string via `--db-url`.\n\nFor example you can connect to your local Postgres instance:\n\n```\nsupabase --db-url postgresql://postgres:postgres@localhost:5432/postgres inspect db bloat\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Connect to a Supabase instance",
          "content": "Working with Supabase, you can link the Supabase CLI with your project:\n\n```\nsupabase link --project-ref \n```\n\nThen the CLI will automatically connect to your Supabase project whenever you are in the project folder and you no longer need to provide `—db-url`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Inspection commands",
          "content": "Below are the `db` inspection commands provided, grouped by different use cases.\n\nSome commands might require `pg_stat_statements` to be enabled or a specific Postgres version to be used.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Disk storage",
          "content": "These commands are handy if you are running low on disk storage:\n\n- [bloat](/docs/reference/cli/supabase-inspect-db-bloat) - estimates the amount of wasted space\n- [vacuum-stats](/docs/reference/cli/supabase-inspect-db-vacuum-stats) - gives information on waste collection routines\n- [table-record-counts](/docs/reference/cli/supabase-inspect-db-table-record-counts) - estimates the number of records per table\n- [table-sizes](/docs/reference/cli/supabase-inspect-db-table-sizes) - shows the sizes of tables\n- [index-sizes](/docs/reference/cli/supabase-inspect-db-index-sizes) - shows the sizes of individual index\n- [table-index-sizes](/docs/reference/cli/supabase-inspect-db-table-index-sizes) - shows the sizes of indexes for each table",
          "level": 4
        },
        {
          "type": "section",
          "title": "Query performance",
          "content": "The commands below are useful if your Postgres database consumes a lot of resources like CPU, RAM or Disk IO. You can also use them to investigate slow queries.\n\n- [cache-hit](/docs/reference/cli/supabase-inspect-db-cache-hit) - shows how efficient your cache usage is overall\n- [unused-indexes](/docs/reference/cli/supabase-inspect-db-unused-indexes) - shows indexes with low index scans\n- [index-usage](/docs/reference/cli/supabase-inspect-db-index-usage) - shows information about the efficiency of indexes\n- [seq-scans](/docs/reference/cli/supabase-inspect-db-seq-scans) - show number of sequential scans recorded against all tables\n- [long-running-queries](/docs/reference/cli/supabase-inspect-db-long-running-queries) - shows long running queries that are executing right now\n- [outliers](/docs/reference/cli/supabase-inspect-db-outliers) - shows queries with high execution time but low call count and queries with high proportion of execution time spent on synchronous I/O",
          "level": 4
        },
        {
          "type": "section",
          "title": "Locks",
          "content": "- [locks](/docs/reference/cli/supabase-inspect-db-locks) - shows statements which have taken out an exclusive lock on a relation\n- [blocking](/docs/reference/cli/supabase-inspect-db-blocking) - shows statements that are waiting for locks to be released",
          "level": 4
        },
        {
          "type": "section",
          "title": "Connections",
          "content": "- [role-connections](/docs/reference/cli/supabase-inspect-db-role-connections) - shows number of active connections for all database roles (Supabase-specific command)\n- [replication-slots](/docs/reference/cli/supabase-inspect-db-replication-slots) - shows information about replication slots on the database",
          "level": 4
        },
        {
          "type": "section",
          "title": "Notes on `pg_stat_statements`",
          "content": "Following commands require `pg_stat_statements` to be enabled: calls, locks, cache-hit, blocking, unused-indexes, index-usage, bloat, outliers, table-record-counts, replication-slots, seq-scans, vacuum-stats, long-running-queries.\n\nWhen using `pg_stat_statements` also take note that it only stores the latest 5,000 statements. Moreover, consider resetting the analysis after optimizing any queries by running `select pg_stat_statements_reset();`\n\nLearn more about pg_stats [here](https://supabase.com/docs/guides/database/extensions/pg_stat_statements).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using SQL",
          "content": "If you're seeing an `insufficient privilege` error when viewing the Query Performance page from the dashboard, run this command:\n\n```shell\n$ grant pg_read_all_stats to postgres;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Postgres cumulative statistics system",
          "content": "Postgres collects data about its own operations using the [cumulative statistics system](https://www.postgresql.org/docs/current/monitoring-stats.html). In addition to this, every Supabase project has the [pg_stat_statements extension](/docs/guides/database/extensions/pg_stat_statements) enabled by default. This extension records query execution performance details and is the best way to find inefficient queries. This information can be combined with the Postgres query plan analyzer to develop more efficient queries.\n\nHere are some example queries to get you started.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Most frequently called queries",
          "content": "```sql\nselect\n auth.rolname,\n statements.query,\n statements.calls,\n -- -- Postgres 13, 14, 15\n statements.total_exec_time + statements.total_plan_time as total_time,\n statements.min_exec_time + statements.min_plan_time as min_time,\n statements.max_exec_time + statements.max_plan_time as max_time,\n statements.mean_exec_time + statements.mean_plan_time as mean_time,\n -- -- Postgres ;\n```\n\nWhen you include `analyze` in the explain statement, the database attempts to execute the query and provides a detailed query plan along with actual execution times. So, be careful using `explain analyze` with `insert`/`update`/`delete` queries, because the query will actually run, and could have unintended side-effects.\n\nIf you run just `explain` without the `analyze` keyword, the database will only perform query planning without actually executing the query. This approach can be beneficial when you want to inspect the query plan without affecting the database or if you encounter timeouts in your queries.\n\nUsing the query plan analyzer to optimize your queries is a large topic, with a number of online resources available:\n\n- [Official docs.](https://www.postgresql.org/docs/current/using-explain.html)\n {/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}\n- [The Art of PostgreSQL.](https://theartofpostgresql.com/explain-plan-visualizer/)\n- [Postgres Wiki.](https://wiki.postgresql.org/wiki/Using_EXPLAIN)\n- [Enterprise DB.](https://www.enterprisedb.com/blog/postgresql-query-optimization-performance-tuning-with-explain-analyze)\n\nYou can pair the information available from `pg_stat_statements` with the detailed system metrics available [via your metrics endpoint](../platform/metrics) to better understand the behavior of your DB and the queries you're executing against it.",
          "level": 3
        }
      ],
      "wordCount": 999,
      "characterCount": 8189
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-joins-and-nesting",
      "identifier": "database-joins-and-nesting",
      "name": "Querying Joins and Nested tables",
      "description": "The Data APIs automatically detect relationships between Postgres tables.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/joins-and-nesting",
      "dateModified": "2025-06-13T12:45:11.287805",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/joins-and-nesting.mdx",
      "frontmatter": {
        "id": "joins-and-nested-tables",
        "title": "Querying Joins and Nested tables",
        "description": "The Data APIs automatically detect relationships between Postgres tables."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The data APIs automatically detect relationships between Postgres tables. Since Postgres is a relational database, this is a very common scenario."
        },
        {
          "type": "section",
          "title": "One-to-many joins",
          "content": "Let's use an example database that stores `orchestral_sections` and `instruments`:\n\n**Orchestral sections**\n\n| `id` | `name` |\n| ---- | --------- |\n| 1 | strings |\n| 2 | woodwinds |\n\n**Instruments**\n\n| `id` | `name` | `section_id` |\n| ---- | ------ | ------------ |\n| 1 | violin | 1 |\n| 2 | viola | 1 |\n| 3 | flute | 2 |\n| 4 | oboe | 2 |\n\n```sql\ncreate table orchestral_sections (\n \"id\" serial primary key,\n \"name\" text\n);\n\ninsert into orchestral_sections\n (id, name)\nvalues\n (1, 'strings'),\n (2, 'woodwinds');\n\ncreate table instruments (\n \"id\" serial primary key,\n \"name\" text,\n \"section_id\" int references \"orchestral_sections\"\n);\n\ninsert into instruments\n (name, section_id)\nvalues\n ('violin', 1),\n ('viola', 1),\n ('flute', 2),\n ('oboe', 2);\n```\n\nThe APIs will automatically detect relationships based on the foreign keys:\n\n```js\nconst { data, error } = await supabase.from('orchestral_sections').select(`\n id,\n name,\n instruments ( id, name )\n`)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "TypeScript types for joins",
          "content": "`supabase-js` always returns a `data` object (for success), and an `error` object (for unsuccessful requests).\n\nThese helper types provide the result types from any query, including nested types for database joins.\n\nGiven the following schema with a relation between orchestral sections and instruments:\n\n```sql\ncreate table orchestral_sections (\n \"id\" serial primary key,\n \"name\" text\n);\n\ncreate table instruments (\n \"id\" serial primary key,\n \"name\" text,\n \"section_id\" int references \"orchestral_sections\"\n);\n```\n\nWe can get the nested `SectionsWithInstruments` type like this:\n\n```ts\nimport { QueryResult, QueryData, QueryError } from '@supabase/supabase-js'\n\nconst sectionsWithInstrumentsQuery = supabase.from('orchestral_sections').select(`\n id,\n name,\n instruments (\n id,\n name\n )\n`)\ntype SectionsWithInstruments = QueryData\n\nconst { data, error } = await sectionsWithInstrumentsQuery\nif (error) throw error\nconst sectionsWithInstruments: SectionsWithInstruments = data\n```\n\n```dart\nfinal data = await supabase.from('orchestral_sections').select('id, name, instruments(id, name)');\n```\n\n```swift\nstruct OrchestralSection: Codable {\n let id: Int\n let name: String\n let instruments: [Instrument]\n\n struct Instrument: Codable {\n let id: Int\n let name: String\n }\n}\n\nlet orchestralSections: [OrchestralSection] = try await supabase\n .from(\"orchestral_sections\")\n .select(\"id, name, instruments(id, name)\")\n .execute()\n .value\n```\n\n```kotlin\nval data = supabase.from(\"orchestral_sections\").select(Columns.raw(\"id, name, instruments(id, name)\"))\n```\n\n```python\ndata = supabase.from_('orchestral_sections').select('id, name, instruments(id, name)').execute()\n```\n\n```javascript\nconst Query = `\n query {\n orchestral_sectionsCollection {\n edges {\n node {\n id\n name\n instruments {\n id,\n name\n }\n }\n }\n }\n }\n`\n```\n\n```bash\nGET https://[REF].supabase.co/rest/v1/orchestral_sections?select=id,name,instruments(id,name)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Many-to-many joins",
          "content": "The data APIs will detect many-to-many joins. For example, if you have a database which stored teams of users (where each user could belong to many teams):\n\n```sql\ncreate table users (\n \"id\" serial primary key,\n \"name\" text\n);\n\ncreate table teams (\n \"id\" serial primary key,\n \"team_name\" text\n);\n\ncreate table members (\n \"user_id\" int references users,\n \"team_id\" int references teams,\n primary key (user_id, team_id)\n);\n```\n\nIn these cases you don't need to explicitly define the joining table (members). If we wanted to fetch all the teams and the members in each team:\n\n```js\nconst { data, error } = await supabase.from('teams').select(`\n id,\n team_name,\n users ( id, name )\n`)\n```\n\n```dart\nfinal data = await supabase.from('teams').select('id, team_name, users(id, name)');\n```\n\n```swift\nstruct Team: Codable {\n let id: Int\n let name: String\n let users: [User]\n\n struct User: Codable {\n let id: Int\n let name: String\n }\n\n enum CodingKeys: String, CodingKey {\n case id, users\n case name = \"team_name\"\n }\n}\nlet teams [Team] = try await supabase\n .from(\"teams\")\n .select(\n \"\"\"\n id,\n team_name,\n users ( id, name )\n \"\"\"\n )\n .execute()\n .value\n```\n\n```kotlin\nval data = supabase.from(\"teams\").select(Columns.raw(\"id, team_name, users(id, name)\"));\n```\n\n```python\ndata = supabase.from_('teams').select('id, team_name, users(id, name)').execute()\n```\n\n````javascript\nconst Query = `\n query {\n\n```javascript\nconst Query = `\n query {\n teamsCollection {\n edges {\n node {\n id\n team_name\n users {\n id,\n name\n }\n }\n }\n }\n }\n`\n````\n\n```bash\nGET https://[REF].supabase.co/rest/v1/teams?select=id,team_name,users(id,name)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Specifying the `ON` clause for joins with multiple foreign keys",
          "content": "For example, if you have a project that tracks when employees check in and out of work shifts:\n\n```sql\n-- Employees\ncreate table users (\n \"id\" serial primary key,\n \"name\" text\n);\n\n-- Badge scans\ncreate table scans (\n \"id\" serial primary key,\n \"user_id\" int references users,\n \"badge_scan_time\" timestamp\n);\n\n-- Work shifts\ncreate table shifts (\n \"id\" serial primary key,\n \"user_id\" int references users,\n \"scan_id_start\" int references scans, -- clocking in\n \"scan_id_end\" int references scans, -- clocking out\n \"attendance_status\" text\n);\n```\n\nIn this case, you need to explicitly define the join because the joining column on `shifts` is ambiguous as they are both referencing the `scans` table.\n\nTo fetch all the `shifts` with `scan_id_start` and `scan_id_end` related to a specific `scan`, use the following syntax:\n\n```js\nconst { data, error } = await supabase.from('shifts').select(\n `\n *,\n start_scan:scans!scan_id_start (\n id,\n user_id,\n badge_scan_time\n ),\n end_scan:scans!scan_id_end (\n id,\n user_id,\n badge_scan_time\n )\n `\n)\n```\n\n```dart\nfinal data = await supabase.from('shifts').select('''\n *,\n start_scan:scans!scan_id_start (\n id,\n user_id,\n badge_scan_time\n ),\nend_scan:scans!scan_id_end (\n id,\n user_id,\n badge_scan_time\n )\n''');\n```\n\n```swift\nstruct Shift: Codable {\n let id: Int\n let userId: Int\n let attendanceStatus: String?\n\n let scans: [Scan]\n\n struct Scan: Codable {\n let id: Int\n let userId: Int\n let badgeScanTime: TimeInterval\n\n enum CodingKeys: String, CodingKey {\n case id\n case userId = \"user_id\"\n case badgeScanTime = \"badge_scan_time\"\n }\n }\n\n enum CodingKeys: String, CodingKey {\n case id\n case userId = \"user_id\"\n case attendanceStatus = \"attendance_status\"\n }\n}\n\nlet shifts: [Shift] = try await supabase\n .from(\"shifts\")\n .select(\n \"\"\"\n *,\n start_scan:scans!scan_id_start (\n id,\n user_id,\n badge_scan_time\n ),\n scans: scan_id_end (\n id,\n user_id,\n badge_scan_time\n )\n \"\"\"\n )\n .execute()\n .value\n```\n\n```kotlin\nval data = supabase.from(\"shifts\").select(Columns.raw('''\n *,\n start_scan:scans!scan_id_start (\n id,\n user_id,\n badge_scan_time\n ),\nend_scan:scans!scan_id_end (\n id,\n user_id,\n badge_scan_time\n )\n'''));\n```\n\n```python\ndata = supabase.from_('shifts').select(\"\"\"\n *,\n start_scan:scans!scan_id_start (\n id,\n user_id,\n badge_scan_time\n ),\n end_scan:scans!scan_id_end (\n id,\n user_id,\n badge_scan_time\n )\n\"\"\").execute()\n```\n\n```javascript\nconst Query = `\n query {\n shiftsCollection {\n edges {\n node {\n id\n user_id\n attendance_status\n scan_id_start {\n id\n user_id\n badge_scan_time\n }\n scan_id_end {\n id\n user_id\n badge_scan_time\n }\n }\n }\n }\n }\n`\n```",
          "level": 2
        }
      ],
      "wordCount": 998,
      "characterCount": 7366
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-json",
      "identifier": "database-json",
      "name": "Managing JSON and unstructured data",
      "description": "Using the JSON data type in Postgres.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/json",
      "dateModified": "2025-06-13T12:45:11.288184",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/json.mdx",
      "frontmatter": {
        "id": "json",
        "title": "Managing JSON and unstructured data",
        "description": "Using the JSON data type in Postgres.",
        "subtitle": "Using the JSON data type in Postgres.",
        "tocVideo": "nxeUiRz4G-M"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres supports storing and querying unstructured data."
        },
        {
          "type": "section",
          "title": "JSON vs JSONB",
          "content": "Postgres supports two types of JSON columns: `json` (stored as a string) and `jsonb` (stored as a binary). The recommended type is `jsonb` for almost all cases.\n\n- `json` stores an exact copy of the input text. Database functions must reparse the content on each execution.\n- `jsonb` stores database in a decomposed binary format. While this makes it slightly slower to input due to added conversion overhead, it is significantly faster to process, since no reparsing is needed.",
          "level": 2
        },
        {
          "type": "section",
          "title": "When to use JSON/JSONB",
          "content": "Generally you should use a `jsonb` column when you have data that is unstructured or has a variable schema. For example, if you wanted to store responses for various webhooks, you might not know the format of the response when creating the table. Instead, you could store the `payload` as a `jsonb` object in a single column.\n\nDon't go overboard with `json/jsonb` columns. They are a useful tool, but most of the benefits of a relational database come from the ability to query and join structured data, and the referential integrity that brings.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create JSONB columns",
          "content": "`json/jsonb` is just another \"data type\" for Postgres columns. You can create a `jsonb` column in the same way you would create a `text` or `int` column:\n\n```sql\ncreate table books (\n id serial primary key,\n title text,\n author text,\n metadata jsonb\n);\n```\n\n1. Go to the [Table Editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n2. Click **New Table** and create a table called `books`.\n3. Include a primary key with the following properties and click save:\n\n- Name: `id`\n - Type: `int8`\n - Default value: `Automatically generate as indentity`\n- **title** column\n - Name: `title`\n - Type: `text`\n- **author** column\n - Name: `author`\n - Type: `text`\n- **metadata** column\n - Name: `metadata`\n - Type: `jsonb`",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inserting JSON data",
          "content": "You can insert JSON data in the same way that you insert any other data. The data must be valid JSON.\n\n```sql\ninsert into books\n (title, author, metadata)\nvalues\n (\n 'The Poky Little Puppy',\n 'Janette Sebring Lowrey',\n '{\"description\":\"Puppy is slower than other, bigger animals.\",\"price\":5.95,\"ages\":[3,6]}'\n ),\n (\n 'The Tale of Peter Rabbit',\n 'Beatrix Potter',\n '{\"description\":\"Rabbit eats some vegetables.\",\"price\":4.49,\"ages\":[2,5]}'\n ),\n (\n 'Tootle',\n 'Gertrude Crampton',\n '{\"description\":\"Little toy train has big dreams.\",\"price\":3.99,\"ages\":[2,5]}'\n ),\n (\n 'Green Eggs and Ham',\n 'Dr. Seuss',\n '{\"description\":\"Sam has changing food preferences and eats unusually colored food.\",\"price\":7.49,\"ages\":[4,8]}'\n ),\n (\n 'Harry Potter and the Goblet of Fire',\n 'J.K. Rowling',\n '{\"description\":\"Fourth year of school starts, big drama ensues.\",\"price\":24.95,\"ages\":[10,99]}'\n );\n```\n\n1. Go to the [Table Editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n2. Select the `books` table in the sidebar.\n3. Click **+ Insert row** and add 5 rows with the following properties:\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | title | author | metadata |\n| --- | ----------------------------------- | ---------------------- | --------------------------------------------------------------------------------------------------------------------- |\n| 1 | The Poky Little Puppy | Janette Sebring Lowrey | `json {\"ages\":[3,6],\"price\":5.95,\"description\":\"Puppy is slower than other, bigger animals.\"}` |\n| 2 | The Tale of Peter Rabbit | Beatrix Potter | `json {\"ages\":[2,5],\"price\":4.49,\"description\":\"Rabbit eats some vegetables.\"}` |\n| 3 | Tootle | Gertrude Crampton | `json {\"ages\":[2,5],\"price\":3.99,\"description\":\"Little toy train has big dreams.\"}` |\n| 4 | Green Eggs and Ham | Dr. Seuss | `json {\"ages\":[4,8],\"price\":7.49,\"description\":\"Sam has changing food preferences and eats unusually colored food.\"}` |\n| 5 | Harry Potter and the Goblet of Fire | J.K. Rowling | `json {\"ages\":[10,99],\"price\":24.95,\"description\":\"Fourth year of school starts, big drama ensues.\"}` |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\n```js\nconst { data, error } = await supabase.from('books').insert([\n {\n title: 'The Poky Little Puppy',\n author: 'Janette Sebring Lowrey',\n metadata: {\n description: 'Puppy is slower than other, bigger animals.',\n price: 5.95,\n ages: [3, 6],\n },\n },\n {\n title: 'The Tale of Peter Rabbit',\n author: 'Beatrix Potter',\n metadata: {\n description: 'Rabbit eats some vegetables.',\n price: 4.49,\n ages: [2, 5],\n },\n },\n {\n title: 'Tootle',\n author: 'Gertrude Crampton',\n metadata: {\n description: 'Little toy train has big dreams.',\n price: 3.99,\n ages: [2, 5],\n },\n },\n {\n title: 'Green Eggs and Ham',\n author: 'Dr. Seuss',\n metadata: {\n description: 'Sam has changing food preferences and eats unusually colored food.',\n price: 7.49,\n ages: [4, 8],\n },\n },\n {\n title: 'Harry Potter and the Goblet of Fire',\n author: 'J.K. Rowling',\n metadata: {\n description: 'Fourth year of school starts, big drama ensues.',\n price: 24.95,\n ages: [10, 99],\n },\n },\n])\n```\n\n```dart\nawait supabase.from('books').insert([\n {\n 'title': 'The Poky Little Puppy',\n 'author': 'Janette Sebring Lowrey',\n 'metadata': {\n 'description': 'Puppy is slower than other, bigger animals.',\n 'price': 5.95,\n 'ages': [3, 6],\n },\n },\n {\n 'title': 'The Tale of Peter Rabbit',\n 'author': 'Beatrix Potter',\n 'metadata': {\n 'description': 'Rabbit eats some vegetables.',\n 'price': 4.49,\n 'ages': [2, 5],\n },\n },\n {\n 'title': 'Tootle',\n 'author': 'Gertrude Crampton',\n 'metadata': {\n 'description': 'Little toy train has big dreams.',\n 'price': 3.99,\n 'ages': [2, 5],\n },\n },\n {\n 'title': 'Green Eggs and Ham',\n 'author': 'Dr. Seuss',\n 'metadata': {\n 'description':\n 'Sam has changing food preferences and eats unusually colored food.',\n 'price': 7.49,\n 'ages': [4, 8],\n },\n },\n {\n 'title': 'Harry Potter and the Goblet of Fire',\n 'author': 'J.K. Rowling',\n 'metadata': {\n 'description': 'Fourth year of school starts, big drama ensues.',\n 'price': 24.95,\n 'ages': [10, 99],\n },\n },\n]);\n```\n\nSupabase Swift provides a convenience `AnyJSON` type.\n\n```swift\nstruct Book {\n val title: String,\n val author: String,\n val metadata: [String: AnyJSON]\n}\n\ntry await supabase.from(\"books\")\n .insert(\n [\n Book(\n title: \"The Poky Little Puppy\",\n author: \"Janette Sebring Lowrey\",\n metadata: [\n \"description\": \"Puppy is slower than other, bigger animals.\",\n \"price\": 5.95,\n \"ages\": [3, 6]\n ]\n ),\n Book(\n title: \"Tale of Peter Rabbit\",\n author: \"Beatrix Potter\",\n metadata: [\n \"description\": \"Rabbit eats some vegetables.\",\n \"price\": 4.49,\n \"ages\": [2, 5]\n ]\n ),\n Book(\n title: \"Tootle\",\n author: \"Gertrude Crampton\",\n metadata: [\n \"description\": \"Little toy train has big dreams.\",\n \"price\": 3.99,\n \"ages\": [2, 5]\n ]\n ),\n Book(\n title: \"Green Eggs and Ham\",\n author: \"Dr. Seuss\",\n metadata: [\n \"description\": \"Sam has changing food preferences and eats unusually colored food.\",\n \"price\": 7.49,\n \"ages\": [4, 8]\n ]\n ),\n Book(\n title: \"Harry Potter and the Goblet of Fire\",\n author: \"J.K. Rowling\",\n metadata: [\n \"description\": \"Fourth year of school starts, big drama ensues.\",\n \"price\": 24.95,\n \"ages\": [10, 99]\n ]\n )\n ]\n )\n```\n\n```kotlin\n@Serializable\ndata class BookMetadata(\n val description: String,\n val price: Double,\n val ages: List\n)\n\n@Serializable\ndata class Book(\n val title: String,\n val author: String,\n val metadata: BookMetadata\n)\n```\n\n```kotlin\nval data = supabase.from(\"books\").insert(listOf(\n Book(\"The Poky Little Puppy\", \"Janette Sebring Lowrey\", BookMetadata(\"Puppy is slower than other, bigger animals.\", 5.95, listOf(3, 6))),\n Book(\"Tale of Peter Rabbit\", \"Beatrix Potter\", BookMetadata(\"Rabbit eats some vegetables.\", 4.49, listOf(2, 5))),\n Book(\"Tootle\", \"Gertrude Crampton\", BookMetadata(\"Little toy train has big dreams.\", 3.99, listOf(2, 5))),\n Book(\"Green Eggs and Ham\", \"Dr. Seuss\", BookMetadata(\"Sam has changing food preferences and eats unusually colored food.\", 7.49, listOf(4, 8))),\n Book(\"Harry Potter and the Goblet of Fire\", \"J.K. Rowling\", BookMetadata(\"Fourth year of school starts, big drama ensues.\", 24.95, listOf(10, 99)))\n))\n```\n\n```python\nsupabase.from_('books').insert([\n {\n 'title': 'The Poky Little Puppy',\n 'author': 'Janette Sebring Lowrey',\n 'metadata': {\n 'description': 'Puppy is slower than other, bigger animals.',\n 'price': 5.95,\n 'ages': [3, 6],\n },\n },\n {\n 'title': 'The Tale of Peter Rabbit',\n 'author': 'Beatrix Potter',\n 'metadata': {\n 'description': 'Rabbit eats some vegetables.',\n 'price': 4.49,\n 'ages': [2, 5],\n },\n },\n {\n 'title': 'Tootle',\n 'author': 'Gertrude Crampton',\n 'metadata': {\n 'description': 'Little toy train has big dreams.',\n 'price': 3.99,\n 'ages': [2, 5],\n },\n },\n {\n 'title': 'Green Eggs and Ham',\n 'author': 'Dr. Seuss',\n 'metadata': {\n 'description':\n 'Sam has changing food preferences and eats unusually colored food.',\n 'price': 7.49,\n 'ages': [4, 8],\n },\n },\n {\n 'title': 'Harry Potter and the Goblet of Fire',\n 'author': 'J.K. Rowling',\n 'metadata': {\n 'description': 'Fourth year of school starts, big drama ensues.',\n 'price': 24.95,\n 'ages': [10, 99],\n },\n },\n]).execute()\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Query JSON data",
          "content": "Querying JSON data is similar to querying other data, with a few other features to access nested values.\n\nPostgres support a range of [JSON functions and operators](https://www.postgresql.org/docs/current/functions-json.html). For example, the `->` operator returns values as `jsonb` data. If you want the data returned as `text`, use the `->>` operator.\n\n```sql\nselect\n title,\n metadata ->> 'description' as description, -- returned as text\n metadata -> 'price' as price,\n metadata -> 'ages' -> 0 as low_age,\n metadata -> 'ages' -> 1 as high_age\nfrom books;\n```\n\n```js\nconst { data, error } = await supabase.from('books').select(`\n title,\n description: metadata->>description,\n price: metadata->price,\n low_age: metadata->ages->0,\n high_age: metadata->ages->1\n `)\n```\n\n```swift\ntry await supabase\n .from(\"books\")\n .select(\n \"\"\"\n title,\n description: metadata->>description,\n price: metadata->price,\n low_age: metadata->ages->0,\n high_age: metadata->ages->1\n \"\"\"\n )\n .execute()\n```\n\n```kotlin\nval data = supabase.from(\"books\").select(Columns.raw(\"\"\"\n title,\n description: metadata->>description,\n price: metadata->price,\n low_age: metadata->ages->0,\n high_age: metadata->ages->1\n\"\"\".trimIndent()))\n```\n\n```python\ndata = supabase.from_('books').select(\"\"\"\n title,\n description: metadata->>description,\n price: metadata->price,\n low_age: metadata->ages->0,\n high_age: metadata->ages->1\n\"\"\"\n).execute()\n```\n\n| title | description | price | low_age | high_age |\n| ----------------------------------- | ------------------------------------------------------------------ | ----- | ------- | -------- |\n| The Poky Little Puppy | Puppy is slower than other, bigger animals. | 5.95 | 3 | 6 |\n| The Tale of Peter Rabbit | Rabbit eats some vegetables. | 4.49 | 2 | 5 |\n| Tootle | Little toy train has big dreams. | 3.99 | 2 | 5 |\n| Green Eggs and Ham | Sam has changing food preferences and eats unusually colored food. | 7.49 | 4 | 8 |\n| Harry Potter and the Goblet of Fire | Fourth year of school starts, big drama ensues. | 24.95 | 10 | 99 |",
          "level": 2
        },
        {
          "type": "section",
          "title": "Validating JSON data",
          "content": "Supabase provides the [`pg_jsonschema` extension](/docs/guides/database/extensions/pg_jsonschema) that adds the ability to validate `json` and `jsonb` data types against [JSON Schema](https://json-schema.org/) documents.\n\nOnce you have enabled the extension, you can add a \"check constraint\" to your table to validate the JSON data:\n\n```sql\ncreate table customers (\n id serial primary key,\n metadata json\n);\n\nalter table customers\nadd constraint check_metadata check (\n json_matches_schema(\n '{\n \"type\": \"object\",\n \"properties\": {\n \"tags\": {\n \"type\": \"array\",\n \"items\": {\n \"type\": \"string\",\n \"maxLength\": 16\n }\n }\n }\n }',\n metadata\n )\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Postgres: JSON Functions and Operators](https://www.postgresql.org/docs/current/functions-json.html)\n- [Postgres JSON types](https://www.postgresql.org/docs/current/datatype-json.html)",
          "level": 2
        }
      ],
      "wordCount": 1655,
      "characterCount": 12011
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-metabase",
      "identifier": "database-metabase",
      "name": "Connecting to Metabase",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/metabase",
      "dateModified": "2025-06-13T12:45:11.288312",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/metabase.mdx",
      "frontmatter": {
        "id": "metabase",
        "title": "Connecting to Metabase",
        "breadcrumb": "GUI Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "[`Metabase`](https://www.metabase.com/) is an Open Source data visualization tool. You can use it to explore your data stored in Supabase.\n\n Create a [Metabase account](https://store.metabase.com/checkout) or deploy locally with [Docker](https://www.docker.com/products/docker-desktop/)\n\n Deploying with Docker:\n ```sh\n docker pull metabase/metabase:latest\n ```\n\n Then run:\n ```sh\n docker run -d -p 3000:3000 --name metabase metabase/metabase\n ```\n\n The server should be available at [`http://localhost:3000/setup`](http://localhost:3000/setup)\n\n Connect your Postgres server to Metabase.\n - On your project dashboard, click [Connect](https://supabase.com/dashboard/project/_?showConnect=true)\n - View parameters under \"Session pooler\"\n\n If you're in an [IPv6 environment](https://supabase.com/docs/guides/platform/ipv4-address#checking-your-network-ipv6-support) or have the [IPv4 Add-On](https://supabase.com/docs/guides/platform/ipv4-address#understanding-ip-addresses), you can use the direct connection string instead of Supavisor in Session mode.\n\n - Enter your database credentials into Metabase\n\n Example credentials:\n ![Name Postgres Server.](/docs/img/guides/database/connecting-to-postgres/metabase/add-pg-server.png)\n\n Explore your data in Metabase\n\n ![explore data](/docs/img/guides/database/connecting-to-postgres/metabase/explore.png)"
        }
      ],
      "wordCount": 117,
      "characterCount": 1349
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-orioledb",
      "identifier": "database-orioledb",
      "name": "OrioleDB Overview",
      "description": "A storage extension for PostgreSQL which uses PostgreSQL's pluggable storage system",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/orioledb",
      "dateModified": "2025-06-13T12:45:11.288501",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/orioledb.mdx",
      "frontmatter": {
        "id": "orioledb",
        "title": "OrioleDB Overview",
        "description": "A storage extension for PostgreSQL which uses PostgreSQL's pluggable storage system"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The [OrioleDB](https://www.orioledb.com/) Postgres extension provides a drop-in replacement storage engine for the default heap storage method. It is designed to improve Postgres' scalability and performance.\n\nOrioleDB addresses PostgreSQL's scalability limitations by removing bottlenecks in the shared memory cache under high concurrency. It also optimizes write-ahead-log (WAL) insertion through row-level WAL logging. These changes lead to significant improvements in the industry standard TPC-C benchmark, which approximates a real-world transactional workload. The following benchmark was performed on a c7g.metal instance and shows OrioleDB's performance outperforming the default Postgres heap method with a 3.3x speedup.\n\nOrioleDB is in active development and currently has [certain limitations](https://www.orioledb.com/docs/usage/getting-started#current-limitations). Currently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available. An Index Access Method bridge to unlock support for all index types used with heap storage is under active development. In the Supabase OrioleDB image the default storage method has been updated to use OrioleDB, granting better performance out of the box."
        },
        {
          "type": "section",
          "title": "Index-organized tables",
          "content": "OrioleDB uses index-organized tables, where table data is stored in the index structure. This design eliminates the need for separate heap storage, reduces overhead and improves lookup performance for primary key queries.",
          "level": 3
        },
        {
          "type": "section",
          "title": "No buffer mapping",
          "content": "In-memory pages are connected to the storage pages using direct links. This allows OrioleDB to bypass PostgreSQL's shared buffer pool and eliminate the associated complexity and contention in buffer mapping.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Undo log",
          "content": "Multi-Version Concurrency Control (MVCC) is implemented using an undo log. The undo log stores previous row versions and transaction information, which enables consistent reads while removing the need for table vacuuming completely.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Copy-on-write checkpoints",
          "content": "OrioleDB implements copy-on-write checkpoints to persist data efficiently. This approach writes only modified data during a checkpoint, reducing the I/O overhead compared to traditional Postgres checkpointing and allowing row-level WAL logging.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Creating OrioleDB project",
          "content": "You can get started with OrioleDB by enabling the extension in your Supabase dashboard.\nTo get started with OrioleDB you need to [create a new Supabase project](https://supabase.com/dashboard/new/_) and choose `OrioleDB Public Alpha` Postgres version.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Creating tables",
          "content": "To create a table using the OrioleDB storage engine just execute the standard `CREATE TABLE` statement. By default it will create a table using OrioleDB storage engine. For example:\n\n```sql\n-- Create a table\ncreate table blog_post (\n id int8 not null,\n title text not null,\n body text not null,\n author text not null,\n published_at timestamptz not null default CURRENT_TIMESTAMP,\n views bigint not null,\n primary key (id)\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Creating indexes",
          "content": "OrioleDB tables always have a primary key. If it wasn't defined explicitly, a hidden primary key is created using the `ctid` column.\nAdditionally you can create secondary indexes.\n\nCurrently, only B-tree indexes are supported, so features like pg_vector's HNSW indexes are not yet available.\n\n```sql\n-- Create an index\ncreate index blog_post_published_at on blog_post (published_at);\n\ncreate index blog_post_views on blog_post (views) where (views > 1000);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Data manipulation",
          "content": "You can query and modify data in OrioleDB tables using standard SQL statements, including `SELECT`, `INSERT`, `UPDATE`, `DELETE` and `INSERT ... ON CONFLICT`.\n\n```sql\nINSERT INTO blog_post (id, title, body, author, views)\nVALUES (1, 'Hello, World!', 'This is my first blog post.', 'John Doe', 1000);\n\nSELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;\n id │ title │ body │ author │ published_at │ views\n────┼───────────────┼─────────────────────────────┼──────────┼───────────────────────────────┼───────\n 1 │ Hello, World! │ This is my first blog post. │ John Doe │ 2024-11-15 12:04:18.756824+01 │ 1000\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Viewing query plans",
          "content": "You can see the execution plan using standard `EXPLAIN` statement.\n\n```sql\nEXPLAIN SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;\n QUERY PLAN\n────────────────────────────────────────────────────────────────────────────────────────────────────────────\n Limit (cost=0.15..1.67 rows=10 width=120)\n -> Index Scan Backward using blog_post_published_at on blog_post (cost=0.15..48.95 rows=320 width=120)\n\nEXPLAIN SELECT * FROM blog_post WHERE id = 1;\n QUERY PLAN\n──────────────────────────────────────────────────────────────────────────────────\n Index Scan using blog_post_pkey on blog_post (cost=0.15..8.17 rows=1 width=120)\n Index Cond: (id = 1)\n\nEXPLAIN (ANALYZE, BUFFERS) SELECT * FROM blog_post ORDER BY published_at DESC LIMIT 10;\n QUERY PLAN\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n Limit (cost=0.15..1.67 rows=10 width=120) (actual time=0.052..0.054 rows=1 loops=1)\n -> Index Scan Backward using blog_post_published_at on blog_post (cost=0.15..48.95 rows=320 width=120) (actual time=0.050..0.052 rows=1 loops=1)\n Planning Time: 0.186 ms\n Execution Time: 0.088 ms\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official OrioleDB documentation](https://www.orioledb.com/docs)\n- [OrioleDB GitHub repository](https://github.com/orioledb/orioledb)",
          "level": 2
        }
      ],
      "wordCount": 699,
      "characterCount": 5509
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-overview",
      "identifier": "database-overview",
      "name": "Database",
      "description": "Use Supabase to manage your data.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/overview",
      "dateModified": "2025-06-13T12:45:11.288637",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/overview.mdx",
      "frontmatter": {
        "id": "database",
        "title": "Database",
        "description": "Use Supabase to manage your data.",
        "sidebar_label": "Overview"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Every Supabase project comes with a full [Postgres](https://www.postgresql.org/) database, a free and open source database which is considered one of the world's most stable and advanced databases."
        },
        {
          "type": "section",
          "title": "Table view",
          "content": "You don't have to be a database expert to start using Supabase. Our table view makes Postgres as easy to use as a spreadsheet.\n\n![Table View.](/docs/img/table-view.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Relationships",
          "content": "Dig into the relationships within your data.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Clone tables",
          "content": "You can duplicate your tables, just like you would inside a spreadsheet.",
          "level": 3
        },
        {
          "type": "section",
          "title": "The SQL editor",
          "content": "Supabase comes with a SQL Editor. You can also save your favorite queries to run later!",
          "level": 3
        },
        {
          "type": "section",
          "title": "Additional features",
          "content": "- Supabase extends Postgres with realtime functionality using our [Realtime Server](https://github.com/supabase/realtime).\n- Every project is a full Postgres database, with `postgres` level access.\n- Supabase manages your database backups.\n- Import data directly from a CSV or excel spreadsheet.\n\nDatabase backups **do not** include objects stored via the Storage API, as the database only includes metadata about these objects. Restoring an old backup does not restore objects that have been deleted since then.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Extensions",
          "content": "To expand the functionality of your Postgres database, you can use extensions.\nYou can enable Postgres extensions with the click of a button within the Supabase dashboard.\n\n[Learn more](/docs/guides/database/extensions) about all the extensions provided on Supabase.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Terminology",
          "content": "{/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}",
          "level": 2
        },
        {
          "type": "section",
          "title": "Postgres or PostgreSQL?",
          "content": "{/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}\n\nPostgreSQL the database was derived from the POSTGRES Project, a package written at the University of California at Berkeley in 1986. This package included a query language called \"PostQUEL\".\n\nIn 1994, Postgres95 was built on top of POSTGRES code, adding an SQL language interpreter as a replacement for PostQUEL.\n{/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}\n\nEventually, Postgres95 was renamed to PostgreSQL to reflect the SQL query capability.\nAfter this, many people referred to it as Postgres since it's less prone to confusion. Supabase is all about simplicity, so we also refer to it as Postgres.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Tips",
          "content": "Read about resetting your database password [here](/docs/guides/database/managing-passwords) and changing the timezone of your server [here](/docs/guides/database/managing-timezones).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Next steps",
          "content": "- Read more about [Postgres](https://www.postgresql.org/about/)\n- Sign in: [supabase.com/dashboard](https://supabase.com/dashboard)",
          "level": 2
        }
      ],
      "wordCount": 352,
      "characterCount": 2613
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-partitions",
      "identifier": "database-partitions",
      "name": "Partitioning tables",
      "description": "Organizing tables into partitions in Postgres.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/partitions",
      "dateModified": "2025-06-13T12:45:11.288873",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/partitions.mdx",
      "frontmatter": {
        "id": "partitions",
        "title": "Partitioning tables",
        "description": "Organizing tables into partitions in Postgres."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Table partitioning is a technique that allows you to divide a large table into smaller, more manageable parts called “partitions”.\n\nEach partition contains a subset of the data based on a specified criteria, such as a range of values or a specific condition. Partitioning can significantly improve query performance and simplify data management for large datasets."
        },
        {
          "type": "section",
          "title": "Benefits of table partitioning",
          "content": "- **Improved query performance:** allows queries to target specific partitions, reducing the amount of data scanned and improving query execution time.\n- **Scalability:** With partitioning, you can add or remove partitions as your data grows or changes, enabling better scalability and flexibility.\n- **Efficient data management:** simplifies tasks such as data loading, archiving, and deletion by operating on smaller partitions instead of the entire table.\n- **Enhanced maintenance operations:** can optimize vacuuming and indexing, leading to faster maintenance tasks.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Partitioning methods",
          "content": "Postgres supports various partitioning methods based on how you want to partition your data. The commonly used methods are:\n\n1. **Range Partitioning**: Data is divided into partitions based on a specified range of values. For example, you can partition a sales table by date, where each partition represents a specific time range (e.g., one partition for each month).\n2. **List Partitioning**: Data is divided into partitions based on a specified list of values. For instance, you can partition a customer table by region, where each partition contains customers from a specific region (e.g., one partition for customers in the US, another for customers in Europe).\n3. **Hash Partitioning**: Data is distributed across partitions using a hash function. This method provides a way to evenly distribute data among partitions, which can be useful for load balancing. However, it doesn't allow direct querying based on specific values.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating partitioned tables",
          "content": "Let's consider an example of range partitioning for a sales table based on the order date. We'll create monthly partitions to store data for each month:\n\n```sql\ncreate table sales (\n id bigint generated by default as identity,\n order_date date not null,\n customer_id bigint,\n amount bigint,\n\n -- We need to include all the\n -- partitioning columns in constraints:\n primary key (order_date, id)\n)\npartition by range (order_date);\n\ncreate table sales_2000_01\n partition of sales\n for values from ('2000-01-01') to ('2000-02-01');\n\ncreate table sales_2000_02\n partition of sales\n for values from ('2000-02-01') to ('2000-03-01');\n\n```\n\nTo create a partitioned table you append `partition by range ()` to the table creation statement. The column that you are partitioning with _must_ be included in any unique index, which is the reason why we specify a composite primary key here (`primary key (order_date, id)`).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Querying partitioned tables",
          "content": "To query a partitioned table, you have two options:\n\n1. Querying the parent table\n2. Querying specific partitions",
          "level": 2
        },
        {
          "type": "section",
          "title": "Querying the parent table",
          "content": "When you query the parent table, Postgres automatically routes the query to the relevant partitions based on the conditions specified in the query. This allows you to retrieve data from all partitions simultaneously.\n\nExample:\n\n```sql\nselect *\nfrom sales\nwhere order_date >= '2000-01-01' and order_date < '2000-03-01';\n```\n\nThis query will retrieve data from both the `sales_2000_01` and `sales_2000_02` partitions.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Querying specific partitions",
          "content": "If you only need to retrieve data from a specific partition, you can directly query that partition instead of the parent table. This approach is useful when you want to target a specific range or condition within a partition.\n\n```sql\nselect *\nfrom sales_2000_02;\n```\n\nThis query will retrieve data only from the `sales_2000_02` partition.",
          "level": 3
        },
        {
          "type": "section",
          "title": "When to partition your tables",
          "content": "There is no real threshold to determine when you should use partitions. Partitions introduce complexity, and complexity should be avoided until it's needed. A few guidelines:\n\n- If you are considering performance, avoid partitions until you see performance degradation on non-partitioned tables.\n- If you are using partitions as a management tool, it's fine to create the partitions any time.\n- If you don't know how you should partition your data, then it's probably too early.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Examples",
          "content": "Here are simple examples for each of the partitioning types in Postgres.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Range partitioning",
          "content": "Let's consider a range partitioning example for a table that stores sales data based on the order date. We'll create monthly partitions to store data for each month.\n\nIn this example, the **`sales`** table is partitioned into two partitions: **`sales_january`** and **`sales_february`**. The data in these partitions is based on the specified range of order dates:\n\n```sql\ncreate table sales (\n id bigint generated by default as identity,\n order_date date not null,\n customer_id bigint,\n amount bigint,\n\n -- We need to include all the\n -- partitioning columns in constraints:\n primary key (order_date, id)\n)\npartition by range (order_date);\n\ncreate table sales_2000_01\n partition of sales\n for values from ('2000-01-01') to ('2000-02-01');\n\ncreate table sales_2000_02\n partition of sales\n for values from ('2000-02-01') to ('2000-03-01');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "List partitioning",
          "content": "Let's consider a list partitioning example for a table that stores customer data based on their region. We'll create partitions to store customers from different regions.\n\nIn this example, the **`customers`** table is partitioned into two partitions: `customers_americas` and `customers_asia`. The data in these partitions is based on the specified list of regions:\n\n```sql\n-- Create the partitioned table\ncreate table customers (\n id bigint generated by default as identity,\n name text,\n country text,\n\n -- We need to include all the\n -- partitioning columns in constraints:\n primary key (country, id)\n)\npartition by list(country);\n\ncreate table customers_americas\n partition of customers\n for values in ('US', 'CANADA');\n\ncreate table customers_asia\n partition of customers\n for values in ('INDIA', 'CHINA', 'JAPAN');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Hash partitioning",
          "content": "You can use hash partitioning to evenly distribute data.\n\nIn this example, the **`products`** table is partitioned into two partitions: `products_one` and `products_two`. The data is distributed across these partitions using a hash function:\n\n```sql\ncreate table products (\n id bigint generated by default as identity,\n name text,\n category text,\n price bigint\n)\npartition by hash (id);\n\ncreate table products_one\n partition of products\n for values with (modulus 2, remainder 1);\n\ncreate table products_two\n partition of products\n for values with (modulus 2, remainder 0);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Other tools",
          "content": "There are several other tools available for Postgres partitioning, most notably [pg_partman](https://github.com/pgpartman/pg_partman). Native partitioning was introduced in Postgres 10 and is generally thought to have better performance.",
          "level": 2
        }
      ],
      "wordCount": 1035,
      "characterCount": 7016
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-pgadmin",
      "identifier": "database-pgadmin",
      "name": "Connecting with pgAdmin",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/pgadmin",
      "dateModified": "2025-06-13T12:45:11.289008",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/pgadmin.mdx",
      "frontmatter": {
        "id": "pgadmin",
        "title": "Connecting with pgAdmin",
        "breadcrumb": "GUI Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "section",
          "title": "What is pgAdmin?",
          "content": "[`pgAdmin`](https://www.pgadmin.org/) is a GUI tool for managing Postgres databases. You can use it to connect to your database via SSL.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Connecting pgAdmin with your Postgres database",
          "content": "Register a new Postgres server.\n\n Name your server.\n\n ![Name Postgres Server.](/docs/img/guides/database/connecting-to-postgres/pgadmin/name-pg-server.png)\n\n Add the connection info. Go to your [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database). Make sure `Use connection pooling` is enabled. Switch the connection mode to `Session` and copy your connection parameters. Fill in your Database password that you made when creating your project (It can be reset in Database Settings above if you don't have it).\n\n ![Add Connection Info.](/docs/img/guides/database/connecting-to-postgres/pgadmin/add-pg-server-conn-info.png)\n\n Download your SSL certificate from Dashboard's [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database).\n\n In pgAdmin, navigate to the Parameters tab and select connection parameter as Root Certificate. Next navigate to the Root certificate input, it will open up a file-picker modal. Select the certificate you downloaded earlier and save the server details. pgAdmin should now be able to connect to your Postgres via SSL.\n\n ![Add Connection Info.](/docs/img/guides/database/connecting-to-postgres/pgadmin/database-settings-host.png)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Why connect to pgAdmin",
          "content": "Connecting your Postgres instance to `pgAdmin` gives you a free, cross-platform GUI that makes tasks such as browsing objects, writing queries with autocomplete, running backups, and monitoring performance much faster and safer than using `psql` alone.\n\nIt acts as a single control panel where you can manage multiple servers, inspect locks and slow queries in real time, and perform maintenance operations with a click.\n\nFor scripted migrations or ultra-light remote work you’ll still lean on plain SQL or CLI tools, but most teams find `pgAdmin` invaluable for exploration and routine administration.",
          "level": 2
        }
      ],
      "wordCount": 255,
      "characterCount": 2055
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-js",
      "identifier": "database-postgres-js",
      "name": "Postgres.js",
      "description": "Postgres.js Quickstart",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres-js",
      "dateModified": "2025-06-13T12:45:11.289098",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres-js.mdx",
      "frontmatter": {
        "id": "postgres-js",
        "title": "Postgres.js",
        "description": "Postgres.js Quickstart",
        "breadcrumb": "ORM Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "section",
          "title": "Connecting with Postgres.js",
          "content": "[Postgres.js](https://github.com/porsager/postgres) is a full-featured Postgres client for Node.js and Deno.\n\n Install Postgres.js and related dependencies.\n\n ```shell\n npm i postgres\n ```\n\n Create a `db.js` file with the connection details.\n\n To get your connection details, go to your [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database). Make sure `Use connection pooling` is enabled. Choose `Transaction Mode` if you're on a platform with transient connections, such as a serverless function, and `Session Mode` if you have a long-lived connection. Copy the URI and save it as the environment variable `DATABASE_URL`.\n\n ```ts\n // db.js\n import postgres from 'postgres'\n\n const connectionString = process.env.DATABASE_URL\n const sql = postgres(connectionString)\n\n export default sql\n ```\n\n Use the connection to execute commands.\n\n ```ts\n import sql from './db.js'\n\n async function getUsersOver(age) {\n const users = await sql`\n select name, age\n from users\n where age > ${ age }\n `\n // users = Result [{ name: \"Walter\", age: 80 }, { name: 'Murray', age: 68 }, ...]\n return users\n }\n ```",
          "level": 3
        }
      ],
      "wordCount": 157,
      "characterCount": 1152
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-cascade-deletes",
      "identifier": "database-postgres-cascade-deletes",
      "name": "Cascade Deletes",
      "description": "Understand the types of foreign key constraint deletes",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/cascade-deletes",
      "dateModified": "2025-06-13T12:45:11.289317",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/cascade-deletes.mdx",
      "frontmatter": {
        "title": "Cascade Deletes",
        "description": "Understand the types of foreign key constraint deletes",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "There are 5 options for foreign key constraint deletes:\n\n1. **CASCADE:** When a row is deleted from the parent table, all related rows in the child tables are deleted as well.\n2. **RESTRICT:** When a row is deleted from the parent table, the delete operation is aborted if there are any related rows in the child tables.\n3. **SET NULL:** When a row is deleted from the parent table, the values of the foreign key columns in the child tables are set to NULL.\n4. **SET DEFAULT:** When a row is deleted from the parent table, the values of the foreign key columns in the child tables are set to their default values.\n5. **NO ACTION:** This option is similar to RESTRICT, but it also has the option to be “deferred” to the end of a transaction. This means that other cascading deletes can run first, and then this delete constraint will only throw an error if there is referenced data remaining _at the end of the transaction_.\n\nThese options can be specified when defining a foreign key constraint using the \"ON DELETE\" clause. For example, the following SQL statement creates a foreign key constraint with the `CASCADE` option:\n\n```sql\nalter table child_table\nadd constraint fk_parent foreign key (parent_id) references parent_table (id)\n on delete cascade;\n```\n\nThis means that when a row is deleted from the `parent_table`, all related rows in the `child_table` will be deleted as well."
        },
        {
          "type": "section",
          "title": "`RESTRICT` vs `NO ACTION`",
          "content": "The difference between `NO ACTION` and `RESTRICT` is subtle and can be a bit confusing.\n\nBoth `NO ACTION` and `RESTRICT` are used to prevent deletion of a row in a parent table if there are related rows in a child table. However, there is a subtle difference in how they behave.\n\nWhen a foreign key constraint is defined with the option `RESTRICT`, it means that if a row in the parent table is deleted, the database will immediately raise an error and prevent the deletion of the row in the parent table. The database will not delete, update or set to NULL any rows in the referenced tables.\n\nWhen a foreign key constraint is defined with the option `NO ACTION`, it means that if a row in the parent table is deleted, the database will also raise an error and prevent the deletion of the row in the parent table. However unlike `RESTRICT`, `NO ACTION` has the option defer the check using `INITIALLY DEFERRED`. This will only raise the above error _if_ the referenced rows still exist at the end of the transaction.\n\nThe difference from `RESTRICT` is that a constraint marked as `NO ACTION INITIALLY DEFERRED` is deferred until the end of the transaction, rather than running immediately. If, for example there is another foreign key constraint between the same tables marked as `CASCADE`, the cascade will occur first and delete the referenced rows, and no error will be thrown by the deferred constraint. Otherwise if there are still rows referencing the parent row by the end of the transaction, an error will be raised just like before. Just like `RESTRICT`, the database will not delete, update or set to NULL any rows in the referenced tables.\n\nIn practice, you can use either `NO ACTION` or `RESTRICT` depending on your needs. `NO ACTION` is the default behavior if you do not specify anything. If you prefer to defer the check until the end of the transaction, use `NO ACTION INITIALLY DEFERRED`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example",
          "content": "Let's further illustrate the difference with an example. We'll use the following data:\n\n`grandparent`\n\n| id | name |\n| --- | --------- |\n| 1 | Elizabeth |\n\n`parent`\n\n| id | name | `parent_id` |\n| --- | ------- | ----------- |\n| 1 | Charles | 1 |\n| 2 | Diana | 1 |\n\n`child`\n\n| id | name | father | mother |\n| --- | ------- | ------ | ------ |\n| 1 | William | 1 | 2 |\n\nTo create these tables and their data, we run:\n\n```sql\ncreate table grandparent (\n id serial primary key,\n name text\n);\n\ncreate table parent (\n id serial primary key,\n name text,\n parent_id integer references grandparent (id)\n on delete cascade\n);\n\ncreate table child (\n id serial primary key,\n name text,\n father integer references parent (id)\n on delete restrict\n);\n\ninsert into grandparent\n (id, name)\nvalues\n (1, 'Elizabeth');\n\ninsert into parent\n (id, name, parent_id)\nvalues\n (1, 'Charles', 1);\n\ninsert into parent\n (id, name, parent_id)\nvalues\n (2, 'Diana', 1);\n\n-- We'll just link the father for now\ninsert into child\n (id, name, father)\nvalues\n (1, 'William', 1);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "`RESTRICT`",
          "content": "`RESTRICT` will prevent a delete and raise an error:\n\n```shell\npostgres=# delete from grandparent;\nERROR: update or delete on table \"parent\" violates foreign key constraint \"child_father_fkey\" on table \"child\"\nDETAIL: Key (id)=(1) is still referenced from table \"child\".\n```\n\nEven though the foreign key constraint between parent and grandparent is `CASCADE`, the constraint between child and father is `RESTRICT`. Therefore an error is raised and no records are deleted.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`NO ACTION`",
          "content": "Let's change the child-father relationship to `NO ACTION`:\n\n```sql\nalter table child\ndrop constraint child_father_fkey;\n\nalter table child\nadd constraint child_father_fkey foreign key (father) references parent (id)\n on delete no action;\n```\n\nWe see that `NO ACTION` will also prevent a delete and raise an error:\n\n```shell\npostgres=# delete from grandparent;\nERROR: update or delete on table \"parent\" violates foreign key constraint \"child_father_fkey\" on table \"child\"\nDETAIL: Key (id)=(1) is still referenced from table \"child\".\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`NO ACTION INITIALLY DEFERRED`",
          "content": "We'll change the foreign key constraint between child and father to be `NO ACTION INITIALLY DEFERRED`:\n\n```sql\nalter table child\ndrop constraint child_father_fkey;\n\nalter table child\nadd constraint child_father_fkey foreign key (father) references parent (id)\n on delete no action initially deferred;\n```\n\nHere you will see that `INITIALLY DEFFERED` seems to operate like `NO ACTION` or `RESTRICT`. When we run a delete, it seems to make no difference:\n\n```shell\npostgres=# delete from grandparent;\nERROR: update or delete on table \"parent\" violates foreign key constraint \"child_father_fkey\" on table \"child\"\nDETAIL: Key (id)=(1) is still referenced from table \"child\".\n```\n\nBut, when we combine it with _other_ constraints, then any other constraints take precedence. For example, let's run the same but add a `mother` column that has a `CASCADE` delete:\n\n```sql\nalter table child\nadd column mother integer references parent (id)\n on delete cascade;\n\nupdate child\nset mother = 2\nwhere id = 1;\n```\n\nThen let's run a delete on the `grandparent` table:\n\n```shell\npostgres=# delete from grandparent;\nDELETE 1\n\npostgres=# select * from parent;\n id | name | parent_id\n----+------+-----------\n(0 rows)\n\npostgres=# select * from child;\n id | name | father | mother\n----+------+--------+--------\n(0 rows)\n```\n\nThe `mother` deletion took precedence over the `father`, and so William was deleted. After William was deleted, there was no reference to “Charles” and so he was free to be deleted, even though previously he wasn't (without `INITIALLY DEFERRED`).",
          "level": 3
        }
      ],
      "wordCount": 1161,
      "characterCount": 7010
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-column-level-security",
      "identifier": "database-postgres-column-level-security",
      "name": "Column Level Security",
      "description": "Secure your data using Postgres Column Level Security.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/column-level-security",
      "dateModified": "2025-06-13T12:45:11.289513",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/column-level-security.mdx",
      "frontmatter": {
        "id": "column-level-security",
        "title": "Column Level Security",
        "description": "Secure your data using Postgres Column Level Security."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "PostgreSQL's [Row Level Security (RLS)](https://www.postgresql.org/docs/current/ddl-rowsecurity.html) gives you granular control over who can access rows of data. However, it doesn't give you control over which columns they can access within rows. Sometimes you want to restrict access to specific columns in your database. Column Level Privileges allows you to do just that.\n\nThis is an advanced feature. We do not recommend using column-level privileges for most users. Instead, we recommend using RLS policies in combination with a dedicated table for handling user roles.\n\nRestricted roles cannot use the wildcard operator (`*`) on the affected table. Instead of using `SELECT * FROM ;` or its API equivalent, you must specify the column names explicitly."
        },
        {
          "type": "section",
          "title": "Policies at the row level",
          "content": "Policies in Row Level Security (RLS) are used to restrict access to rows in a table. Think of them like adding a `WHERE` clause to every query.\n\nFor example, let's assume you have a `posts` table with the following columns:\n\n- `id`\n- `user_id`\n- `title`\n- `content`\n- `created_at`\n- `updated_at`\n\nYou can restrict updates to just the user who created it using [RLS](/docs/guides/auth#row-level-security), with the following policy:\n\n```sql\ncreate policy \"Allow update for owners\" on posts for\nupdate\n using ((select auth.uid()) = user_id);\n```\n\nHowever, this gives the post owner full access to update the row, including all of the columns.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Privileges at the column level",
          "content": "To restrict access to columns, you can use [Privileges](https://www.postgresql.org/docs/current/ddl-priv.html).\n\nThere are two types of privileges in Postgres:\n\n1. **table-level**: Grants the privilege on all columns in the table.\n2. **column-level** Grants the privilege on a specific column in the table.\n\nYou can have both types of privileges on the same table. If you have both, and you revoke the column-level privilege, the table-level privilege will still be in effect.\n\nBy default, our table will have a table-level `UPDATE` privilege, which means that the `authenticated` role can update all the columns in the table.\n\n```sql\nrevoke\nupdate\n on table public.posts\nfrom\n authenticated;\n\ngrant\nupdate\n (title, content) on table public.posts to authenticated;\n```\n\nIn the above example, we are revoking the table-level `UPDATE` privilege from the `authenticated` role and granting a column-level `UPDATE` privilege on just the `title` and `content` columns.\n\nIf we want to restrict access to updating the `title` column:\n\n```sql\nrevoke\nupdate\n (title) on table public.posts\nfrom\n authenticated;\n```\n\nThis time, we are revoking the column-level `UPDATE` privilege of the `title` column from the `authenticated` role. We didn't need to revoke the table-level `UPDATE` privilege because it's already revoked.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Manage column privileges in the Dashboard",
          "content": "Column-level privileges are a powerful tool, but they're also quite advanced and in many cases, not the best fit for common access control needs. For that reason, we've intentionally moved the UI for this feature under the Feature Preview section in the dashboard.\n\nYou can view and edit the privileges in the [Supabase Studio](https://supabase.com/dashboard/project/_/database/column-privileges).\n\n![Column level privileges](/docs/img/guides/privileges/column-level-privileges-2.png)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Manage column privileges in migrations",
          "content": "While you can manage privileges directly from the Dashboard, as your project grows you may want to manage them in your migrations. Read about database migrations in the [Local Development](https://supabase.com/docs/guides/deployment/database-migrations) guide.\n\n To get started, generate a [new migration](https://supabase.com/docs/reference/cli/supabase-migration-new) to store the SQL needed to create your table along with row and column-level privileges.\n\n```bash\nsupabase migration new create_posts_table\n```\n\n This creates a new migration: supabase/migrations/\\\n _create_posts_table.sql.\n\n To that file, add the SQL to create this `posts` table with row and column-level privileges.\n\n ```sql\n create table\n posts (\n id bigint primary key generated always as identity,\n user_id text,\n title text,\n content text,\n created_at timestamptz default now()\n updated_at timestamptz default now()\n );\n\n -- Add row-level security\n create policy \"Allow update for owners\" on posts for\n update\n using ((select auth.uid()) = user_id);\n\n -- Add column-level security\n revoke\n update\n (title) on table public.posts\n from\n authenticated;\n ```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Considerations when using column-level privileges",
          "content": "- If you turn off a column privilege you won't be able to use that column at all.\n- All operations (insert, update, delete) as well as using `select *` will fail.",
          "level": 2
        }
      ],
      "wordCount": 667,
      "characterCount": 4704
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-configuration",
      "identifier": "database-postgres-configuration",
      "name": "Database configuration",
      "description": "Updating the default configuration for your Postgres database.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/configuration",
      "dateModified": "2025-06-13T12:45:11.289626",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/configuration.mdx",
      "frontmatter": {
        "id": "postgres-configuration",
        "title": "Database configuration",
        "slug": "postgres-configuration",
        "description": "Updating the default configuration for your Postgres database.",
        "subtitle": "Updating the default configuration for your Postgres database."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres provides a set of sensible defaults for you database size. In some cases, these defaults can be updated. We do not recommend changing these defaults unless you know what you're doing."
        },
        {
          "type": "section",
          "title": "Timeouts",
          "content": "See the [Timeouts](/docs/guides/database/postgres/timeouts) section.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Statement optimization",
          "content": "All Supabase projects come with the [`pg_stat_statements`](https://www.postgresql.org/docs/current/pgstatstatements.html) extension installed, which tracks planning and execution statistics for all statements executed against it. These statistics can be used in order to diagnose the performance of your project.\n\nThis data can further be used in conjunction with the [`explain`](https://www.postgresql.org/docs/current/using-explain.html) functionality of Postgres to optimize your usage.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Managing timezones",
          "content": "Every hosted Supabase database is set to UTC timezone by default. We strongly recommend keeping it this way, even if your users are in a different location. This is because it makes it much easier to calculate differences between timezones if you adopt the mental model that everything in your database is in UTC time.\n\nOn self-hosted databases, the timezone defaults to your local timezone. We recommend [changing this to UTC](/docs/guides/database/postgres/configuration#change-timezone) for the same reasons.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Change timezone",
          "content": "```sql\nalter database postgres\nset timezone to 'America/New_York';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Full list of timezones",
          "content": "Get a full list of timezones supported by your database. This will return the following columns:\n\n- `name`: Time zone name\n- `abbrev`: Time zone abbreviation\n- `utc_offset`: Offset from UTC (positive means east of Greenwich)\n- `is_dst`: True if currently observing daylight savings\n\n```sql\nselect name, abbrev, utc_offset, is_dst\nfrom pg_timezone_names()\norder by name;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Search for a specific timezone",
          "content": "Use `ilike` (case insensitive search) to find specific timezones.\n\n```sql\nselect *\nfrom pg_timezone_names()\nwhere name ilike '%york%';\n```",
          "level": 3
        }
      ],
      "wordCount": 270,
      "characterCount": 2001
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-custom-claims-and-role-based-access-control-rbac",
      "identifier": "database-postgres-custom-claims-and-role-based-access-control-rbac",
      "name": "Custom Claims & Role-based Access Control (RBAC)",
      "description": "Use Auth Hooks to add custom claims for managing role-based access control.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/custom-claims-and-role-based-access-control-rbac",
      "dateModified": "2025-06-13T12:45:11.289849",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/custom-claims-and-role-based-access-control-rbac.mdx",
      "frontmatter": {
        "id": "custom-claims-and-role-based-access-control-rbac",
        "title": "Custom Claims & Role-based Access Control (RBAC)",
        "description": "Use Auth Hooks to add custom claims for managing role-based access control."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Custom Claims are special attributes attached to a user that you can use to control access to portions of your application. For example:\n\n```json\n{\n \"user_role\": \"admin\",\n \"plan\": \"TRIAL\",\n \"user_level\": 100,\n \"group_name\": \"Super Guild!\",\n \"joined_on\": \"2022-05-20T14:28:18.217Z\",\n \"group_manager\": false,\n \"items\": [\"toothpick\", \"string\", \"ring\"]\n}\n```\n\nTo implement Role-Based Access Control (RBAC) with `custom claims`, use a [Custom Access Token Auth Hook](/docs/guides/auth/auth-hooks#hook-custom-access-token). This hook runs before a token is issued. You can use it to add additional claims to the user's JWT.\n\nThis guide uses the [Slack Clone example](https://github.com/supabase/supabase/tree/master/examples/slack-clone/nextjs-slack-clone) to demonstrate how to add a `user_role` claim and use it in your [Row Level Security (RLS) policies](/docs/guides/database/postgres/row-level-security)."
        },
        {
          "type": "section",
          "title": "Create a table to track user roles and permissions",
          "content": "In this example, you will implement two user roles with specific permissions:\n\n- `moderator`: A moderator can delete all messages but not channels.\n- `admin`: An admin can delete all messages and channels.\n\n```sql supabase/migrations/init.sql\n-- Custom types\ncreate type public.app_permission as enum ('channels.delete', 'messages.delete');\ncreate type public.app_role as enum ('admin', 'moderator');\n\n-- USER ROLES\ncreate table public.user_roles (\n id bigint generated by default as identity primary key,\n user_id uuid references auth.users on delete cascade not null,\n role app_role not null,\n unique (user_id, role)\n);\ncomment on table public.user_roles is 'Application roles for each user.';\n\n-- ROLE PERMISSIONS\ncreate table public.role_permissions (\n id bigint generated by default as identity primary key,\n role app_role not null,\n permission app_permission not null,\n unique (role, permission)\n);\ncomment on table public.role_permissions is 'Application permissions for each role.';\n```\n\nFor the [full schema](https://github.com/supabase/supabase/blob/master/examples/slack-clone/nextjs-slack-clone/README.md), see the example application on [GitHub](https://github.com/supabase/supabase/tree/master/examples/slack-clone/nextjs-slack-clone).\n\nYou can now manage your roles and permissions in SQL. For example, to add the mentioned roles and permissions from above, run:\n\n```sql supabase/seed.sql\ninsert into public.role_permissions (role, permission)\nvalues\n ('admin', 'channels.delete'),\n ('admin', 'messages.delete'),\n ('moderator', 'messages.delete');\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create Auth Hook to apply user role",
          "content": "The [Custom Access Token Auth Hook](/docs/guides/auth/auth-hooks#hook-custom-access-token) runs before a token is issued. You can use it to edit the JWT.\n\n```sql supabase/migrations/auth_hook.sql\n-- Create the auth hook function\ncreate or replace function public.custom_access_token_hook(event jsonb)\nreturns jsonb\nlanguage plpgsql\nstable\nas $$\n declare\n claims jsonb;\n user_role public.app_role;\n begin\n -- Fetch the user role in the user_roles table\n select role into user_role from public.user_roles where user_id = (event->>'user_id')::uuid;\n\n claims := event->'claims';\n\n if user_role is not null then\n -- Set the claim\n claims := jsonb_set(claims, '{user_role}', to_jsonb(user_role));\n else\n claims := jsonb_set(claims, '{user_role}', 'null');\n end if;\n\n -- Update the 'claims' object in the original event\n event := jsonb_set(event, '{claims}', claims);\n\n -- Return the modified or original event\n return event;\n end;\n$$;\n\ngrant usage on schema public to supabase_auth_admin;\n\ngrant execute\n on function public.custom_access_token_hook\n to supabase_auth_admin;\n\nrevoke execute\n on function public.custom_access_token_hook\n from authenticated, anon, public;\n\ngrant all\n on table public.user_roles\nto supabase_auth_admin;\n\nrevoke all\n on table public.user_roles\n from authenticated, anon, public;\n\ncreate policy \"Allow auth admin to read user roles\" ON public.user_roles\nas permissive for select\nto supabase_auth_admin\nusing (true)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the hook",
          "content": "In the dashboard, navigate to [`Authentication > Hooks (Beta)`](/dashboard/project/_/auth/hooks) and select the appropriate Postgres function from the dropdown menu.\n\nWhen developing locally, follow the [local development](/docs/guides/auth/auth-hooks#local-development) instructions.\n\nTo learn more about Auth Hooks, see the [Auth Hooks docs](/docs/guides/auth/auth-hooks).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Accessing custom claims in RLS policies",
          "content": "To utilize Role-Based Access Control (RBAC) in Row Level Security (RLS) policies, create an `authorize` method that reads the user's role from their JWT and checks the role's permissions:\n\n```sql supabase/migrations/init.sql\ncreate or replace function public.authorize(\n requested_permission app_permission\n)\nreturns boolean as $$\ndeclare\n bind_permissions int;\n user_role public.app_role;\nbegin\n -- Fetch user role once and store it to reduce number of calls\n select (auth.jwt() ->> 'user_role')::public.app_role into user_role;\n\n select count(*)\n into bind_permissions\n from public.role_permissions\n where role_permissions.permission = requested_permission\n and role_permissions.role = user_role;\n\n return bind_permissions > 0;\nend;\n$$ language plpgsql stable security definer set search_path = '';\n```\n\nYou can read more about using functions in RLS policies in the [RLS guide](/docs/guides/database/postgres/row-level-security#using-functions).\n\nYou can then use the `authorize` method within your RLS policies. For example, to enable the desired delete access, you would add the following policies:\n\n```sql\ncreate policy \"Allow authorized delete access\" on public.channels for delete to authenticated using ( (SELECT authorize('channels.delete')) );\ncreate policy \"Allow authorized delete access\" on public.messages for delete to authenticated using ( (SELECT authorize('messages.delete')) );\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Accessing custom claims in your application",
          "content": "The auth hook will only modify the access token JWT but not the auth response. Therefore, to access the custom claims in your application, e.g. your browser client, or server-side middleware, you will need to decode the `access_token` JWT on the auth session.\n\nIn a JavaScript client application you can for example use the [`jwt-decode` package](https://www.npmjs.com/package/jwt-decode):\n\n```js\nimport { jwtDecode } from 'jwt-decode'\n\nconst { subscription: authListener } = supabase.auth.onAuthStateChange(async (event, session) => {\n if (session) {\n const jwt = jwtDecode(session.access_token)\n const userRole = jwt.user_role\n }\n})\n```\n\nFor server-side logic you can use packages like [`express-jwt`](https://github.com/auth0/express-jwt), [`koa-jwt`](https://github.com/stiang/koa-jwt), [`PyJWT`](https://github.com/jpadilla/pyjwt), [dart_jsonwebtoken](https://pub.dev/packages/dart_jsonwebtoken), [Microsoft.AspNetCore.Authentication.JwtBearer](https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer), etc.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Conclusion",
          "content": "You now have a robust system in place to manage user roles and permissions within your database that automatically propagates to Supabase Auth.",
          "level": 2
        },
        {
          "type": "section",
          "title": "More resources",
          "content": "- [Auth Hooks](/docs/guides/auth/auth-hooks)\n- [Row Level Security](/docs/guides/database/postgres/row-level-security)\n- [RLS Functions](/docs/guides/database/postgres/row-level-security#using-functions)\n- [Next.js Slack Clone Example](https://github.com/supabase/supabase/tree/master/examples/slack-clone/nextjs-slack-clone)",
          "level": 2
        }
      ],
      "wordCount": 854,
      "characterCount": 7441
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-dropping-all-tables-in-schema",
      "identifier": "database-postgres-dropping-all-tables-in-schema",
      "name": "Drop all tables in a PostgreSQL schema",
      "description": "Useful snippet for deleting all tables in a given schema",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/dropping-all-tables-in-schema",
      "dateModified": "2025-06-13T12:45:11.289940",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/dropping-all-tables-in-schema.mdx",
      "frontmatter": {
        "title": "Drop all tables in a PostgreSQL schema",
        "description": "Useful snippet for deleting all tables in a given schema",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Execute the following query to drop all tables in a given schema.\nReplace `my-schema-name` with the name of your schema. In Supabase, the default schema is `public`.\n\nThis deletes all tables and their associated data. Ensure you have a recent [backup](/docs/guides/platform/backups) before proceeding.\n\n```sql\ndo $$ declare\n r record;\nbegin\n for r in (select tablename from pg_tables where schemaname = 'my-schema-name') loop\n execute 'drop table if exists ' || quote_ident(r.tablename) || ' cascade';\n end loop;\nend $$;\n```\n\nThis query works by listing out all the tables in the given schema and then executing a `drop table` for each (hence the `for... loop`).\n\nYou can run this query using the [SQL Editor](https://supabase.com/dashboard/project/_/sql) in the Supabase Dashboard, or via `psql` if you're [connecting directly to the database](/docs/guides/database/connecting-to-postgres#direct-connections)."
        }
      ],
      "wordCount": 126,
      "characterCount": 910
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-enums",
      "identifier": "database-postgres-enums",
      "name": "Managing Enums in Postgres",
      "description": "Define a strict set of values that can be used in table and function definitions.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/enums",
      "dateModified": "2025-06-13T12:45:11.290175",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/enums.mdx",
      "frontmatter": {
        "id": "enums",
        "title": "Managing Enums in Postgres",
        "description": "Define a strict set of values that can be used in table and function definitions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Enums in Postgres are a custom data type. They allow you to define a set of values (or labels) that a column can hold. They are useful when you have a fixed set of possible values for a column."
        },
        {
          "type": "section",
          "title": "Creating enums",
          "content": "You can define a Postgres Enum using the `create type` statement. Here's an example:\n\n{/* prettier-ignore */}\n```sql\ncreate type mood as enum (\n 'happy',\n 'sad',\n 'excited',\n 'calm'\n);\n```\n\nIn this example, we've created an Enum called \"mood\" with four possible values.",
          "level": 2
        },
        {
          "type": "section",
          "title": "When to use enums",
          "content": "There is a lot of overlap between Enums and foreign keys. Both can be used to define a set of values for a column. However, there are some advantages to using Enums:\n\n- Performance: You can query a single table instead of finding the value from a lookup table.\n- Simplicity: Generally the SQL is easier to read and write.\n\nThere are also some disadvantages to using Enums:\n\n- Limited Flexibility: Adding and removing values requires modifying the database schema (i.e.: using migrations) rather than adding data to a table.\n- Maintenance Overhead: Enum types require ongoing maintenance. If your application's requirements change frequently, maintaining enums can become burdensome.\n\nIn general you should only use Enums when the list of values is small, fixed, and unlikely to change often. Things like \"a list of continents\" or \"a list of departments\" are good candidates for Enums.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using enums in tables",
          "content": "To use the Enum in a table, you can define a column with the Enum type. For example:\n\n{/* prettier-ignore */}\n```sql\ncreate table person (\n id serial primary key,\n name text,\n current_mood mood\n);\n```\n\nHere, the `current_mood` column can only have values from the \"mood\" Enum.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Inserting data with enums",
          "content": "You can insert data into a table with Enum columns by specifying one of the Enum values:\n\n{/* prettier-ignore */}\n```sql\ninsert into person\n (name, current_mood)\nvalues\n ('Alice', 'happy');\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Querying data with enums",
          "content": "When querying data, you can filter and compare Enum values as usual:\n\n{/* prettier-ignore */}\n```sql\nselect * \nfrom person \nwhere current_mood = 'sad';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Managing enums",
          "content": "You can manage your Enums using the `alter type` statement. Here are some examples:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Updating enum values",
          "content": "You can update the value of an Enum column:\n\n{/* prettier-ignore */}\n```sql\nupdate person\nset current_mood = 'excited'\nwhere name = 'Alice';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Adding enum values",
          "content": "To add new values to an existing Postgres Enum, you can use the `ALTER TYPE` statement. Here's how you can do it:\n\nLet's say you have an existing Enum called `mood`, and you want to add a new value, `content`:\n\n{/* prettier-ignore */}\n```sql\nalter type mood add value 'content';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Removing enum values",
          "content": "Even though it is possible, it is unsafe to remove enum values once they have been created. It's better to leave the enum value in place.\n\nRead the [Postgres mailing list](https://www.postgresql.org/message-id/21012.1459434338%40sss.pgh.pa.us) for more information:\n\nThere is no `ALTER TYPE DELETE VALUE` in Postgres. Even if you delete every occurrence of an Enum value within a table (and vacuumed away those rows), the target value could still exist in upper index pages. If you delete the `pg_enum` entry you'll break the index.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Getting a list of enum values",
          "content": "Check your existing Enum values by querying the enum_range function:\n\n{/* prettier-ignore */}\n```sql\nselect enum_range(null::mood);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official Postgres Docs: [Enumerated Types](https://www.postgresql.org/docs/current/datatype-enum.html)",
          "level": 2
        }
      ],
      "wordCount": 570,
      "characterCount": 3544
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-first-row-in-group",
      "identifier": "database-postgres-first-row-in-group",
      "name": "Select first row for each group in PostgreSQL",
      "description": "PostgreSQL snippet for grabbing the first row in each distinct group by group",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/first-row-in-group",
      "dateModified": "2025-06-13T12:45:11.290302",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/first-row-in-group.mdx",
      "frontmatter": {
        "title": "Select first row for each group in PostgreSQL",
        "description": "PostgreSQL snippet for grabbing the first row in each distinct group by group",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Given a table `seasons`:\n\n| id | team | points |\n| --- | :-------: | -----: |\n| 1 | Liverpool | 82 |\n| 2 | Liverpool | 84 |\n| 3 | Brighton | 34 |\n| 4 | Brighton | 28 |\n| 5 | Liverpool | 79 |\n\nWe want to find the rows containing the maximum number of points _per team_.\n\nThe expected output we want is:\n\n| id | team | points |\n| --- | :-------: | -----: |\n| 3 | Brighton | 34 |\n| 2 | Liverpool | 84 |\n\nFrom the [SQL Editor](https://supabase.com/dashboard/project/_/sql), you can run a query like:\n\n```sql\nselect distinct\n on (team) id,\n team,\n points\nfrom\n seasons\norder BY\n id,\n points desc,\n team;\n```\n\nThe important bits here are:\n\n- The `desc` keyword to order the `points` from highest to lowest.\n- The `distinct` keyword that tells Postgres to only return a single row per team.\n\nThis query can also be executed via `psql` or any other query editor if you prefer to [connect directly to the database](/docs/guides/database/connecting-to-postgres#direct-connections)."
        }
      ],
      "wordCount": 182,
      "characterCount": 971
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-indexes",
      "identifier": "database-postgres-indexes",
      "name": "Managing Indexes in PostgreSQL",
      "description": "Improve query performance using various index types in Postgres",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/indexes",
      "dateModified": "2025-06-13T12:45:11.290496",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/indexes.mdx",
      "frontmatter": {
        "title": "Managing Indexes in PostgreSQL",
        "description": "Improve query performance using various index types in Postgres",
        "footerHelpType": "postgres",
        "tocVideo": "bBu_V8CfWgM"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "An index makes your Postgres queries faster. The index is like a \"table of contents\" for your data - a reference list which allows queries to quickly locate a row in a given table without needing to scan the entire table (which in large tables can take a long time).\n\nIndexes can be structured in a few different ways. The type of index chosen depends on the values you are indexing. By far the most common index type, and the default in Postgres, is the B-Tree. A B-Tree is the generalized form of a binary search tree, where nodes can have more than two children.\n\nEven though indexes improve query performance, the Postgres query planner may not always make use of a given index when choosing which optimizations to make. Additionally indexes come with some overhead - additional writes and increased storage - so it's useful to understand how and when to use indexes, if at all."
        },
        {
          "type": "section",
          "title": "Create an index",
          "content": "Let's take an example table:\n\n```sql\ncreate table persons (\n id bigint generated by default as identity primary key,\n age int,\n height int,\n weight int,\n name text,\n deceased boolean\n);\n```\n\nAll the queries in this guide can be run using the [SQL Editor](https://supabase.com/dashboard/project/_/sql) in the Supabase Dashboard, or via `psql` if you're [connecting directly to the database](/docs/guides/database/connecting-to-postgres#direct-connections).\n\nWe might want to frequently query users based on their age:\n\n```sql\nselect name from persons where age = 32;\n```\n\nWithout an index, Postgres will scan every row in the table to find equality matches on age.\n\nYou can verify this by doing an explain on the query:\n\n```sql\nexplain select name from persons where age = 32;\n```\n\nOutputs:\n\n```\nSeq Scan on persons (cost=0.00..22.75 rows=x width=y)\nFilter: (age = 32)\n```\n\nTo add a simple B-Tree index you can run:\n\n```sql\ncreate index idx_persons_age on persons (age);\n```\n\nIt can take a long time to build indexes on large datasets and the default behaviour of `create index` is to lock the table from writes.\n\nLuckily Postgres provides us with `create index concurrently` which prevents blocking writes on the table, but does take a bit longer to build.\n\nHere is a simplified diagram of the index we just created (note that in practice, nodes actually have more than two children).\n\nYou can see that in any large data set, traversing the index to locate a given value can be done in much less operations (O(log n)) than compared to scanning the table one value at a time from top to bottom (O(n)).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Partial indexes",
          "content": "If you are frequently querying a subset of rows then it may be more efficient to build a partial index. In our example, perhaps we only want to match on `age` where `deceased is false`. We could build a partial index:\n\n```sql\ncreate index idx_living_persons_age on persons (age)\nwhere deceased is false;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Ordering indexes",
          "content": "By default B-Tree indexes are sorted in ascending order, but sometimes you may want to provide a different ordering. Perhaps our application has a page featuring the top 10 oldest people. Here we would want to sort in descending order, and include `NULL` values last. For this we can use:\n\n```sql\ncreate index idx_persons_age_desc on persons (age desc nulls last);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Reindexing",
          "content": "After a while indexes can become stale and may need rebuilding. Postgres provides a `reindex` command for this, but due to Postgres locks being placed on the index during this process, you may want to make use of the `concurrent` keyword.\n\n```sql\nreindex index concurrently idx_persons_age;\n```\n\nAlternatively you can reindex all indexes on a particular table:\n\n```sql\nreindex table concurrently persons;\n```\n\nTake note that `reindex` can be used inside a transaction, but `reindex [index/table] concurrently` cannot.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Index Advisor",
          "content": "Indexes can improve query performance of your tables as they grow. The Supabase Dashboard offers an Index Advisor, which suggests potential indexes to add to your tables.\n\nFor more information on the Index Advisor and its suggestions, see the [`index_advisor` extension](/docs/guides/database/extensions/index_advisor).\n\nTo use the Dashboard Index Advisor:\n\n1. Go to the [Query Performance](/dashboard/project/_/advisors/query-performance) page.\n1. Click on a query to bring up the Details side panel.\n1. Select the Indexes tab.\n1. Enable Index Advisor if prompted.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Understanding Index Advisor results",
          "content": "The Indexes tab shows the existing indexes used in the selected query. Note that indexes suggested in the \"New Index Recommendations\" section may not be used when you create them. Postgres' query planner may intentionally ignore an available index if it determines that the query will be faster without. For example, on a small table, a sequential scan might be faster than an index scan. In that case, the planner will switch to using the index as the table size grows, helping to future proof the query.\n\nIf additional indexes might improve your query, the Index Advisor shows the suggested indexes with the estimated improvement in startup and total costs:\n\n- Startup cost is the cost to fetch the first row\n- Total cost is the cost to fetch all the rows\n\nCosts are in arbitrary units, where a single sequential page read costs 1.0 units.",
          "level": 3
        }
      ],
      "wordCount": 854,
      "characterCount": 5227
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-roles-superuser",
      "identifier": "database-postgres-roles-superuser",
      "name": "Roles, superuser access and unsupported operations",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/roles-superuser",
      "dateModified": "2025-06-13T12:45:11.290670",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/roles-superuser.mdx",
      "frontmatter": {
        "id": "roles",
        "title": "Roles, superuser access and unsupported operations",
        "slug": "roles"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase provides the default `postgres` role to all instances deployed. Superuser access is not given as it allows destructive operations to be performed on the database.\n\nTo ensure you are not impacted by this, additional privileges are granted to the `postgres` user to allow it to run some operations that are normally restricted to superusers.\n\nHowever, this does mean that some operations, that typically require `superuser` privileges, are not available on Supabase. These are documented below:"
        },
        {
          "type": "section",
          "title": "Unsupported operations",
          "content": "- `CREATE SUBSCRIPTION`\n- `CREATE EVENT TRIGGER`\n- `COPY ... FROM PROGRAM`\n- `ALTER USER ... WITH SUPERUSER`",
          "level": 2
        }
      ],
      "wordCount": 97,
      "characterCount": 638
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-roles",
      "identifier": "database-postgres-roles",
      "name": "Postgres Roles",
      "description": "Managing access to your Postgres database and configuring permissions.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/roles",
      "dateModified": "2025-06-13T12:45:11.290856",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/roles.mdx",
      "frontmatter": {
        "id": "postgres-roles",
        "title": "Postgres Roles",
        "description": "Managing access to your Postgres database and configuring permissions.",
        "subtitle": "Managing access to your Postgres database and configuring permissions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Postgres manages database access permissions using the concept of roles. Generally you wouldn't use these roles for your own application - they are mostly for configuring _system access_ to your database. If you want to configure _application access_, then you should use [Row Level Security](/docs/guides/database/postgres/row-level-security) (RLS). You can also implement [Role-based Access Control](/docs/guides/database/postgres/custom-claims-and-role-based-access-control-rbac) on top of RLS."
        },
        {
          "type": "section",
          "title": "Users vs roles",
          "content": "In Postgres, roles can function as users or groups of users. Users are roles with login privileges, while groups (also known as role groups) are roles that don't have login privileges but can be used to manage permissions for multiple users.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating roles",
          "content": "You can create a role using the `create role` command:\n\n```sql\ncreate role \"role_name\";\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating users",
          "content": "Roles and users are essentially the same in Postgres, however if you want to use password-logins for a specific role, then you can use `WITH LOGIN PASSWORD`:\n\n```sql\ncreate role \"role_name\" with login password 'extremely_secure_password';\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Passwords",
          "content": "Your Postgres database is the core of your Supabase project, so it's important that every role has a strong, secure password at all times. Here are some tips for creating a secure password:\n\n- Use a password manager to generate it.\n- Make a long password (12 characters at least).\n- Don't use any common dictionary words.\n- Use both upper and lower case characters, numbers, and special symbols.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Special symbols in passwords",
          "content": "If you use special symbols in your Postgres password, you must remember to [percent-encode](https://en.wikipedia.org/wiki/Percent-encoding) your password later if using the Postgres connection string, for example, `postgresql://postgres.projectref:p%3Dword@aws-0-us-east-1.pooler.supabase.com:6543/postgres`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Changing your project password",
          "content": "When you created your project you were also asked to enter a password. This is the password for the `postgres` role in your database. You can update this from the Dashboard under the [database settings](https://supabase.com/dashboard/project/_/settings/database) page. You should _never_ give this to third-party service unless you absolutely trust them. Instead, we recommend that you create a new user for every service that you want to give access too. This will also help you with debugging - you can see every query that each role is executing in your database within `pg_stat_statements`.\n\nChanging the password does not result in any downtime. All connected services, such as PostgREST, PgBouncer, and other Supabase managed services, are automatically updated to use the latest password to ensure availability. However, if you have any external services connecting to the Supabase database using hardcoded username/password credentials, a manual update will be required.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Granting permissions",
          "content": "Roles can be granted various permissions on database objects using the `GRANT` command. Permissions include `SELECT`, `INSERT`, `UPDATE`, and `DELETE`. You can configure access to almost any object inside your database - including tables, views, functions, and triggers.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Revoking permissions",
          "content": "Permissions can be revoked using the `REVOKE` command:\n\n```sql\nREVOKE permission_type ON object_name FROM role_name;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Role hierarchy",
          "content": "Roles can be organized in a hierarchy, where one role can inherit permissions from another. This simplifies permission management, as you can define permissions at a higher level and have them automatically apply to all child roles.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Role inheritance",
          "content": "To create a role hierarchy, you first need to create the parent and child roles. The child role will inherit permissions from its parent. Child roles can be added using the INHERIT option when creating the role:\n\n```sql\ncreate role \"child_role_name\" inherit \"parent_role_name\";\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Preventing inheritance",
          "content": "In some cases, you might want to prevent a role from having a child relationship (typically superuser roles). You can prevent inheritance relations using `NOINHERIT`:\n\n```sql\nalter role \"child_role_name\" noinherit;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Supabase roles",
          "content": "Postgres comes with a set of [predefined roles](https://www.postgresql.org/docs/current/predefined-roles.html). Supabase extends this with a default set of roles which are configured on your database when you start a new project:",
          "level": 2
        },
        {
          "type": "section",
          "title": "`postgres`",
          "content": "The default Postgres role. This has admin privileges.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`anon`",
          "content": "For unauthenticated, public access. This is the role which the API (PostgREST) will use when a user _is not_ logged in.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`authenticator`",
          "content": "A special role for the API (PostgREST). It has very limited access, and is used to validate a JWT and then\n\"change into\" another role determined by the JWT verification.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`authenticated`",
          "content": "For \"authenticated access.\" This is the role which the API (PostgREST) will use when a user _is_ logged in.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`service_role`",
          "content": "For elevated access. This role is used by the API (PostgREST) to bypass Row Level Security.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`supabase_auth_admin`",
          "content": "Used by the Auth middleware to connect to the database and run migration. Access is scoped to the `auth` schema.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`supabase_storage_admin`",
          "content": "Used by the Auth middleware to connect to the database and run migration. Access is scoped to the `storage` schema.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`dashboard_user`",
          "content": "For running commands via the Supabase UI.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`supabase_admin`",
          "content": "An internal role Supabase uses for administrative tasks, such as running upgrades and automations.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official Postgres docs: [Database Roles](https://www.postgresql.org/docs/current/database-roles.html)\n- Official Postgres docs: [Role Membership](https://www.postgresql.org/docs/current/role-membership.html)\n- Official Postgres docs: [Function Permissions](https://www.postgresql.org/docs/current/perm-functions.html)",
          "level": 2
        }
      ],
      "wordCount": 817,
      "characterCount": 5853
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-row-level-security",
      "identifier": "database-postgres-row-level-security",
      "name": "Row Level Security",
      "description": "Secure your data using Postgres Row Level Security.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/row-level-security",
      "dateModified": "2025-06-13T12:45:11.291543",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/row-level-security.mdx",
      "frontmatter": {
        "id": "row-level-security",
        "title": "Row Level Security",
        "description": "Secure your data using Postgres Row Level Security.",
        "subtitle": "Secure your data using Postgres Row Level Security."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "When you need granular authorization rules, nothing beats Postgres's [Row Level Security (RLS)](https://www.postgresql.org/docs/current/ddl-rowsecurity.html)."
        },
        {
          "type": "section",
          "title": "Row Level Security in Supabase",
          "content": "Supabase allows convenient and secure data access from the browser, as long as you enable RLS.\n\nRLS _must_ always be enabled on any tables stored in an exposed schema. By default, this is the `public` schema.\n\nRLS is enabled by default on tables created with the Table Editor in the dashboard. If you create one in raw SQL or with the SQL editor, remember to enable RLS yourself:\n\n```sql\nalter table .\nenable row level security;\n```\n\nRLS is incredibly powerful and flexible, allowing you to write complex SQL rules that fit your unique business needs. RLS can be combined with [Supabase Auth](/docs/guides/auth) for end-to-end user security from the browser to the database.\n\nRLS is a Postgres primitive and can provide \"[defense in depth]()\" to protect your data from malicious actors even when accessed through third-party tooling.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Policies",
          "content": "[Policies](https://www.postgresql.org/docs/current/sql-createpolicy.html) are Postgres's rule engine. Policies are easy to understand once you get the hang of them. Each policy is attached to a table, and the policy is executed every time a table is accessed.\n\nYou can just think of them as adding a `WHERE` clause to every query. For example a policy like this ...\n\n```sql\ncreate policy \"Individuals can view their own todos.\"\non todos for select\nusing ( (select auth.uid()) = user_id );\n```\n\n.. would translate to this whenever a user tries to select from the todos table:\n\n```sql\nselect *\nfrom todos\nwhere auth.uid() = todos.user_id;\n-- Policy is implicitly added.\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enabling Row Level Security",
          "content": "You can enable RLS for any table using the `enable row level security` clause:\n\n```sql\nalter table \"table_name\" enable row level security;\n```\n\nOnce you have enabled RLS, no data will be accessible via the [API](/docs/guides/api) when using the public `anon` key, until you create policies.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Authenticated and unauthenticated roles",
          "content": "Supabase maps every request to one of the roles:\n\n- `anon`: an unauthenticated request (the user is not logged in)\n- `authenticated`: an authenticated request (the user is logged in)\n\nThese are actually [Postgres Roles](/docs/guides/database/postgres/roles). You can use these roles within your Policies using the `TO` clause:\n\n```sql\ncreate policy \"Profiles are viewable by everyone\"\non profiles for select\nto authenticated, anon\nusing ( true );\n\n-- OR\n\ncreate policy \"Public profiles are viewable only by authenticated users\"\non profiles for select\nto authenticated\nusing ( true );\n```\n\nUsing the `anon` Postgres role is different from an [anonymous user](/docs/guides/auth/auth-anonymous) in Supabase Auth. An anonymous user assumes the `authenticated` role to access the database and can be differentiated from a permanent user by checking the `is_anonymous` claim in the JWT.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating policies",
          "content": "Policies are SQL logic that you attach to a Postgres table. You can attach as many policies as you want to each table.\n\nSupabase provides some [helpers](#helper-functions) that simplify RLS if you're using Supabase Auth. We'll use these helpers to illustrate some basic policies:",
          "level": 2
        },
        {
          "type": "section",
          "title": "SELECT policies",
          "content": "You can specify select policies with the `using` clause.\n\nLet's say you have a table called `profiles` in the public schema and you want to enable read access to everyone.\n\n```sql\n-- 1. Create table\ncreate table profiles (\n id uuid primary key,\n user_id references auth.users,\n avatar_url text\n);\n\n-- 2. Enable RLS\nalter table profiles enable row level security;\n\n-- 3. Create Policy\ncreate policy \"Public profiles are visible to everyone.\"\non profiles for select\nto anon -- the Postgres Role (recommended)\nusing ( true ); -- the actual Policy\n```\n\nAlternatively, if you only wanted users to be able to see their own profiles:\n\n```sql\ncreate policy \"User can see their own profile only.\"\non profiles\nfor select using ( (select auth.uid()) = user_id );\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "INSERT policies",
          "content": "You can specify insert policies with the `with check` clause. The `with check` expression ensures that any new row data adheres to the policy constraints.\n\nLet's say you have a table called `profiles` in the public schema and you only want users to be able to create a profile for themselves. In that case, we want to check their User ID matches the value that they are trying to insert:\n\n```sql\n-- 1. Create table\ncreate table profiles (\n id uuid primary key,\n user_id uuid references auth.users,\n avatar_url text\n);\n\n-- 2. Enable RLS\nalter table profiles enable row level security;\n\n-- 3. Create Policy\ncreate policy \"Users can create a profile.\"\non profiles for insert\nto authenticated -- the Postgres Role (recommended)\nwith check ( (select auth.uid()) = user_id ); -- the actual Policy\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "UPDATE policies",
          "content": "You can specify update policies by combining both the `using` and `with check` expressions.\n\nThe `using` clause represents the condition that must be true for the update to be allowed, and `with check` clause ensures that the updates made adhere to the policy constraints.\n\nLet's say you have a table called `profiles` in the public schema and you only want users to be able to update their own profile.\n\nYou can create a policy where the `using` clause checks if the user owns the profile being updated. And the `with check` clause ensures that, in the resultant row, users do not change the `user_id` to a value that is not equal to their User ID, maintaining that the modified profile still meets the ownership condition.\n\n```sql\n-- 1. Create table\ncreate table profiles (\n id uuid primary key,\n user_id uuid references auth.users,\n avatar_url text\n);\n\n-- 2. Enable RLS\nalter table profiles enable row level security;\n\n-- 3. Create Policy\ncreate policy \"Users can update their own profile.\"\non profiles for update\nto authenticated -- the Postgres Role (recommended)\nusing ( (select auth.uid()) = user_id ) -- checks if the existing row complies with the policy expression\nwith check ( (select auth.uid()) = user_id ); -- checks if the new row complies with the policy expression\n```\n\nIf no `with check` expression is defined, then the `using` expression will be used both to determine which rows are visible (normal USING case) and which new rows will be allowed to be added (WITH CHECK case).\n\nTo perform an `UPDATE` operation, a corresponding [`SELECT` policy](#select-policies) is required. Without a `SELECT` policy, the `UPDATE` operation will not work as expected.",
          "level": 3
        },
        {
          "type": "section",
          "title": "DELETE policies",
          "content": "You can specify delete policies with the `using` clause.\n\nLet's say you have a table called `profiles` in the public schema and you only want users to be able to delete their own profile:\n\n```sql\n-- 1. Create table\ncreate table profiles (\n id uuid primary key,\n user_id uuid references auth.users,\n avatar_url text\n);\n\n-- 2. Enable RLS\nalter table profiles enable row level security;\n\n-- 3. Create Policy\ncreate policy \"Users can delete a profile.\"\non profiles for delete\nto authenticated -- the Postgres Role (recommended)\nusing ( (select auth.uid()) = user_id ); -- the actual Policy\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Views",
          "content": "Views bypass RLS by default because they are usually created with the `postgres` user. This is a feature of Postgres, which automatically creates views with `security definer`.\n\nIn Postgres 15 and above, you can make a view obey the RLS policies of the underlying tables when invoked by `anon` and `authenticated` roles by setting `security_invoker = true`.\n\n```sql\ncreate view \nwith(security_invoker = true)\nas select \n```\n\nIn older versions of Postgres, protect your views by revoking access from the `anon` and `authenticated` roles, or by putting them in an unexposed schema.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Helper functions",
          "content": "Supabase provides some helper functions that make it easier to write Policies.",
          "level": 2
        },
        {
          "type": "section",
          "title": "`auth.uid()`",
          "content": "Returns the ID of the user making the request.",
          "level": 3
        },
        {
          "type": "section",
          "title": "`auth.jwt()`",
          "content": "Not all information present in the JWT should be used in RLS policies. For instance, creating an RLS policy that relies on the `user_metadata` claim can create security issues in your application as this information can be modified by authenticated end users.\n\nReturns the JWT of the user making the request. Anything that you store in the user's `raw_app_meta_data` column or the `raw_user_meta_data` column will be accessible using this function. It's important to know the distinction between these two:\n\n- `raw_user_meta_data` - can be updated by the authenticated user using the `supabase.auth.update()` function. It is not a good place to store authorization data.\n- `raw_app_meta_data` - cannot be updated by the user, so it's a good place to store authorization data.\n\nThe `auth.jwt()` function is extremely versatile. For example, if you store some team data inside `app_metadata`, you can use it to determine whether a particular user belongs to a team. For example, if this was an array of IDs:\n\n```sql\ncreate policy \"User is in team\"\non my_table\nto authenticated\nusing ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));\n```\n\nKeep in mind that a JWT is not always \"fresh\". In the example above, even if you remove a user from a team and update the `app_metadata` field, that will not be reflected using `auth.jwt()` until the user's JWT is refreshed.\n\nAlso, if you are using Cookies for Auth, then you must be mindful of the JWT size. Some browsers are limited to 4096 bytes for each cookie, and so the total size of your JWT should be small enough to fit inside this limitation.",
          "level": 3
        },
        {
          "type": "section",
          "title": "MFA",
          "content": "The `auth.jwt()` function can be used to check for [Multi-Factor Authentication](/docs/guides/auth/auth-mfa#enforce-rules-for-mfa-logins). For example, you could restrict a user from updating their profile unless they have at least 2 levels of authentication (Assurance Level 2):\n\n```sql\ncreate policy \"Restrict updates.\"\non profiles\nas restrictive\nfor update\nto authenticated using (\n (select auth.jwt()->>'aal') = 'aal2'\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Bypassing Row Level Security",
          "content": "Supabase provides special \"Service\" keys, which can be used to bypass RLS. These should never be used in the browser or exposed to customers, but they are useful for administrative tasks.\n\nSupabase will adhere to the RLS policy of the signed-in user, even if the client library is initialized with a Service Key.\n\nYou can also create new [Postgres Roles](/docs/guides/database/postgres/roles) which can bypass Row Level Security using the \"bypass RLS\" privilege:\n\n```sql\nalter role \"role_name\" with bypassrls;\n```\n\nThis can be useful for system-level access. You should _never_ share login credentials for any Postgres Role with this privilege.",
          "level": 2
        },
        {
          "type": "section",
          "title": "RLS performance recommendations",
          "content": "Every authorization system has an impact on performance. While row level security is powerful, the performance impact is important to keep in mind. This is especially true for queries that scan every row in a table - like many `select` operations, including those using limit, offset, and ordering.\n\nBased on a series of [tests](https://github.com/GaryAustin1/RLS-Performance), we have a few recommendations for RLS:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Add indexes",
          "content": "Make sure you've added [indexes](/docs/guides/database/postgres/indexes) on any columns used within the Policies which are not already indexed (or primary keys). For a Policy like this:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing ( (select auth.uid()) = user_id );\n```\n\nYou can add an index like:\n\n```sql\ncreate index userid\non test_table\nusing btree (user_id);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Benchmarks",
          "content": "| Test | Before (ms) | After (ms) | % Improvement | Change |\n| --------------------------------------------------------------------------------------------- | ----------- | ---------- | ------------- | -------------------------------------------------------------------------------------------------------- |\n| [test1-indexed](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test1-indexed) | 171 | Before:No indexAfter:`user_id` indexed |",
          "level": 4
        },
        {
          "type": "section",
          "title": "Call functions with `select`",
          "content": "You can use `select` statement to improve policies that use functions. For example, instead of this:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing ( auth.uid() = user_id );\n```\n\nYou can do:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing ( (select auth.uid()) = user_id );\n```\n\nThis method works well for JWT functions like `auth.uid()` and `auth.jwt()` as well as `security definer` Functions. Wrapping the function causes an `initPlan` to be run by the Postgres optimizer, which allows it to \"cache\" the results per-statement, rather than calling the function on each row.\n\nYou can only use this technique if the results of the query or function do not change based on the row data.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Benchmarks",
          "content": "| Test | Before (ms) | After (ms) | % Improvement | Change |\n| --------------------------------------------------------------------------------------------------------------------------------- | ----------- | ---------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [test2a-wrappedSQL-uid]() | 179 | 9 | 94.97% | Before:`auth.uid() = user_id` After: `(select auth.uid()) = user_id` |\n| [test2b-wrappedSQL-isadmin]() | 11,000 | 7 | 99.94% | Before:`is_admin()` _table join_After:`(select is_admin())` _table join_ |\n| [test2c-wrappedSQL-two-functions](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test2c-wrappedSQL-two-functions) | 11,000 | 10 | 99.91% | Before:`is_admin() OR auth.uid() = user_id`After:`(select is_admin()) OR (select auth.uid() = user_id)` |\n| [test2d-wrappedSQL-sd-fun](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test2d-wrappedSQL-sd-fun) | 178,000 | 12 | 99.993% | Before:`has_role() = role` After:(select has_role()) = role |\n| [test2e-wrappedSQL-sd-fun-array](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test2e-wrappedSQL-sd-fun-array) | 173000 | 16 | 99.991% | Before:`team_id=any(user_teams())` After:team_id=any(array(select user_teams())) |",
          "level": 4
        },
        {
          "type": "section",
          "title": "Add filters to every query",
          "content": "Policies are \"implicit where clauses,\" so it's common to run `select` statements without any filters. This is a bad pattern for performance. Instead of doing this (JS client example):\n\n{/* prettier-ignore */}\n```js\nconst { data } = supabase\n .from('table')\n .select()\n```\n\nYou should always add a filter:\n\n{/* prettier-ignore */}\n```js\nconst { data } = supabase\n .from('table')\n .select()\n .eq('user_id', userId)\n```\n\nEven though this duplicates the contents of the Policy, Postgres can use the filter to construct a better query plan.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Benchmarks",
          "content": "| Test | Before (ms) | After (ms) | % Improvement | Change |\n| ------------------------------------------------------------------------------------------------- | ----------- | ---------- | ------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n| [test3-addfilter](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test3-addfilter) | 171 | 9 | 94.74% | Before:`auth.uid() = user_id`After:add `.eq` or `where` on `user_id` |",
          "level": 4
        },
        {
          "type": "section",
          "title": "Use security definer functions",
          "content": "A \"security definer\" function runs using the same role that _created_ the function. This means that if you create a role with a superuser (like `postgres`), then that function will have `bypassrls` privileges. For example, if you had a policy like this:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing (\n exists (\n select 1 from roles_table\n where (select auth.uid()) = user_id and role = 'good_role'\n )\n);\n```\n\nWe can instead create a `security definer` function which can scan `roles_table` without any RLS penalties:\n\n```sql\ncreate function private.has_good_role()\nreturns boolean\nlanguage plpgsql\nsecurity definer -- will run as the creator\nas $$\nbegin\n return exists (\n select 1 from roles_table\n where (select auth.uid()) = user_id and role = 'good_role'\n );\nend;\n$$;\n\n-- Update our policy to use this function:\ncreate policy \"rls_test_select\"\non test_table\nto authenticated\nusing ( private.has_good_role() );\n```\n\nSecurity-definer functions should never be created in a schema in the \"Exposed schemas\" inside your [API settings](https://supabase.com/dashboard/project/_/settings/api)`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Minimize joins",
          "content": "You can often rewrite your Policies to avoid joins between the source and the target table. Instead, try to organize your policy to fetch all the relevant data from the target table into an array or set, then you can use an `IN` or `ANY` operation in your filter.\n\nFor example, this is an example of a slow policy which joins the source `test_table` to the target `team_user`:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing (\n (select auth.uid()) in (\n select user_id\n from team_user\n where team_user.team_id = team_id -- joins to the source \"test_table.team_id\"\n )\n);\n```\n\nWe can rewrite this to avoid this join, and instead select the filter criteria into a set:\n\n```sql\ncreate policy \"rls_test_select\" on test_table\nto authenticated\nusing (\n team_id in (\n select team_id\n from team_user\n where user_id = (select auth.uid()) -- no join\n )\n);\n```\n\nIn this case you can also consider [using a `security definer` function](#use-security-definer-functions) to bypass RLS on the join table:\n\nIf the list exceeds 1000 items, a different approach may be needed or you may need to analyze the approach to ensure that the performance is acceptable.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Benchmarks",
          "content": "| Test | Before (ms) | After (ms) | % Improvement | Change |\n| --------------------------------------------------------------------------------------------------- | ----------- | ---------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [test5-fixed-join](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test5-fixed-join) | 9,000 | 20 | 99.78% | Before:`auth.uid()` in table join on colAfter:col in table join on `auth.uid()` |",
          "level": 4
        },
        {
          "type": "section",
          "title": "Specify roles in your policies",
          "content": "Always use the Role of inside your policies, specified by the `TO` operator. For example, instead of this query:\n\n```sql\ncreate policy \"rls_test_select\" on rls_test\nusing ( auth.uid() = user_id );\n```\n\nUse:\n\n```sql\ncreate policy \"rls_test_select\" on rls_test\nto authenticated\nusing ( (select auth.uid()) = user_id );\n```\n\nThis prevents the policy `( (select auth.uid()) = user_id )` from running for any `anon` users, since the execution stops at the `to authenticated` step.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Benchmarks",
          "content": "| Test | Before (ms) | After (ms) | % Improvement | Change |\n| --------------------------------------------------------------------------------------------- | ----------- | ---------- | ------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n| [test6-To-role](https://github.com/GaryAustin1/RLS-Performance/tree/main/tests/test6-To-role) | 170 | Before:No `TO` policyAfter:`TO authenticated` (anon accessing) |",
          "level": 4
        },
        {
          "type": "section",
          "title": "More resources",
          "content": "- [Testing your database](/docs/guides/database/testing)\n- [Row Level Security and Supabase Auth](/docs/guides/database/postgres/row-level-security)\n- [RLS Guide and Best Practices](https://github.com/orgs/supabase/discussions/14576)\n- Community repo on testing RLS using [pgTAP and dbdev](https://github.com/usebasejump/supabase-test-helpers/tree/main)",
          "level": 2
        }
      ],
      "wordCount": 2784,
      "characterCount": 19645
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-setup-replication-external",
      "identifier": "database-postgres-setup-replication-external",
      "name": "Replicate to another Postgres database using Logical Replication",
      "description": "Example to setup logical replication using publish-subscribe to a Postgres database outside of Supabase",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/setup-replication-external",
      "dateModified": "2025-06-13T12:45:11.291699",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/setup-replication-external.mdx",
      "frontmatter": {
        "title": "Replicate to another Postgres database using Logical Replication",
        "description": "Example to setup logical replication using publish-subscribe to a Postgres database outside of Supabase",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "For this example, you will need:\n\n- A Supabase project\n- A Postgres database (running v10 or newer)\n\nYou will be running commands on both of these databases to publish changes from the Supabase database to the external database.\n\n1. Create a `publication` on the **Supabase database**:\n\n```sql\nCREATE PUBLICATION example_pub;\n```\n\n2. Also on the **Supabase database**, create a `replication slot`:\n\n```sql\nselect pg_create_logical_replication_slot('example_slot', 'pgoutput');\n```\n\n3. Now we will connect to our **external database** and subscribe to our `publication` Note: ):\n\nThis will need a **direct** connection (not a Connection Pooler) to your database and you can find the connection info in the [Dashboard](https://supabase.com/dashboard/project/_/settings/database).\n\nYou will also need to ensure that IPv6 is supported by your replication destination (or you can enable the [IPv4 add-on](/guides/platform/ipv4-address))\n\nIf you would prefer not to use the `postgres` user, then you can run `CREATE ROLE WITH REPLICATION;` using the `postgres` user.\n\n```sql\nCREATE SUBSCRIPTION example_sub\nCONNECTION 'host=db.oaguxblfdassqxvvwtfe.supabase.co user=postgres password=YOUR_PASS dbname=postgres'\nPUBLICATION example_pub\nWITH (copy_data = true, create_slot=false, slot_name=example_slot);\n```\n\n`create_slot` is set to `false` because `slot_name` is provided and the slot was already created in Step 2.\nTo copy data from before the slot was created, set `copy_data` to `true`.\n\n4. Add all the tables that you want replicated to the publication.\n\n```sql\nALTER PUBLICATION example_pub ADD TABLE example_table;\n```\n\n5. Check the replication status using `pg_stat_replication`\n\n```sql\nselect * from pg_stat_replication;\n```"
        }
      ],
      "wordCount": 231,
      "characterCount": 1725
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-timeouts",
      "identifier": "database-postgres-timeouts",
      "name": "Timeouts",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/timeouts",
      "dateModified": "2025-06-13T12:45:11.291950",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/timeouts.mdx",
      "frontmatter": {
        "title": "Timeouts",
        "subtitle": "Extend database timeouts to execute longer transactions"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Dashboard and [Client](/docs/guides/api/rest/client-libs) queries have a max-configurable timeout of 60 seconds. For longer transactions, use [Supavisor or direct connections](/docs/guides/database/connecting-to-postgres#quick-summary)."
        },
        {
          "type": "section",
          "title": "Change Postgres timeout",
          "content": "You can change the Postgres timeout at the:\n\n1. [Session level](#session-level)\n1. [Function level](#function-level)\n1. [Global level](#global-level)\n1. [Role level](#role-level)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Session level",
          "content": "Session level settings persist only for the duration of the connection.\n\nSet the session timeout by running:\n\n```sql\nset statement_timeout = '10min';\n```\n\nBecause it applies to sessions only, it can only be used with connections through Supavisor in session mode (port 5432) or a direct connection. It cannot be used in the Dashboard, with the Supabase Client API, nor with Supavisor in Transaction mode (port 6543).\n\nThis is most often used for single, long running, administrative tasks, such as creating an HSNW index. Once the setting is implemented, you can view it by executing:\n\n```sql\nSHOW statement_timeout;\n```\n\nSee the full guide on [changing session timeouts](https://github.com/orgs/supabase/discussions/21133).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Function level",
          "content": "This works with the Database REST API when called from the Supabase client libraries:\n\n```sql\ncreate or replace function myfunc()\nreturns void as $$\n select pg_sleep(3); -- simulating some long-running process\n$$\nlanguage sql\nset statement_timeout TO '4s'; -- set custom timeout\n```\n\nThis is mostly for recurring functions that need a special exemption for runtimes.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Role level",
          "content": "This sets the timeout for a specific role.\n\nThe default role timeouts are:\n\n- `anon`: 3s\n- `authenticated`: 8s\n- `service_role`: none (defaults to the `authenticator` role's 8s timeout if unset)\n- `postgres`: none (capped by default global timeout to be 2min)\n\nRun the following query to change a role's timeout:\n\n```sql\nalter role example_role set statement_timeout = '10min'; -- could also use seconds '10s'\n```\n\n If you are changing the timeout for the Supabase Client API calls, you will need to reload PostgREST to reflect the timeout changes by running the following script:\n\n```sql\nNOTIFY pgrst, 'reload config';\n```\n\nUnlike global settings, the result cannot be checked with `SHOW\nstatement_timeout`. Instead, run:\n\n```sql\nselect\n rolname,\n rolconfig\nfrom pg_roles\nwhere\n rolname in (\n 'anon',\n 'authenticated',\n 'postgres',\n 'service_role'\n -- ,\n );\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Global level",
          "content": "This changes the statement timeout for all roles and sessions without an explicit timeout already set.\n\n```sql\nalter database postgres set statement_timeout TO '4s';\n```\n\nCheck if your changes took effect:\n\n```sql\nshow statement_timeout;\n```\n\nAlthough not necessary, if you are uncertain if a timeout has been applied, you can run a quick test:\n\n```sql\ncreate or replace function myfunc()\nreturns void as $$\n select pg_sleep(601); -- simulating some long-running process\n$$\nlanguage sql;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Identifying timeouts",
          "content": "The Supabase Dashboard contains tools to help you identify timed-out and long-running queries.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using the Logs Explorer",
          "content": "Go to the [Logs Explorer](/dashboard/project/_/logs/explorer), and run the following query to identify timed-out events (`statement timeout`) and queries that successfully run for longer than 10 seconds (`duration`).\n\n```sql\nselect\n cast(postgres_logs.timestamp as datetime) as timestamp,\n event_message,\n parsed.error_severity,\n parsed.user_name,\n parsed.query,\n parsed.detail,\n parsed.hint,\n parsed.sql_state_code,\n parsed.backend_type\nfrom\n postgres_logs\n cross join unnest(metadata) as metadata\n cross join unnest(metadata.parsed) as parsed\nwhere\n regexp_contains(event_message, 'duration|statement timeout')\n -- (OPTIONAL) MODIFY OR REMOVE\n and parsed.user_name = 'authenticator' -- <--------CHANGE\norder by timestamp desc\nlimit 100;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using the Query Performance page",
          "content": "Go to the [Query Performance page](/dashboard/project/_/advisors/query-performance?preset=slowest_execution) and filter by relevant role and query speeds. This only identifies slow-running but successful queries. Unlike the Log Explorer, it does not show you timed-out queries.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Understanding roles in logs",
          "content": "Each API server uses a designated user for connecting to the database:\n\n| Role | API/Tool |\n| ---------------------------- | ------------------------------------------------------------------------- |\n| `supabase_admin` | Used by Realtime and for project configuration |\n| `authenticator` | PostgREST |\n| `supabase_auth_admin` | Auth |\n| `supabase_storage_admin` | Storage |\n| `supabase_replication_admin` | Synchronizes Read Replicas |\n| `postgres` | Supabase Dashboard and External Tools (e.g., Prisma, SQLAlchemy, PSQL...) |\n| Custom roles | External Tools (e.g., Prisma, SQLAlchemy, PSQL...) |\n\nFilter by the `parsed.user_name` field to only retrieve logs made by specific users:\n\n```sql\n-- find events based on role/server\n... query\nwhere\n -- find events from the relevant role\n parsed.user_name = ''\n```",
          "level": 3
        }
      ],
      "wordCount": 677,
      "characterCount": 5023
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-triggers",
      "identifier": "database-postgres-triggers",
      "name": "Postgres Triggers",
      "description": "Automatically execute SQL on table events.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/triggers",
      "dateModified": "2025-06-13T12:45:11.292175",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/triggers.mdx",
      "frontmatter": {
        "id": "postgres-triggers",
        "title": "Postgres Triggers",
        "description": "Automatically execute SQL on table events.",
        "subtitle": "Automatically execute SQL on table events."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "In Postgres, a trigger executes a set of actions automatically on table events such as INSERTs, UPDATEs, DELETEs, or TRUNCATE operations."
        },
        {
          "type": "section",
          "title": "Creating a trigger",
          "content": "Creating triggers involve 2 parts:\n\n1. A [Function](/docs/guides/database/functions) which will be executed (called the Trigger Function)\n2. The actual Trigger object, with parameters around when the trigger should be run.\n\nAn example of a trigger is:\n\n```sql\ncreate trigger \"trigger_name\"\nafter insert on \"table_name\"\nfor each row\nexecute function trigger_function();\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Trigger functions",
          "content": "A trigger function is a user-defined [Function](/docs/guides/database/functions) that Postgres executes when the trigger is fired.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example trigger function",
          "content": "Here is an example that updates `salary_log` whenever an employee's salary is updated:\n\n```sql\n-- Example: Update salary_log when salary is updated\ncreate function update_salary_log()\nreturns trigger\nlanguage plpgsql\nas $$\nbegin\n insert into salary_log(employee_id, old_salary, new_salary)\n values (new.id, old.salary, new.salary);\n return new;\nend;\n$$;\n\ncreate trigger salary_update_trigger\nafter update on employees\nfor each row\nexecute function update_salary_log();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Trigger variables",
          "content": "Trigger functions have access to several special variables that provide information about the context of the trigger event and the data being modified. In the example above you can see the values inserted into the salary log are `old.salary` and `new.salary` - in this case `old` specifies the previous values and `new` specifies the updated values.\n\nHere are some of the key variables and options available within trigger functions:\n\n- `TG_NAME`: The name of the trigger being fired.\n- `TG_WHEN`: The timing of the trigger event (`BEFORE` or `AFTER`).\n- `TG_OP`: The operation that triggered the event (`INSERT`, `UPDATE`, `DELETE`, or `TRUNCATE`).\n- `OLD`: A record variable holding the old row's data in `UPDATE` and `DELETE` triggers.\n- `NEW`: A record variable holding the new row's data in `UPDATE` and `INSERT` triggers.\n- `TG_LEVEL`: The trigger level (`ROW` or `STATEMENT`), indicating whether the trigger is row-level or statement-level.\n- `TG_RELID`: The object ID of the table on which the trigger is being fired.\n- `TG_TABLE_NAME`: The name of the table on which the trigger is being fired.\n- `TG_TABLE_SCHEMA`: The schema of the table on which the trigger is being fired.\n- `TG_ARGV`: An array of string arguments provided when creating the trigger.\n- `TG_NARGS`: The number of arguments in the `TG_ARGV` array.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Types of triggers",
          "content": "There are two types of trigger, `BEFORE` and `AFTER`:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Trigger before changes are made",
          "content": "Executes before the triggering event.\n\n```sql\ncreate trigger before_insert_trigger\nbefore insert on orders\nfor each row\nexecute function before_insert_function();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Trigger after changes are made",
          "content": "Executes after the triggering event.\n\n```sql\ncreate trigger after_delete_trigger\nafter delete on customers\nfor each row\nexecute function after_delete_function();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Execution frequency",
          "content": "There are two options available for executing triggers:\n\n- `for each row`: specifies that the trigger function should be executed once for each affected row.\n- `for each statement`: the trigger is executed once for the entire operation (for example, once on insert). This can be more efficient than `for each row` when dealing with multiple rows affected by a single SQL statement, as they allow you to perform calculations or updates on groups of rows at once.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Dropping a trigger",
          "content": "You can delete a trigger using the `drop trigger` command:\n\n```sql\ndrop trigger \"trigger_name\" on \"table_name\";\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official Postgres Docs: [Triggers](https://www.postgresql.org/docs/current/triggers.html)\n- Official Postgres Docs: [Overview of Trigger Behavior](https://www.postgresql.org/docs/current/trigger-definition.html)\n- Official Postgres Docs: [CREATE TRIGGER](https://www.postgresql.org/docs/current/sql-createtrigger.html)",
          "level": 2
        }
      ],
      "wordCount": 561,
      "characterCount": 3990
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-postgres-which-version-of-postgres",
      "identifier": "database-postgres-which-version-of-postgres",
      "name": "Print PostgreSQL version",
      "description": "Useful snippet for finding out which version of postgres you are running",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/postgres/which-version-of-postgres",
      "dateModified": "2025-06-13T12:45:11.292254",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/postgres/which-version-of-postgres.mdx",
      "frontmatter": {
        "title": "Print PostgreSQL version",
        "description": "Useful snippet for finding out which version of postgres you are running",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "It's important to know which version of Postgres you are running as each major version has different features and may cause breaking changes. You may also need to update your schema when [upgrading](https://www.postgresql.org/docs/current/pgupgrade.html) or downgrading to a major Postgres version.\n\nRun the following query using the [SQL Editor](https://supabase.com/dashboard/project/_/sql) in the Supabase Dashboard:\n\n```sql\nselect\n version();\n```\n\nWhich should return something like:\n\n```sql\nPostgreSQL 15.1 on aarch64-unknown-linux-gnu, compiled by gcc (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0, 64-bit\n```\n\nThis query can also be executed via `psql` or any other query editor if you prefer to [connect directly to the database](/docs/guides/database/connecting-to-postgres#direct-connections)."
        }
      ],
      "wordCount": 96,
      "characterCount": 795
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-prisma",
      "identifier": "database-prisma",
      "name": "Prisma",
      "description": "Prisma Quickstart",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/prisma",
      "dateModified": "2025-06-13T12:45:11.292567",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/prisma.mdx",
      "frontmatter": {
        "id": "prisma",
        "title": "Prisma",
        "description": "Prisma Quickstart",
        "breadcrumb": "ORM Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "This quickly shows how to connect your Prisma application to Supabase Postgres. If you encounter any problems, reference the [Prisma troubleshooting docs](/docs/guides/database/prisma/prisma-troubleshooting).\n\nIf you plan to solely use Prisma instead of the Supabase Data API (PostgREST), turn it off in the [API Settings](https://supabase.com/dashboard/project/_/settings/api).\n\n - In the [SQL Editor](https://supabase.com/dashboard/project/_/sql/new), create a Prisma DB user with full privileges on the public schema.\n - This gives you better control over Prisma's access and makes it easier to monitor using Supabase tools like the [Query Performance Dashboard](https://supabase.com/dashboard/project/_/advisors/query-performance) and [Log Explorer](https://supabase.com/dashboard/project/_/logs/explorer).\n\n For security, consider using a [password generator](https://bitwarden.com/password-generator/) for the Prisma role.\n\n ```sql\n -- Create custom user\n create user \"prisma\" with password 'custom_password' bypassrls createdb;\n\n -- extend prisma's privileges to postgres (necessary to view changes in Dashboard)\n grant \"prisma\" to \"postgres\";\n\n -- Grant it necessary permissions over the relevant schemas (public)\n grant usage on schema public to prisma;\n grant create on schema public to prisma;\n grant all on all tables in schema public to prisma;\n grant all on all routines in schema public to prisma;\n grant all on all sequences in schema public to prisma;\n alter default privileges for role postgres in schema public grant all on tables to prisma;\n alter default privileges for role postgres in schema public grant all on routines to prisma;\n alter default privileges for role postgres in schema public grant all on sequences to prisma;\n ```\n\n ```sql\n -- alter prisma password if needed\n alter user \"prisma\" with password 'new_password';\n ```\n\n Create a new Prisma Project on your computer\n\n Create a new directory\n ```bash Terminal\n mkdir hello-prisma\n cd hello-prisma\n ```\n\n Initiate a new Prisma project\n\n ```bash\n npm init -y\n npm install prisma typescript ts-node @types/node --save-dev\n\n npx tsc --init\n\n npx prisma init\n ```\n\n ```bash\n pnpm init -y\n pnpm install prisma typescript ts-node @types/node --save-dev\n\n pnpx tsc --init\n\n pnpx prisma init\n ```\n\n ```bash\n yarn init -y\n yarn add prisma typescript ts-node @types/node --save-dev\n\n npx tsc --init\n\n npx prisma init\n ```\n\n ```bash\n bun init -y\n bun install prisma typescript ts-node @types/node --save-dev\n\n bunx tsc --init\n\n bunx prisma init\n ```\n\n - On your project dashboard, click [Connect](https://supabase.com/dashboard/project/_?showConnect=true)\n - Find your Supavisor Session pooler string. It should end with 5432. It will be used in your `.env` file.\n \n If you're in an [IPv6 environment](https://github.com/orgs/supabase/discussions/27034) or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.\n\n - If you plan on deploying Prisma to a serverless or auto-scaling environment, you'll also need your Supavisor transaction mode string.\n - The string is identical to the session mode string but uses port 6543 at the end.\n\n In your .env file, set the DATABASE_URL variable to your connection string\n ```text .env\n # Used for Prisma Migrations and within your application\n DATABASE_URL=\"postgres://[DB-USER].[PROJECT-REF]:[PRISMA-PASSWORD]@[DB-REGION].pooler.supabase.com:5432/postgres\"\n ```\n\n Change your string's `[DB-USER]` to `prisma` and add the password you created in step 1\n ```md\n postgres://prisma.[PROJECT-REF]...\n ```\n\n Assign the connection string for Supavisor Transaction Mode (using port 6543) to the DATABASE_URL variable in your .env file. Make sure to append \"pgbouncer=true\" to the end of the string to work with Supavisor.\n\n Next, create a DIRECT_URL variable in your .env file and assign the connection string that ends with port 5432 to it.\n\n ```text .env # Used in your application (use transaction mode)\n DATABASE_URL=\"postgres://[DB-USER].[PROJECT-REF]:[PRISMA-PASSWORD]@aws-0-us-east-1.pooler.supabase.com:6543/postgres?pgbouncer=true\"\n\n # Used for Prisma Migrations (use session mode or direct connection)\n DIRECT_URL=\"postgres://[DB-USER].[PROJECT-REF]:[PRISMA-PASSWORD]@aws-0-us-east-1.pooler.supabase.com:5432/postgres\"\n ```\n\n Change both your strings' `[DB-USER]` to `prisma` and then add the password created in step 1\n ```md\n postgres://prisma.[PROJECT-REF]...\n ```\n\n In your schema.prisma file, edit your `datasource db` configs to reference your DIRECT_URL\n ```text schema.prisma\n datasource db {\n provider = \"postgresql\"\n url = env(\"DATABASE_URL\")\n directUrl = env(\"DIRECT_URL\")\n }\n ```\n\n If you have already modified your Supabase database, synchronize it with your migration file. Otherwise create new tables for your database\n\n Create new tables in your prisma.schema file\n\n ```ts prisma/schema.prisma\n model Post {\n id Int @id @default(autoincrement())\n title String\n content String?\n published Boolean @default(false)\n author User? @relation(fields: [authorId], references: [id])\n authorId Int?\n }\n\n model User {\n id Int @id @default(autoincrement())\n email String @unique\n name String?\n posts Post[]\n }\n ```\n commit your migration\n\n ```bash\n npx prisma migrate dev --name first_prisma_migration\n\n ```\n\n ```bash\n pnpx prisma migrate dev --name first_prisma_migration\n\n ```\n\n ```bash\n npx prisma migrate dev --name first_prisma_migration\n\n ```\n\n ```bash\n bunx prisma migrate dev --name first_prisma_migration\n\n ```\n\n Synchronize changes from your project:\n\n ```bash\n npx prisma db pull\n ```\n\n Create a migration file\n ```bash\n mkdir -p prisma/migrations/0_init_supabase\n ```\n\n Synchronize the migrations\n ```bash\n npx prisma migrate diff \\\n --from-empty \\\n --to-schema-datamodel prisma/schema.prisma \\\n --script > prisma/migrations/0_init_supabase/migration.sql\n ```\n \n If there are any conflicts, reference [Prisma's official doc](https://www.prisma.io/docs/orm/prisma-migrate/getting-started#work-around-features-not-supported-by-prisma-schema-language) or the [trouble shooting guide](/docs/guides/database/prisma/prisma-troubleshooting) for more details\n\n ```bash\n npx prisma migrate resolve --applied 0_init_supabase\n ```\n\n ```bash\n pnpx prisma db pull\n ```\n\n Create a migration file\n ```bash\n mkdir -p prisma/migrations/0_init_supabase\n ```\n\n Synchronize the migrations\n ```bash\n pnpx prisma migrate diff \\\n --from-empty \\\n --to-schema-datamodel prisma/schema.prisma \\\n --script > prisma/migrations/0_init_supabase/migration.sql\n ```\n \n If there are any conflicts, reference [Prisma's official doc](https://www.prisma.io/docs/orm/prisma-migrate/getting-started#work-around-features-not-supported-by-prisma-schema-language) or the [trouble shooting guide](/docs/guides/database/prisma/prisma-troubleshooting) for more details\n\n ```bash\n pnpx prisma migrate resolve --applied 0_init_supabase\n ```\n\n ```bash\n npx prisma db pull\n ```\n\n Create a migration file\n ```bash\n mkdir -p prisma/migrations/0_init_supabase\n ```\n\n Synchronize the migrations\n ```bash\n npx prisma migrate diff \\\n --from-empty \\\n --to-schema-datamodel prisma/schema.prisma \\\n --script > prisma/migrations/0_init_supabase/migration.sql\n ```\n \n If there are any conflicts, reference [Prisma's official doc](https://www.prisma.io/docs/orm/prisma-migrate/getting-started#work-around-features-not-supported-by-prisma-schema-language) or the [trouble shooting guide](/docs/guides/database/prisma/prisma-troubleshooting) for more details\n\n ```bash\n npx prisma migrate resolve --applied 0_init_supabase\n ```\n\n ```bash\n bunx prisma db pull\n ```\n\n Create a migration file\n ```bash\n mkdir -p prisma/migrations/0_init_supabase\n ```\n\n Synchronize the migrations\n ```bash\n bunx prisma migrate diff \\\n --from-empty \\\n --to-schema-datamodel prisma/schema.prisma \\\n --script > prisma/migrations/0_init_supabase/migration.sql\n ```\n \n If there are any conflicts, reference [Prisma's official doc](https://www.prisma.io/docs/orm/prisma-migrate/getting-started#work-around-features-not-supported-by-prisma-schema-language) or the [trouble shooting guide](/docs/guides/database/prisma-troubleshooting) for more details\n\n ```bash\n bunx prisma migrate resolve --applied 0_init_supabase\n ```\n\n Install the Prisma client and generate its model\n\n ```sh\n npm install @prisma/client\n npx prisma generate\n ```\n\n ```sh\n pnpm install @prisma/client\n pnpx prisma generate\n ```\n\n ```sh\n yarn add @prisma/client\n npx prisma generate\n ```\n\n ```sh\n bun install @prisma/client\n bunx prisma generate\n ```\n\n Create a index.ts file and run it to test your connection\n\n ```ts index.ts\n const { PrismaClient } = require('@prisma/client');\n\n const prisma = new PrismaClient();\n\n async function main() {\n //change to reference a table in your schema\n const val = await prisma..findMany({\n take: 10,\n });\n console.log(val);\n }\n\n main()\n .then(async () => {\n await prisma.$disconnect();\n })\n .catch(async (e) => {\n console.error(e);\n await prisma.$disconnect();\n process.exit(1);\n });\n\n ```"
        }
      ],
      "wordCount": 1040,
      "characterCount": 8988
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-prisma-prisma-troubleshooting",
      "identifier": "database-prisma-prisma-troubleshooting",
      "name": "Troubleshooting prisma errors",
      "description": "Prisma error troubleshooting",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/prisma/prisma-troubleshooting",
      "dateModified": "2025-06-13T12:45:11.292909",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/prisma/prisma-troubleshooting.mdx",
      "frontmatter": {
        "id": "prisma-troubleshooting",
        "title": "Troubleshooting prisma errors",
        "description": "Prisma error troubleshooting",
        "breadcrumb": "ORM Quickstarts"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "This guide addresses common Prisma errors that you might encounter while using Supabase.\n\nA full list of errors can be found in [Prisma's official docs](https://www.prisma.io/docs/orm/reference/error-reference)."
        },
        {
          "type": "section",
          "title": "Understanding connection string parameters: [#start]",
          "content": "Unlike other libraries, Prisma lets you configure [its settings](https://www.prisma.io/docs/orm/overview/databases/postgresql#arguments) through special options appended to your connection string.\n\nThese options, called \"query parameters,\" can be used to address specific errors.\n\n```md",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example of query parameters",
          "content": "connection_string.../postgres?KEY1=VALUE&KEY2=VALUE&KEY3=VALUE\n```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Errors",
          "content": "{/* supa-mdx-lint-disable-next-line Rule001HeadingCase */}",
          "level": 1
        },
        {
          "type": "section",
          "title": "... prepared statement already exists",
          "content": "Supavisor in transaction mode (port 6543) does not support [prepared statements](https://www.postgresql.org/docs/current/sql-prepare.html), which Prisma will try to create in the background.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Solution: [#solution-prepared-statement-exists]",
          "content": "- Add `pgbouncer=true` to the connection string. This turns off prepared statements in Prisma.\n\n```md\n.../postgres?pgbouncer=true\n```\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Can't reach database server at:",
          "content": "Prisma couldn't establish a connection with Postgres or Supavisor before the timeout",
          "level": 2
        },
        {
          "type": "section",
          "title": "Possible causes: [#possible-causes-cant-reach-database-server-at]",
          "content": "- **Database overload**: The database server is under heavy load, causing Prisma to struggle to connect.\n- **Malformed connection string**: The connection string used by Prisma is incorrect or incomplete.\n- **Transient network issues**: Temporary network problems are disrupting the connection.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Solutions: [#solution-cant-reach-database-server-at]",
          "content": "- **Check database health**: Use the [Reports Dashboard](https://supabase.com/dashboard/project/_/reports/database) to monitor CPU, memory, and I/O usage. If the database is overloaded, consider increasing your [compute size](https://supabase.com/docs/guides/platform/compute-add-ons) or [optimizing your queries](https://supabase.com/docs/guides/database/query-optimization).\n- **Verify connection string**: Double-check the connection string in your Prisma configuration to ensure it matches in your [project connect page](https://supabase.com/dashboard/project/_?showConnect=true).\n- **Increase connection timeout**: Try increasing the `connect_timeout` parameter in your Prisma configuration to give it more time to establish a connection.\n\n```md\n.../postgres?connect_timeout=30\n```\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Timed out fetching a new connection from the connection pool:",
          "content": "Prisma is unable to allocate connections to pending queries fast enough to meet demand.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Possible causes: [#possible-causes-timed-out-fetching-a-new-connection]",
          "content": "- **Overwhelmed server**: The server hosting Prisma is under heavy load, limiting its ability to manage connections. By default, Prisma will create the default `num_cpus * 2 + 1` worth of connections. A common cause for server strain is increasing the `connection_limit` significantly past the default.\n- **Insufficient pool size**: The Supavisor pooler does not have enough connections available to quickly satisfy Prisma's requests.\n- **Slow queries**: Prisma's queries are taking too long to execute, preventing it from releasing connections for reuse.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Solutions: [#solution-timed-out-fetching-a-new-connection]",
          "content": "- **Increase the pool timeout**: Increase the `pool_timeout` parameter in your Prisma configuration to give the pooler more time to allocate connections.\n- **Reduce the connection limit**: If you've explicitly increased the `connection_limit` parameter in your Prisma configuration, try reducing it to a more reasonable value.\n- **Increase pool size**: If you are connecting with Supavisor, try increasing the pool size in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database).\n- **Optimize queries**: [Improve the efficiency of your queries](https://supabase.com/docs/guides/database/query-optimization) to reduce execution time.\n- **Increase compute size**: Like the preceding option, this is a strategy to reduce query execution time.\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Server has closed the connection",
          "content": "According to this [GitHub Issue for Prisma](https://github.com/prisma/prisma/discussions/7389), this error may be related to large return values for queries. It may also be caused by significant database strain.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Solutions: [#solution-server-has-closed-the-connection]",
          "content": "- **Limit row return sizes**: Try to limit the total amount of rows returned for particularly large requests.\n- **Minimize database strain**:Check the Reports Page for database strain. If there is obvious strain, consider [optimizing](https://supabase.com/docs/guides/database/query-optimization) or increasing compute size\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Drift detected: Your database schema is not in sync with your migration history",
          "content": "Prisma relies on migration files to ensure your database aligns with Prisma's model. External schema changes are detected as \"drift\", which Prisma will try to overwrite, potentially causing data loss.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Possible causes: [#possible-causes-your-database-schema-is-not-in-sync]",
          "content": "- **Supabase Managed Schemas**: Supabase may update managed schemas like auth and storage to introduce new features. Granting Prisma access to these schemas can lead to drift during updates.\n- **External Schema Modifications**: Your team or another tool might have modified the database schema outside of Prisma, causing drift.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Solution: [#solution-your-database-schema-is-not-in-sync]",
          "content": "- **Baselining migrations**: [baselining](https://www.prisma.io/docs/orm/prisma-migrate/workflows/baselining) re-syncs Prisma by capturing the current database schema as the starting point for future migrations.\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Max client connections reached",
          "content": "Postgres or Supavisor rejected a request for more connections",
          "level": 2
        },
        {
          "type": "section",
          "title": "Possible causes:[#possible-causes-max-client-connections-reached]",
          "content": "- **When working in transaction mode (port 6543):** The error \"Max client connections reached\" occurs when clients try to form more connections with the pooler than it can support.\n- **When working in session mode (port 5432):** The max amount of clients is restricted to the \"Pool Size\" value in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database). If the \"Pool Size\" is set to 15, even if the pooler can handle 200 client connections, it will still be effectively capped at 15 for each unique [\"database-role+database\" combination](https://github.com/orgs/supabase/discussions/21566).\n- **When working with direct connections**: Postgres is already servicing the max amount of connections",
          "level": 3
        },
        {
          "type": "section",
          "title": "Solutions [#solutions-causes-max-client-connections-reached]",
          "content": "- **Transaction Mode for serverless apps**: If you are using serverless functions (Supabase Edge, Vercel, AWS Lambda), switch to transaction mode (port 6543). It handles more connections than session mode or direct connections.\n- **Reduce the number of Prisma connections**: A single client-server can establish multiple connections with a pooler. Typically, serverless setups do not need many connections. Starting with fewer, like five or three, or even just one, is often sufficient. In serverless setups, begin with `connection_limit=1`, increasing cautiously if needed to avoid maxing out connections.\n- **Increase pool size**: If you are connecting with Supavisor, try increasing the pool size in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database).\n- **Disconnect appropriately**: Close Prisma connections when they are no longer needed.\n- **Decrease query time**: Reduce query complexity or add [strategic indexes](https://supabase.com/docs/guides/database/postgres/indexes) to your tables to speed up queries.\n- **Increase compute size**: Sometimes the best option is to increase your compute size, which also increases your max client size and query execution speed\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "Cross schema references are only allowed when the target schema is listed in the schemas property of your data-source",
          "content": "A Prisma migration is referencing a schema it is not permitted to manage.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Possible causes: [#possible-causes-cross-schema-references]",
          "content": "- A migration references a schema that Prisma is not permitted to manage",
          "level": 3
        },
        {
          "type": "section",
          "title": "Solutions: [#solutions-cross-schema-references]",
          "content": "- Multi-Schema support: If the external schema isn't Supabase managed, modify your `prisma.schema` file to enable the multi-Schema preview\n\n```ts prisma.schema\ngenerator client {\n provider = \"prisma-client-js\"\n previewFeatures = [\"multiSchema\"] //Add line\n}\n\ndatasource db {\n provider = \"postgresql\"\n url = env(\"DATABASE_URL\")\n directUrl = env(\"DIRECT_URL\")\n schemas = [\"public\", \"other_schema\"] //list out relevant schemas\n}\n```\n\n- Supabase managed schemas: Schemas managed by Supabase, such as `auth` and `storage`, may be changed to support new features. Referencing these schemas directly will cause schema drift in the future. It is best to remove references to these schemas from your migrations.\n\nAn alternative strategy to reference these tables is to duplicate values into Prisma managed table with triggers. Below is an example for duplicating values from `auth.users` into a table called `profiles`.\n\n Show/Hide Details\n\n ```sql table_in_public\n -- Create the 'profiles' table in the 'public' schema\n create table public.profiles (\n id uuid primary key, -- 'id' is a UUID and the primary key for the table\n email varchar(256) -- 'email' is a variable character field with a maximum length of 256 characters\n );\n ```\n\n ```sql trigger_on_insert\n -- Function to handle the insertion of a new user into the 'profiles' table\n create function public.handle_new_user()\n returns trigger\n language plpgsql\n security definer set search_path = ''\n as $$\n begin\n\n -- Insert the new user's data into the 'profiles' table\n insert into public.profiles (id, email)\n values (new.id, new.email);\n\n return new; -- Return the new record\n end;\n $$;\n ```\n\n ```sql trigger_on_update\n -- Function to handle the updating of a user's information in the 'profiles' table\n create function public.update_user()\n returns trigger\n language plpgsql\n security definer set search_path = ''\n as\n $$\n begin\n -- Update the user's data in the 'profiles' table\n update public.profiles\n set email = new.email -- Update the 'email' field\n where id = new.id; -- Match the 'id' field with the new record\n\n return new; -- Return the new record\n end;\n $$;\n ```\n\n ```sql trigger_on_delete\n -- Function to handle the deletion of a user from the 'profiles' table\n create function public.delete_user()\n returns trigger\n language plpgsql\n security definer set search_path = ''\n as\n $$\n begin\n -- Delete the user's data from the 'profiles' table\n delete from public.profiles\n where id = old.id; -- Match the 'id' field with the old record\n\n return old; -- Return the old record\n end;\n $$;\n ```\n\n ```sql triggers_on_auth\n -- Trigger to run 'handle_new_user' function after a new user is inserted into 'auth.users' table\n create trigger on_auth_user_created\n after insert on auth.users\n for each row execute procedure public.handle_new_user();\n\n -- Trigger to run 'update_user' function after a user is updated in the 'auth.users' table\n create trigger on_auth_user_updated\n after update on auth.users\n for each row execute procedure public.update_user();\n\n -- Trigger to run 'delete_user' function after a user is deleted from the 'auth.users' table\n create trigger on_auth_user_deleted\n after delete on auth.users\n for each row execute procedure public.delete_user();\n ```",
          "level": 3
        }
      ],
      "wordCount": 1470,
      "characterCount": 11528
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-psql",
      "identifier": "database-psql",
      "name": "Connecting with PSQL",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/psql",
      "dateModified": "2025-06-13T12:45:11.293009",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/psql.mdx",
      "frontmatter": {
        "id": "psql",
        "title": "Connecting with PSQL",
        "breadcrumb": "GUI Quickstarts",
        "hideToc": "true"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[`psql`](https://www.postgresql.org/docs/current/app-psql.html) is a command-line tool that comes with Postgres."
        },
        {
          "type": "section",
          "title": "Connecting with SSL",
          "content": "You should connect to your database using SSL wherever possible, to prevent snooping and man-in-the-middle attacks.\n\nYou can obtain your connection info and Server root certificate from your application's dashboard:\n\n![Connection Info and Certificate.](/docs/img/database/database-settings-ssl.png)\n\nDownload your [SSL certificate](#connecting-with-ssl) to `/path/to/prod-supabase.cer`.\n\nFind your connection settings. Go to your [`Database Settings`](https://supabase.com/dashboard/project/_/settings/database) and make sure `Use connection pooling` is checked. Change the connection mode to `Session`, and copy the parameters into the connection string:\n\n```shell\npsql \"sslmode=verify-full sslrootcert=/path/to/prod-supabase.cer host=[CLOUD_PROVIDER]-0-[REGION].pooler.supabase.com dbname=postgres user=postgres.[PROJECT_REF]\"\n```",
          "level": 2
        }
      ],
      "wordCount": 92,
      "characterCount": 970
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-query-optimization",
      "identifier": "database-query-optimization",
      "name": "Query Optimization",
      "description": "Choosing indexes to improve your query performance.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/query-optimization",
      "dateModified": "2025-06-13T12:45:11.293179",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/query-optimization.mdx",
      "frontmatter": {
        "title": "Query Optimization",
        "description": "Choosing indexes to improve your query performance.",
        "subtitle": "Choosing indexes to improve your query performance.",
        "footerHelpType": "postgres"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "When working with Postgres, or any relational database, indexing is key to improving query performance. Aligning indexes with common query patterns can speed up data retrieval by an order of magnitude.\n\nThis guide is intended to:\n\n- help identify parts of a query that have the potential to be improved by indexes\n- introduce tooling to help identify useful indexes\n\nThis is not a comprehensive resource, but rather a helpful starting point for your optimization journey.\n\nIf you're new to query optimization, you may be interested in [`index_advisor`](/docs/guides/database/extensions/index_advisor), our tool for automatically detecting indexes that improve performance on a given query."
        },
        {
          "type": "section",
          "title": "Example query",
          "content": "Consider the following example query that retrieves customer names and purchase dates from two tables:\n\n```sql\nselect\n a.name,\n b.date_of_purchase\nfrom\n customers as a\n join orders as b on a.id = b.customer_id\nwhere a.sign_up_date > '2023-01-01' and b.status = 'shipped'\norder by b.date_of_purchase\nlimit 10;\n```\n\nIn this query, there are several parts that indexes could likely help in optimizing the performance:",
          "level": 2
        },
        {
          "type": "section",
          "title": "`where` clause:",
          "content": "The `where` clause filters rows based on certain conditions, and indexing the columns involved can improve this process:\n\n- `a.sign_up_date`: If filtering by `sign_up_date` is common, indexing this column can speed up the query.\n- `b.status`: Indexing the status may be beneficial if the column has diverse values.\n\n```sql\ncreate index idx_customers_sign_up_date on customers (sign_up_date);\n\ncreate index idx_orders_status on orders (status);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`join` columns",
          "content": "Indexes on the columns used for joining tables can help Postgres avoid scanning tables in their entirety when connecting tables.\n\n- Indexing `a.id` and `b.customer_id` would likely improve the performance of the join in this query.\n- Note that if `a.id` is the primary key of the `customers` table it is already indexed\n\n```sql\ncreate index idx_orders_customer_id on orders (customer_id);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`order by` clause",
          "content": "Sorting can also be optimized by indexing:\n\n- An index on `b.date_of_purchase` can improve the sorting process, and is particularly beneficial when a subset of rows is being returned with a `limit` clause.\n\n```sql\ncreate index idx_orders_date_of_purchase on orders (date_of_purchase);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Key concepts",
          "content": "Here are some concepts and tools to keep in mind to help you identify the best index for the job, and measure the impact that your index had:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Analyze the query plan",
          "content": "Use the `explain` command to understand the query's execution. Look for slow parts, such as Sequential Scans or high cost numbers. If creating an index does not reduce the cost of the query plan, remove it.\n\nFor example:\n\n```sql\nexplain select * from customers where sign_up_date > 25;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Use appropriate index types",
          "content": "Postgres offers various index types like [B-tree, Hash, GIN, etc](https://www.postgresql.org/docs/current/indexes-types.html). Select the type that best suits your data and query pattern. Using the right index type can make a significant difference. For example, using a BRIN index on a field that always increases and lives within a table that updates infrequently - like `created_at` on an `orders` table - routinely results in indexes that are +10x smaller than the equivalent default B-tree index. That translates into better scalability.\n\n```sql\ncreate index idx_orders_created_at ON customers using brin(created_at);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Partial indexes",
          "content": "For queries that frequently target a subset of data, a partial index could be faster and smaller than indexing the entire column. A partial index contains a `where` clause to filter the values included in the index. Note that a query's `where` clause must match the index for it to be used.\n\n```sql\ncreate index idx_orders_status on orders (status)\nwhere status = 'shipped';\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Composite indexes",
          "content": "If filtering or joining on multiple columns, a composite index prevents Postgres from referring to multiple indexes when identifying the relevant rows.\n\n```sql\ncreate index idx_customers_sign_up_date_priority on customers (sign_up_date, priority);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Over-Indexing",
          "content": "Avoid the urge to index columns you operate on infrequently. While indexes can speed up reads, they also slow down writes, so it's important to balance those factors when making indexing decisions.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Statistics",
          "content": "Postgres maintains a set of statistics about the contents of your tables. Those statistics are used by the query planner to decide when it's is more efficient to use an index vs scanning the entire table. If the collected statistics drift too far from reality, the query planner may make poor decisions. To avoid this risk, you can periodically `analyze` tables.\n\n```sql\nanalyze customers;\n```\n\n---\n\nBy following this guide, you'll be able to discern where indexes can optimize queries and enhance your Postgres performance. Remember that each database is unique, so always consider the specific context and use case of your queries.",
          "level": 3
        }
      ],
      "wordCount": 756,
      "characterCount": 5006
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-replication",
      "identifier": "database-replication",
      "name": "Replication and change data capture",
      "description": "An introduction to logical replication and change data capture",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/replication",
      "dateModified": "2025-06-13T12:45:11.293358",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/replication.mdx",
      "frontmatter": {
        "title": "Replication and change data capture",
        "description": "An introduction to logical replication and change data capture"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Replication is the process of copying changes from your database to another location. It's also referred to as change data capture (CDC): capturing all the changes that occur to your data."
        },
        {
          "type": "section",
          "title": "Use cases",
          "content": "You might use replication for:\n\n- **Analytics and Data Warehousing**: Replicate your operational database to analytics platforms like BigQuery for complex analysis without impacting your application's performance.\n- **Data Integration**: Keep your data synchronized across different systems and services in your tech stack.\n- **Backup and Disaster Recovery**: Maintain up-to-date copies of your data in different locations.\n- **Read Scaling**: Distribute read operations across multiple database instances to improve performance.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Replication in Postgres",
          "content": "Postgres comes with built-in support for replication via publications and replication slots. Refer to the [Concepts and terms](#concepts-and-terms) section to learn how replication works.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Setting up and monitoring replication in Supabase",
          "content": "- [Setting up replication](/docs/guides/database/replication/setting-up-replication)\n- [Monitoring replication](/docs/guides/database/replication/monitoring-replication)\n\nIf you want to set up a read replica, see [Read Replicas](/docs/guides/platform/read-replicas) instead. If you want to sync your data in real time to a client such as a browser or mobile app, see [Realtime](/docs/guides/realtime) instead.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Write-Ahead Log (WAL)",
          "content": "Postgres uses a system called the Write-Ahead Log (WAL) to manage changes to the database. As you make changes, they are appended to the WAL (which is a series of files (also called \"segments\"), where the file size can be specified). Once one segment is full, Postgres will start appending to a new segment. After a period of time, a checkpoint occurs and Postgres synchronizes the WAL with your database. Once the checkpoint is complete, then the WAL files can be removed from disk and free up space.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Logical replication and WAL",
          "content": "Logical replication is a method of replication where Postgres uses the WAL files and transmit those changes to another Postgres database, or a system that supports reading WAL files.",
          "level": 3
        },
        {
          "type": "section",
          "title": "LSN",
          "content": "LSN is a Log Sequence Number that is used to identify the position of a WAL file in the WAL directory. It is often used to determine the progress of replication in subscribers and calculate the lag of a replication slot.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Logical replication architecture",
          "content": "When setting up logical replication, three key components are involved:\n\n- `publication` - A set of tables on your primary database that will be `published`\n- `replication slot` - A slot used for replicating the data from a single publication. The slot, when created, will specify the output format of the changes\n- `subscription` - A subscription is created from an external system (i.e. another Postgres database) and must specify the name of the `publication`. If you do not specify a replication slot, one is automatically created",
          "level": 2
        },
        {
          "type": "section",
          "title": "Logical replication output format",
          "content": "Logical replication is typically output in 2 forms, `pgoutput` and `wal2json`. The output method is how Postgres sends changes to any active replication slot.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Logical replication configuration",
          "content": "When using logical replication, Postgres is then configured to keep WAL files around for longer than it needs them. If the files are removed too quickly, then your `replication slot` will become inactive and, if the database receives a large number of changes in a short time, then the `replication slot` can become lost as it was not able to keep up.\n\nIn order to mitigate this, Postgres has many options and settings that can be [tweaked](/guides/database/custom-postgres-config) to manage the WAL usage effectively. Not all of these settings are user configurable as they can impact the stability of your database. For those that are, these should be considered as advanced configuration and not changed without understanding that they can cause additional disk space and resources to be used, as well as incur additional costs.\n\n| Setting | Description | User-facing | Default |\n| ---------------------------------------------------------------------------------------- | ------------------------------------------------------ | ----------- | ------- |\n| [`max_replication_slots`](https://postgresqlco.nf/doc/en/param/max_replication_slots/) | Max count of replication slots allowed | No | |\n| [`wal_keep_size`](https://postgresqlco.nf/doc/en/param/wal_keep_size/) | Minimum size of WAL files to keep for replication | No | |\n| [`max_slot_wal_keep_size`](https://postgresqlco.nf/doc/en/param/max_slot_wal_keep_size/) | Max WAL size that can be reserved by replication slots | No | |\n| [`checkpoint_timeout`](https://postgresqlco.nf/doc/en/param/checkpoint_timeout/) | Max time between WAL checkpoints | No | |",
          "level": 2
        }
      ],
      "wordCount": 690,
      "characterCount": 4840
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-replication-faq",
      "identifier": "database-replication-faq",
      "name": "FAQs",
      "description": "Considerations and FAQs when setting up replication",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/replication/faq",
      "dateModified": "2025-06-13T12:45:11.293457",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/replication/faq.mdx",
      "frontmatter": {
        "id": "replication-faq",
        "title": "FAQs",
        "description": "Considerations and FAQs when setting up replication",
        "sidebar_label": "FAQs"
      },
      "sections": [
        {
          "type": "section",
          "title": "Which connection string should be used?",
          "content": "Always use the direct connection string for logical replication.\n\nConnections through a pooler, such as Supavisor, will not work.",
          "level": 1
        },
        {
          "type": "section",
          "title": "The tool in use does not support IPv6",
          "content": "You can enable the [IPv4 add-on](/guides/platform/ipv4-address) for your project.",
          "level": 1
        },
        {
          "type": "section",
          "title": "What is XMIN and should it be used?",
          "content": "Xmin is a different form of replication from logical replication and should only be used if logical replication is not available for your database (i.e. older versions of Postgres).\n\nXmin performs replication by checking the [xmin system column](https://www.postgresql.org/docs/current/ddl-system-columns.html) and determining if that row has already been synchronized.\n\nIt does not capture deletion of data and is **not recommended**, particularly for larger databases.",
          "level": 1
        },
        {
          "type": "section",
          "title": "Can replication be configured in the Dashboard?",
          "content": "You can view [publications](https://supabase.com/dashboard/project/default/database/publications) in the Dashboard but all steps to configure replication must be done using the [SQL Editor](https://supabase.com/dashboard/project/default/sql/new) or a CLI tool of your choice.",
          "level": 1
        },
        {
          "type": "section",
          "title": "How to configure database settings for replication?",
          "content": "Yes. Using the Supabase CLI, you can [configure database settings](/guides/database/custom-postgres-config#cli-configurable-settings) to optimize them for your replication needs. These values can vary depending on the activity of your database size and activity.",
          "level": 1
        },
        {
          "type": "section",
          "title": "What are some important configuration options?",
          "content": "Some of the more important options to be aware of are:\n\n- `max_wal_size`\n- `max_slot_wal_keep_size`\n- `wal_keep_size`\n- `max_wal_senders`",
          "level": 1
        }
      ],
      "wordCount": 215,
      "characterCount": 1643
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-replication-monitoring-replication",
      "identifier": "database-replication-monitoring-replication",
      "name": "Monitoring replication",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/replication/monitoring-replication",
      "dateModified": "2025-06-13T12:45:11.293579",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/replication/monitoring-replication.mdx",
      "frontmatter": {
        "title": "Monitoring replication"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Monitoring replication lag is important and there are 3 ways to do this:\n\n1. Dashboard - Under the [Reports](/guides/platform/reports) of the dashboard, you can view the replication lag of your project\n2. Database -\n - pg_stat_subscription (subscriber) - if PID is null, then the subscription is not active\n - pg_stat_subscription_stats - look here for error_count to see if there were issues applying or syncing (if yes, check the logs for why)\n - pg_replication_slots - use this to check if the slot is active and you can also calculate the lag from here\n3. [Metrics](/guides/telemetry/metrics) - Using the prometheus endpoint for your project\n - replication_slots_max_lag_bytes - this is the more important one\n - pg_stat_replication_replay_lag - lag to replay WAL files from the source DB on the target DB (throttled by disk or high activity)\n - pg_stat_replication_send_lag - lag in sending WAL files from the source DB (a high lag means that the publisher is not being asked to send new WAL files OR a network issues)"
        },
        {
          "type": "section",
          "title": "Replication status and lag",
          "content": "The `pg_stat_replication` table shows the status of any replicas connected to the primary database.\n\n```sql\nselect pid, application_name, state, sent_lsn, write_lsn, flush_lsn, replay_lsn, sync_state\nfrom pg_stat_replication;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Replication slot status",
          "content": "A replication slot can be in one of three states:\n\n- `active` - The slot is active and is receiving data\n- `inactive` - The slot is not active and is not receiving data\n- `lost` - The slot is lost and is not receiving data\n\nThe state can be checked using the `pg_replication_slots` table:\n\n```sql\nselect slot_name, active, state from pg_replication_slots;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "WAL size",
          "content": "The WAL size can be checked using the `pg_ls_waldir()` function:\n\n```sql\nselect * from pg_ls_waldir();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Check LSN",
          "content": "```sql\nselect pg_current_wal_lsn();\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Subscription status",
          "content": "The `pg_subscription` table shows the status of any subscriptions on a replica and the `pg_subscription_rel` table shows the status of each table within a subscription.\n\nThe `srsubstate` column in `pg_subscription_rel` can be one of the following:\n\n- `i` - Initializing - The subscription is being initialized\n- `d` - Data Synchronizing - The subscription is synchronizing data for the first time (i.e. doing the initial copy)\n- `s` - Synchronized - The subscription is synchronized\n- `r` - Replicating - The subscription is replicating data\n\n```sql\nSELECT\n sub.subname AS subscription_name,\n relid::regclass AS table_name,\n srel.srsubstate AS replication_state,\n CASE srel.srsubstate\n WHEN 'i' THEN 'Initializing'\n WHEN 'd' THEN 'Data Synchronizing'\n WHEN 's' THEN 'Synchronized'\n WHEN 'r' THEN 'Replicating'\n ELSE 'Unknown'\n END AS state_description,\n srel.srsyncedlsn AS last_synced_lsn\nFROM\n pg_subscription sub\nJOIN\n pg_subscription_rel srel ON sub.oid = srel.srsubid\nORDER BY\n table_name;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Check LSN",
          "content": "```sql\nselect pg_last_wal_replay_lsn();\n```",
          "level": 3
        }
      ],
      "wordCount": 441,
      "characterCount": 2966
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-replication-setting-up-replication",
      "identifier": "database-replication-setting-up-replication",
      "name": "Setting up replication and CDC with Supabase",
      "description": "Performing Extract Transform Load (ETL) with Supabase",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/replication/setting-up-replication",
      "dateModified": "2025-06-13T12:45:11.293701",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/replication/setting-up-replication.mdx",
      "frontmatter": {
        "id": "setting-up-replication",
        "title": "Setting up replication and CDC with Supabase",
        "description": "Performing Extract Transform Load (ETL) with Supabase",
        "sidebar_label": "Setting up replication and CDC"
      },
      "sections": [
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "To set up replication, the following is recommended:\n\n- Instance size of XL or greater\n- [IPv4 add-on](/docs/guides/platform/ipv4-address) enabled\n\nTo create a replication slot, you will need to use the `postgres` user and follow the instructions in our [guide](/docs/guides/database/postgres/setup-replication-external).\n\n If you are running Postgres 17 or higher, you can create a new user and grant them replication\n permissions with the `postgres` user. For versions below 17, you will need to use the `postgres`\n user.\n\nIf you are replicating to an external system and using any of the tools below, check their documentation first and we have added additional information where the setup with Supabase can vary.\n\nAirbyte has the following [documentation](https://docs.airbyte.com/integrations/sources/postgres/) for setting up Postgres as a source, either in their cloud offering or by self-hosting.\n\nYou can follow those steps with the following modifications:\n\n1. Use the `postgres` user\n2. Select `logical replication` as the replication method (`xmin` is possible, but not recommended)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Troubleshooting",
          "content": "Airbyte has a known [issue](https://discuss.airbyte.io/t/postgres-source-replication-slot-safe-wal-size-only-reset-when-a-change-occurs/3263/7) where it does not clear WAL files on each successful sync. The recommended workaround is to have a `heartbeat` table that you write changes to once an hour.>\n\nEstuary has the following [documentation](https://docs.estuary.dev/reference/Connectors/capture-connectors/PostgreSQL/Supabase/) for setting up Postgres as a source.\n\nFivetran has the following [documentation](https://fivetran.com/docs/connectors/databases/postgresql/setup-guide) for setting up Postgres as a source.\n\nYou can follow those steps with the following modifications:\n\n1. In Step 2, choose `logical replication` as the sync mechanism\n2. In Step 3, do not create a user and use the existing `postgres` user for replication\n3. In Step 5, no need to modify any WAL settings as we have done that for you\n\nMaterialize has the following [documentation](https://materialize.com/docs/sql/create-source/postgres/) on setting up Postgres as a source.\n\nYou can follow those steps with the following modifications:\n\n1. Follow the steps in our [guide](/guides/database/postgres/setup-replication-external) to create a publication slot\n\nStitch has the following [documentation](https://www.stitchdata.com/docs/integrations/databases/postgresql/v2#extract-data) on configuring Postgres as a source.\n\nYou can follow those steps with the following modifications:\n\n1. Use the `postgres` user for replication\n2. Skip step 3",
          "level": 2
        }
      ],
      "wordCount": 334,
      "characterCount": 2653
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-secure-data",
      "identifier": "database-secure-data",
      "name": "Securing your data",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/secure-data",
      "dateModified": "2025-06-13T12:45:11.293796",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/secure-data.mdx",
      "frontmatter": {
        "id": "securing-your-data",
        "title": "Securing your data"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase helps you control access to your data. With access policies, you can protect sensitive data and make sure users only access what they're allowed to see."
        },
        {
          "type": "section",
          "title": "Connecting your app securely",
          "content": "Supabase allows you to access your database using the auto-generated [Data APIs](/docs/guides/database/connecting-to-postgres#data-apis). This speeds up the process of building web apps, since you don't need to write your own backend services to pass database queries and results back and forth.\n\nYou can keep your data secure while accessing the Data APIs from the frontend, so long as you:\n\n- Turn on [Row Level Security](/docs/guides/database/postgres/row-level-security) (RLS) for your tables\n- Use your Supabase **anon key** when you create a Supabase client\n\nYour anon key is safe to expose with RLS enabled, because row access permission is checked against your access policies and the user's [JSON Web Token (JWT)](/docs/learn/auth-deep-dive/auth-deep-dive-jwts). The JWT is automatically sent by the Supabase client libraries if the user is logged in using Supabase Auth.\n\nUnlike your anon key, your **service role key** is **never** safe to expose because it bypasses RLS. Only use your service role key on the backend. Treat it as a secret (for example, import it as a sensitive environment variable instead of hardcoding it).",
          "level": 2
        },
        {
          "type": "section",
          "title": "More information",
          "content": "Supabase and Postgres provide you with multiple ways to manage security, including but not limited to Row Level Security. See the Access and Security pages for more information:\n\n- [Row Level Security](/docs/guides/database/postgres/row-level-security)\n- [Column Level Security](/docs/guides/database/postgres/column-level-security)\n- [Hardening the Data API](/docs/guides/database/hardening-data-api)\n- [Managing Postgres roles](/docs/guides/database/postgres/roles)\n- [Managing secrets with Vault](/docs/guides/database/vault)",
          "level": 2
        }
      ],
      "wordCount": 255,
      "characterCount": 1884
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-supavisor",
      "identifier": "database-supavisor",
      "name": "Supavisor",
      "description": "",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/supavisor",
      "dateModified": "2025-06-13T12:45:11.293910",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/supavisor.mdx",
      "frontmatter": {
        "title": "Supavisor",
        "subtitle": "Troubleshooting Supavisor errors"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Supavisor logs are available under [Pooler Logs](/dashboard/project/_/logs/pooler-logs) in the Dashboard. The following are common errors and their solutions:\n\n| Error Type | Description | Resolution Link |\n| ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n| Max client connections reached | This error happens when the number of connections to Supavisor is more than [the allowed limit of your compute add-on](https://supabase.com/docs/guides/platform/compute-add-ons). | Follow this [guide](https://github.com/orgs/supabase/discussions/22305) to resolve. |\n| Connection failed `{:error, :eaddrnotavail}` to 'db.xxx.supabase.co':5432 | Supavisor cannot connect to the customer database. This is usually caused if the target database is unable to respond. | N/A |\n| Connection failed `{:error, :nxdomain}` to 'db.xxx.supabase.co':5432 | Supavisor cannot connect to the customer database. This is usually caused if the target database is unable to respond. | N/A |\n| Connection closed when state was authentication | This error happens when either the database doesn’t exist or if the user doesn't have the right credentials. | N/A |\n| Subscribe error: `{:error, :worker_not_found}` | This log event is emitted when the client tries to connect to the database, but Supavisor does not have the necessary information to route the connection. Try reconnecting to the database as it can take some time for the project information to propagate to Supavisor. | N/A |\n| Subscribe error: `{:error, {:badrpc, {:error, {:erpc, :timeout}}}}` | This is a timeout error when the communication between different Supavisor nodes takes longer than expected. Try reconnecting to the database. | N/A |\n| Terminating with reason :client_termination when state was :busy | This error happens when the client terminates the connection before the connection with the database is completed. | N/A |\n| Error: received invalid response to GSSAPI negotiation: S | This error happens due to `gssencmode` parameter not set to disabled. | Follow this [guide](https://github.com/orgs/supabase/discussions/30173) to resolve. |"
        }
      ],
      "wordCount": 301,
      "characterCount": 2478
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-tables",
      "identifier": "database-tables",
      "name": "Tables and Data",
      "description": "Creating and using Postgres tables.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/tables",
      "dateModified": "2025-06-13T12:45:11.294627",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/tables.mdx",
      "frontmatter": {
        "id": "tables",
        "title": "Tables and Data",
        "description": "Creating and using Postgres tables.",
        "video": "https://www.youtube.com/v/TKwF3IGij5c"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Tables are where you store your data.\n\nTables are similar to excel spreadsheets. They contain columns and rows.\nFor example, this table has 3 \"columns\" (`id`, `name`, `description`) and 4 \"rows\" of data:\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| `id` | `name` | `description` |\n| ---- | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 1 | The Phantom Menace | Two Jedi escape a hostile blockade to find allies and come across a young boy who may bring balance to the Force. |\n| 2 | Attack of the Clones | Ten years after the invasion of Naboo, the Galactic Republic is facing a Separatist movement. |\n| 3 | Revenge of the Sith | As Obi-Wan pursues a new threat, Anakin acts as a double agent between the Jedi Council and Palpatine and is lured into a sinister plan to rule the galaxy. |\n| 4 | Star Wars | Luke Skywalker joins forces with a Jedi Knight, a cocky pilot, a Wookiee and two droids to save the galaxy from the Empire's world-destroying battle station. |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\nThere are a few important differences from a spreadsheet, but it's a good starting point if you're new to Relational databases."
        },
        {
          "type": "section",
          "title": "Creating tables",
          "content": "When creating a table, it's best practice to add columns at the same time.\n\nYou must define the \"data type\" of each column when it is created. You can add and remove columns at any time after creating a table.\n\nSupabase provides several options for creating tables. You can use the Dashboard or create them directly using SQL.\nWe provide a SQL editor within the Dashboard, or you can [connect](../../guides/database/connecting-to-postgres) to your database\nand run the SQL queries yourself.\n\n1. Go to the [Table Editor](https://supabase.com/dashboard/project/_/editor) page in the Dashboard.\n2. Click **New Table** and create a table with the name `todos`.\n3. Click **Save**.\n4. Click **New Column** and create a column with the name `task` and type `text`.\n5. Click **Save**.\n\n```sql\ncreate table movies (\n id bigint generated by default as identity primary key,\n name text,\n description text\n);\n```\n\nWhen naming tables, use lowercase and underscores instead of spaces (e.g., `table_name`, not `Table Name`).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Columns",
          "content": "You must define the \"data type\" when you create a column.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Data types",
          "content": "Every column is a predefined type. Postgres provides many [default types](https://www.postgresql.org/docs/current/datatype.html), and you can even design your own (or use extensions) if the default types don't fit your needs. You can use any data type that Postgres supports via the SQL editor. We only support a subset of these in the Table Editor in an effort to keep the experience simple for people with less experience with databases.\n\nShow/Hide default data types\n\n| `Name` | `Aliases` | `Description` |\n| --------------------------------- | ------------- | ---------------------------------------------------------------- |\n| `bigint` | `int8` | signed eight-byte integer |\n| `bigserial` | `serial8` | autoincrementing eight-byte integer |\n| `bit` | | fixed-length bit string |\n| `bit varying` | `varbit` | variable-length bit string |\n| `boolean` | `bool` | logical Boolean (true/false) |\n| `box` | | rectangular box on a plane |\n| `bytea` | | binary data (“byte array”) |\n| `character` | `char` | fixed-length character string |\n| `character varying` | `varchar` | variable-length character string |\n| `cidr` | | IPv4 or IPv6 network address |\n| `circle` | | circle on a plane |\n| `date` | | calendar date (year, month, day) |\n| `double precision` | `float8` | double precision floating-point number (8 bytes) |\n| `inet` | | IPv4 or IPv6 host address |\n| `integer` | `int`, `int4` | signed four-byte integer |\n| `interval [ fields ]` | | time span |\n| `json` | | textual JSON data |\n| `jsonb` | | binary JSON data, decomposed |\n| `line` | | infinite line on a plane |\n| `lseg` | | line segment on a plane |\n| `macaddr` | | MAC (Media Access Control) address |\n| `macaddr8` | | MAC (Media Access Control) address (EUI-64 format) |\n| `money` | | currency amount |\n| `numeric` | `decimal` | exact numeric of selectable precision |\n| `path` | | geometric path on a plane |\n| `pg_lsn` | | Postgres Log Sequence Number |\n| `pg_snapshot` | | user-level transaction ID snapshot |\n| `point` | | geometric point on a plane |\n| `polygon` | | closed geometric path on a plane |\n| `real` | `float4` | single precision floating-point number (4 bytes) |\n| `smallint` | `int2` | signed two-byte integer |\n| `smallserial` | `serial2` | autoincrementing two-byte integer |\n| `serial` | `serial4` | autoincrementing four-byte integer |\n| `text` | | variable-length character string |\n| `time [ without time zone ]` | | time of day (no time zone) |\n| `time with time zone` | `timetz` | time of day, including time zone |\n| `timestamp [ without time zone ]` | | date and time (no time zone) |\n| `timestamp with time zone` | `timestamptz` | date and time, including time zone |\n| `tsquery` | | text search query |\n| `tsvector` | | text search document |\n| `txid_snapshot` | | user-level transaction ID snapshot (deprecated; see pg_snapshot) |\n| `uuid` | | universally unique identifier |\n| `xml` | | XML data |\n\nYou can \"cast\" columns from one type to another, however there can be some incompatibilities between types.\nFor example, if you cast a `timestamp` to a `date`, you will lose all the time information that was previously saved.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Primary keys",
          "content": "A table can have a \"primary key\" - a unique identifier for every row of data. A few tips for Primary Keys:\n\n- It's recommended to create a Primary Key for every table in your database.\n- You can use any column as a primary key, as long as it is unique for every row.\n- It's common to use a `uuid` type or a numbered `identity` column as your primary key.\n\n```sql\ncreate table movies (\n id bigint generated always as identity primary key\n);\n```\n\nIn the example above, we have:\n\n1. created a column called `id`\n1. assigned the data type `bigint`\n1. instructed the database that this should be `generated always as identity`, which means that Postgres will automatically assign a unique number to this column.\n1. Because it's unique, we can also use it as our `primary key`.\n\nWe could also use `generated by default as identity`, which would allow us to insert our own unique values.\n\n```sql\ncreate table movies (\n id bigint generated by default as identity primary key\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Loading data",
          "content": "There are several ways to load data in Supabase. You can load data directly into the database or using the [APIs](../../guides/database/api).\nUse the \"Bulk Loading\" instructions if you are loading large data sets.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Basic data loading",
          "content": "```sql\ninsert into movies\n (name, description)\nvalues\n (\n 'The Empire Strikes Back',\n 'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.'\n ),\n (\n 'Return of the Jedi',\n 'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.'\n );\n```\n\n```js\nconst { data, error } = await supabase.from('movies').insert([\n {\n name: 'The Empire Strikes Back',\n description:\n 'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.',\n },\n {\n name: 'Return of the Jedi',\n description:\n 'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.',\n },\n])\n```\n\n```dart\nawait supabase\n .from('movies')\n .insert([{\n name: 'The Empire Strikes Back',\n description: 'After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.'\n }, {\n name: 'Return of the Jedi',\n description: 'After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.'\n }]);\n```\n\n```swift\ntry await supabase.from(\"movies\")\n .insert(\n [\n [\n \"name\": \"The Empire Strikes Back\",\n \"description\":\n \"After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.\",\n ],\n [\n \"name\": \"Return of the Jedi\",\n \"description\":\n \"After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.\",\n ],\n ]\n )\n .execute()\n```\n\n```python\nclient.from_(\"movies\").insert([\n {\n \"name\": \"The Empire Strikes Back\",\n \"description\": \"After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.\"\n },\n {\n \"name\": \"Return of the Jedi\",\n \"description\": \"After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.\"\n }\n]).execute()\n```\n\n```kotlin\n@Serializable\ndata class Movie(\n val name: String,\n val description: String\n)\n```\n\n```kotlin\nsupabase\n .from(\"movies\")\n .insert(listOf(\n Movie(\"The Empire Strikes Back\", \"After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.\"),\n Movie(\"Return of the Jedi\", \"After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.\"),\n ))\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Bulk data loading",
          "content": "When inserting large data sets it's best to use PostgreSQL's [COPY](https://www.postgresql.org/docs/current/sql-copy.html) command.\nThis loads data directly from a file into a table. There are several file formats available for copying data: text, CSV, binary, JSON, etc.\n\nFor example, if you wanted to load a CSV file into your movies table:\n\n```text ./movies.csv\n\"The Empire Strikes Back\", \"After the Rebels are brutally overpowered by the Empire on the ice planet Hoth, Luke Skywalker begins Jedi training with Yoda.\"\n\"Return of the Jedi\", \"After a daring mission to rescue Han Solo from Jabba the Hutt, the Rebels dispatch to Endor to destroy the second Death Star.\"\n```\n\nYou would [connect](../../guides/database/connecting-to-postgres#direct-connections) to your database directly and load the file with the COPY command:\n\n```bash\npsql -h DATABASE_URL -p 5432 -d postgres -U postgres \\\n -c \"\\COPY movies FROM './movies.csv';\"\n```\n\nAdditionally use the `DELIMITER`, `HEADER` and `FORMAT` options as defined in the Postgres [COPY](https://www.postgresql.org/docs/current/sql-copy.html) docs.\n\n```bash\npsql -h DATABASE_URL -p 5432 -d postgres -U postgres \\\n -c \"\\COPY movies FROM './movies.csv' WITH DELIMITER ',' CSV HEADER\"\n```\n\nIf you receive an error `FATAL: password authentication failed for user \"postgres\"`, reset your database password in the Database Settings and try again.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Joining tables with foreign keys",
          "content": "Tables can be \"joined\" together using Foreign Keys.\n\nThis is where the \"Relational\" naming comes from, as data typically forms some sort of relationship.\n\nIn our \"movies\" example above, we might want to add a \"category\" for each movie (for example, \"Action\", or \"Documentary\").\nLet's create a new table called `categories` and \"link\" our `movies` table.\n\n```sql\ncreate table categories (\n id bigint generated always as identity primary key,\n name text -- category name\n);\n\nalter table movies\n add column category_id bigint references categories;\n```\n\nYou can also create \"many-to-many\" relationships by creating a \"join\" table.\nFor example if you had the following situations:\n\n- You have a list of `movies`.\n- A movie can have several `actors`.\n- An `actor` can perform in several movies.\n\n```sql\ncreate table movies (\n id bigint generated by default as identity primary key,\n name text,\n description text\n);\n\ncreate table actors (\n id bigint generated by default as identity primary key,\n name text\n);\n\ncreate table performances (\n id bigint generated by default as identity primary key,\n movie_id bigint not null references movies,\n actor_id bigint not null references actors\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Schemas",
          "content": "Tables belong to `schemas`. Schemas are a way of organizing your tables, often for security reasons.\n\nIf you don't explicitly pass a schema when creating a table, Postgres will assume that you want to create the table in the `public` schema.\n\nWe can create schemas for organizing tables. For example, we might want a private schema which is hidden from our API:\n\n```sql\ncreate schema private;\n```\n\nNow we can create tables inside the `private` schema:\n\n```sql\ncreate table private.salaries (\n id bigint generated by default as identity primary key,\n salary bigint not null,\n actor_id bigint not null references public.actors\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Views",
          "content": "A View is a convenient shortcut to a query. Creating a view does not involve new tables or data. When run, an underlying query is executed, returning its results to the user.\n\nSay we have the following tables from a database of a university:\n\n**`students`**\n\n{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| id | name | type |\n| --- | ---------------- | ------------- |\n| 1 | Princess Leia | undergraduate |\n| 2 | Yoda | graduate |\n| 3 | Anakin Skywalker | graduate |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}\n\n**`courses`**\n\n| id | title | code |\n| --- | ------------------------ | ------- |\n| 1 | Introduction to Postgres | PG101 |\n| 2 | Authentication Theories | AUTH205 |\n| 3 | Fundamentals of Supabase | SUP412 |\n\n**`grades`**\n\n| id | student_id | course_id | result |\n| --- | ---------- | --------- | ------ |\n| 1 | 1 | 1 | B+ |\n| 2 | 1 | 3 | A+ |\n| 3 | 2 | 2 | A |\n| 4 | 3 | 1 | A- |\n| 5 | 3 | 2 | A |\n| 6 | 3 | 3 | B- |\n\nCreating a view consisting of all the three tables will look like this:\n\n```sql\ncreate view transcripts as\n select\n students.name,\n students.type,\n courses.title,\n courses.code,\n grades.result\n from grades\n left join students on grades.student_id = students.id\n left join courses on grades.course_id = courses.id;\n\ngrant all on table transcripts to authenticated;\n```\n\nOnce done, we can now access the underlying query with:\n\n```sql\nselect * from transcripts;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "View security",
          "content": "By default, views are accessed with their creator's permission (\"security definer\"). If a privileged role creates a view, others accessing it will use that role's elevated permissions. To enforce row level security policies, define the view with the \"security invoker\" modifier.\n\n```sql\n-- alter a security_definer view to be security_invoker\nalter view \nset (security_invoker = true);\n\n-- create a view with the security_invoker modifier\ncreate view with(security_invoker=true) as (\n select * from \n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "When to use views",
          "content": "Views provide the several benefits:\n\n- Simplicity\n- Consistency\n- Logical Organization\n- Security",
          "level": 3
        },
        {
          "type": "section",
          "title": "Simplicity",
          "content": "As a query becomes more complex, it can be a hassle to call it over and over - especially when we run it regularly. In the example above, instead of repeatedly running:\n\n```sql\nselect\n students.name,\n students.type,\n courses.title,\n courses.code,\n grades.result\nfrom\n grades\n left join students on grades.student_id = students.id\n left join courses on grades.course_id = courses.id;\n```\n\nWe can run this instead:\n\n```sql\nselect * from transcripts;\n```\n\nAdditionally, a view behaves like a typical table. We can safely use it in table `JOIN`s or even create new views using existing views.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Consistency",
          "content": "Views ensure that the likelihood of mistakes decreases when repeatedly executing a query. In our example above, we may decide that we want to exclude the course _Introduction to Postgres_. The query would become:\n\n```sql\nselect\n students.name,\n students.type,\n courses.title,\n courses.code,\n grades.result\nfrom\n grades\n left join students on grades.student_id = students.id\n left join courses on grades.course_id = courses.id\nwhere courses.code != 'PG101';\n```\n\nWithout a view, we would need to go into every dependent query to add the new rule. This would increase in the likelihood of errors and inconsistencies, as well as introducing a lot of effort for a developer. With views, we can alter just the underlying query in the view **transcripts**. The change will be applied to all applications using this view.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Logical organization",
          "content": "With views, we can give our query a name. This is extremely useful for teams working with the same database. Instead of guessing what a query is supposed to do, a well-named view can explain it. For example, by looking at the name of the view **transcripts**, we can infer that the underlying query might involve the **students**, **courses**, and **grades** tables.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Security",
          "content": "Views can restrict the amount and type of data presented to a user. Instead of allowing a user direct access to a set of tables, we provide them a view instead. We can prevent them from reading sensitive columns by excluding them from the underlying query.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Materialized views",
          "content": "A [materialized view](https://www.postgresql.org/docs/12/rules-materializedviews.html) is a form of view but it also stores the results to disk. In subsequent reads of a materialized view, the time taken to return its results would be much faster than a conventional view. This is because the data is readily available for a materialized view while the conventional view executes the underlying query each time it is called.\n\nUsing our example above, a materialized view can be created like this:\n\n```sql\ncreate materialized view transcripts as\n select\n students.name,\n students.type,\n courses.title,\n courses.code,\n grades.result\n from\n grades\n left join students on grades.student_id = students.id\n left join courses on grades.course_id = courses.id;\n```\n\nReading from the materialized view is the same as a conventional view:\n\n```sql\nselect * from transcripts;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Refreshing materialized views",
          "content": "Unfortunately, there is a trade-off - data in materialized views are not always up to date. We need to refresh it regularly to prevent the data from becoming too stale. To do so:\n\n```sql\nrefresh materialized view transcripts;\n```\n\nIt's up to you how regularly refresh your materialized views, and it's probably different for each view depending on its use-case.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Materialized views vs conventional views",
          "content": "Materialized views are useful when execution times for queries or views are too slow. These could likely occur in views or queries involving multiple tables and billions of rows. When using such a view, however, there should be tolerance towards data being outdated. Some use-cases for materialized views are internal dashboards and analytics.\n\nCreating a materialized view is not a solution to inefficient queries. You should always seek to optimize a slow running query even if you are implementing a materialized view.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [Official Docs: Create table](https://www.postgresql.org/docs/current/sql-createtable.html)\n- [Official Docs: Create view](https://www.postgresql.org/docs/12/sql-createview.html)\n- [Postgres Tutorial: Create tables](https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-create-table/)\n- [Postgres Tutorial: Add column](https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-add-column/)\n- [Postgres Tutorial: Views](https://www.postgresqltutorial.com/postgresql-views/)",
          "level": 2
        }
      ],
      "wordCount": 3060,
      "characterCount": 19198
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-testing",
      "identifier": "database-testing",
      "name": "Testing Your Database",
      "description": "Test your database schema, tables, functions, and policies.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/testing",
      "dateModified": "2025-06-13T12:45:11.294799",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/testing.mdx",
      "frontmatter": {
        "id": "testing",
        "title": "Testing Your Database",
        "description": "Test your database schema, tables, functions, and policies."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "To ensure that queries return the expected data, RLS policies are correctly applied and etc., we encourage you to write automated tests. There are essentially two approaches to testing:\n\n- Firstly, you can write tests that interface with a Supabase client instance (same way you use Supabase client in your application code) in the programming language(s) you use in your application and using your favorite testing framework.\n\n- Secondly, you can test through the Supabase CLI, which is a more low-level approach where you write tests in SQL."
        },
        {
          "type": "section",
          "title": "Testing using the Supabase CLI",
          "content": "You can use the Supabase CLI to test your database. The minimum required version of the CLI is [v1.11.4](https://github.com/supabase/cli/releases). To get started:\n\n- [Install the Supabase CLI](/docs/guides/cli) on your local machine",
          "level": 1
        },
        {
          "type": "section",
          "title": "Creating a test",
          "content": "Create a tests folder inside the `supabase` folder:\n\n```bash\nmkdir -p ./supabase/tests/database\n```\n\nCreate a new file with the `.sql` extension which will contain the test.\n\n```bash\ntouch ./supabase/tests/database/hello_world.test.sql\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Writing tests",
          "content": "All `sql` files use [pgTAP](/docs/guides/database/extensions/pgtap) as the test runner.\n\nLet's write a simple test to check that our `auth.users` table has an ID column. Open `hello_world.test.sql` and add the following code:\n\n```sql\nbegin;\nselect plan(1); -- only one statement to run\n\nSELECT has_column(\n 'auth',\n 'users',\n 'id',\n 'id should exist'\n);\n\nselect * from finish();\nrollback;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Running tests",
          "content": "To run the test, you can use:\n\n```bash\nsupabase test db\n```\n\nThis will produce the following output:\n\n```bash\n$ supabase test db\nsupabase/tests/database/hello_world.test.sql .. ok\nAll tests successful.\nFiles=1, Tests=1, 1 wallclock secs ( 0.01 usr 0.00 sys + 0.04 cusr 0.02 csys = 0.07 CPU)\nResult: PASS\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "More resources",
          "content": "- [Testing RLS policies](/docs/guides/database/extensions/pgtap#testing-rls-policies)\n- [pgTAP extension](/docs/guides/database/extensions/pgtap)\n- Official [pgTAP documentation](https://pgtap.org/)",
          "level": 2
        }
      ],
      "wordCount": 285,
      "characterCount": 2031
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-vault",
      "identifier": "database-vault",
      "name": "Vault",
      "description": "Vault is a Postgres extension and accompanying Supabase UI that makes it safe and easy to store encrypted secrets.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/vault",
      "dateModified": "2025-06-13T12:45:11.295046",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/vault.mdx",
      "frontmatter": {
        "id": "vault",
        "title": "Vault",
        "description": "Vault is a Postgres extension and accompanying Supabase UI that makes it safe and easy to store encrypted secrets.",
        "subtitle": "Managing secrets in Postgres.",
        "sidebar_label": "Overview",
        "video": "https://www.youtube.com/v/J9mTPY8rIXE"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Vault is a Postgres extension and accompanying Supabase UI that makes it safe and easy to store encrypted secrets and other data in your database. This opens up a lot of possibilities to use Postgres in ways that go beyond what is available in a stock distribution.\n\nUnder the hood, the Vault is a table of Secrets that are stored using [Authenticated Encryption](https://en.wikipedia.org/wiki/Authenticated_encryption) on disk. They are then available in decrypted form through a Postgres view so that the secrets can be used by applications from SQL. Because the secrets are stored on disk encrypted and authenticated, any backups or replication streams also preserve this encryption in a way that can't be decrypted or forged.\n\nSupabase provides a dashboard UI for the Vault that makes storing secrets easy. Click a button, type in your secret, and save.\n\nYou can use Vault to store secrets - everything from Environment Variables to API Keys. You can then use these secrets anywhere in your database: Postgres [Functions](/docs/guides/database/functions), Triggers, and [Webhooks](/docs/guides/database/webhooks). From a SQL perspective, accessing secrets is as easy as querying a table (or in this case, a view). The underlying secrets tables will be stored in encrypted form."
        },
        {
          "type": "section",
          "title": "Using Vault",
          "content": "You can manage secrets from the UI or using SQL.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Adding secrets",
          "content": "There is also a handy function for creating secrets called `vault.create_secret()`:\n\n```sql\nselect vault.create_secret('my_s3kre3t');\n```\n\nThe function returns the UUID of the new secret.\n\nShow Result\n\n```sql\n-[ RECORD 1 ]-+-------------------------------------\ncreate_secret | c9b00867-ca8b-44fc-a81d-d20b8169be17\n```\n\nSecrets can also have an optional _unique_ name and an optional description. These are also arguments to `vault.create_secret()`:\n\n```sql\nselect vault.create_secret('another_s3kre3t', 'unique_name', 'This is the description');\n```\n\nShow Result\n\n```sql\n-[ RECORD 1 ]-----------------------------------------------------------------\nid | 7095d222-efe5-4cd5-b5c6-5755b451e223\nname | unique_name\ndescription | This is the description\nsecret | 3mMeOcoG84a5F2uOfy2ugWYDp9sdxvCTmi6kTeT97bvA8rCEsG5DWWZtTU8VVeE=\nkey_id |\nnonce | \\x9f2d60954ba5eb566445736e0760b0e3\ncreated_at | 2022-12-14 02:34:23.85159+00\nupdated_at | 2022-12-14 02:34:23.85159+00\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Viewing secrets",
          "content": "If you look in the `vault.secrets` table, you will see that your data is stored encrypted. To decrypt the data, there is an automatically created view `vault.decrypted_secrets`. This view will decrypt secret data on the fly:\n\n{/* prettier-ignore */}\n```sql\nselect * \nfrom vault.decrypted_secrets \norder by created_at desc \nlimit 3;\n```\n\nShow Result\n\n```sql\n-[ RECORD 1 ]----+-----------------------------------------------------------------\nid | 7095d222-efe5-4cd5-b5c6-5755b451e223\nname | unique_name\ndescription | This is the description\nsecret | 3mMeOcoG84a5F2uOfy2ugWYDp9sdxvCTmi6kTeT97bvA8rCEsG5DWWZtTU8VVeE=\ndecrypted_secret | another_s3kre3t\nkey_id |\nnonce | \\x9f2d60954ba5eb566445736e0760b0e3\ncreated_at | 2022-12-14 02:34:23.85159+00\nupdated_at | 2022-12-14 02:34:23.85159+00\n-[ RECORD 2 ]----+-----------------------------------------------------------------\nid | c9b00867-ca8b-44fc-a81d-d20b8169be17\nname |\ndescription |\nsecret | a1CE4vXwQ53+N9bllJj1D7fasm59ykohjb7K90PPsRFUd9IbBdxIGZNoSQLIXl4=\ndecrypted_secret | another_s3kre3t\nkey_id |\nnonce | \\x1d3b2761548c4efb2d29ca11d44aa22f\ncreated_at | 2022-12-14 02:32:50.58921+00\nupdated_at | 2022-12-14 02:32:50.58921+00\n-[ RECORD 3 ]----+-----------------------------------------------------------------\nid | d91596b8-1047-446c-b9c0-66d98af6d001\nname |\ndescription |\nsecret | S02eXS9BBY+kE3r621IS8beAytEEtj+dDHjs9/0AoMy7HTbog+ylxcS22A==\ndecrypted_secret | s3kre3t_k3y\nkey_id |\nnonce | \\x3aa2e92f9808e496aa4163a59304b895\ncreated_at | 2022-12-14 02:29:21.3625+00\nupdated_at | 2022-12-14 02:29:21.3625+00\n```\n\nNotice how this view has a `decrypted_secret` column that contains the decrypted secrets. Views are not stored on disk, they are only run at query time, so the secret remains encrypted on disk, and in any backup dumps or replication streams.\n\nYou should ensure that you protect access to this view with the appropriate SQL privilege settings at all times, as anyone that has access to the view has access to decrypted secrets.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Updating secrets",
          "content": "A secret can be updated with the `vault.update_secret()` function, this function makes updating secrets easy, just provide the secret UUID as the first argument, and then an updated secret, updated optional unique name, or updated description:\n\n```sql\nselect\n vault.update_secret(\n '7095d222-efe5-4cd5-b5c6-5755b451e223',\n 'n3w_upd@ted_s3kret',\n 'updated_unique_name',\n 'This is the updated description'\n );\n```\n\nShow Result\n\n```sql\n-[ RECORD 1 ]-+-\nupdate_secret |\n\npostgres=> select * from vault.decrypted_secrets where id = '7095d222-efe5-4cd5-b5c6-5755b451e223';\n-[ RECORD 1 ]----+---------------------------------------------------------------------\nid | 7095d222-efe5-4cd5-b5c6-5755b451e223\nname | updated_unique_name\ndescription | This is the updated description\nsecret | lhb3HBFxF+qJzp/HHCwhjl4QFb5dYDsIQEm35DaZQOovdkgp2iy6UMufTKJGH4ThMrU=\ndecrypted_secret | n3w_upd@ted_s3kret\nkey_id |\nnonce | \\x9f2d60954ba5eb566445736e0760b0e3\ncreated_at | 2022-12-14 02:34:23.85159+00\nupdated_at | 2022-12-14 02:51:13.938396+00\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deep dive",
          "content": "As we mentioned, Vault uses Transparent Column Encryption (TCE) to store secrets in an authenticated encrypted form. There are some details around that you may be curious about. What does authenticated mean? Where is the encryption key stored? This section explains those details.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Authenticated encryption with associated data",
          "content": "The first important feature of TCE is that it uses an [Authenticated Encryption with Associated Data]() encryption algorithm (based on `libsodium`).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Encryption key location",
          "content": "**Authenticated Encryption** means that in addition to the data being encrypted, it is also signed so that it cannot be forged. You can guarantee that the data was encrypted by someone you trust, which you wouldn't get with encryption alone. The decryption function verifies that the signature is valid _before decrypting the value_.\n\n**Associated Data** means that you can include any other columns from the same row as part of the signature computation. This doesn't encrypt those other columns - rather it ensures that your encrypted value is only associated with columns from that row. If an attacker were to copy an encrypted value from another row to the current one, the signature would be rejected (assuming you used a unique column in the associated data).\n\nAnother important feature is that the encryption key is never stored in the database alongside the encrypted data. Even if an attacker can capture a dump of your entire database, they will see only encrypted data, _never the encryption key itself_.\n\nThis is an important safety precaution - there is little value in storing the encryption key in the database itself as this would be like locking your front door but leaving the key in the lock! Storing the key outside the database fixes this issue.\n\nWhere is the key stored? Supabase creates and manages the encryption key in our secured backend systems. We keep this key safe and separate from your data. You remain in control of your key - a separate API endpoint is available that you can use to access the key if you want to decrypt your data outside of Supabase.\n\nWhich roles should have access to the `vault.secrets` table should be carefully considered. There are two ways to grant access, the first is that the `postgres` user can explicitly grant access to the vault table itself.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Read more about Supabase Vault in the [blog post](https://supabase.com/blog/vault-now-in-beta)\n- [Supabase Vault on GitHub](https://github.com/supabase/vault)\n- [Column Encryption](/docs/guides/database/column-encryption)",
          "level": 3
        }
      ],
      "wordCount": 1047,
      "characterCount": 7970
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:database-webhooks",
      "identifier": "database-webhooks",
      "name": "Database Webhooks",
      "description": "Trigger external payloads on database events.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/database/webhooks",
      "dateModified": "2025-06-13T12:45:11.295186",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/database/webhooks.mdx",
      "frontmatter": {
        "id": "webhooks",
        "title": "Database Webhooks",
        "description": "Trigger external payloads on database events.",
        "subtitle": "Trigger external payloads on database events.",
        "tocVideo": "codAs9-NeHM"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Database Webhooks allow you to send real-time data from your database to another system whenever a table event occurs.\n\nYou can hook into three table events: `INSERT`, `UPDATE`, and `DELETE`. All events are fired _after_ a database row is changed."
        },
        {
          "type": "section",
          "title": "Webhooks vs triggers",
          "content": "Database Webhooks are very similar to triggers, and that's because Database Webhooks are just a convenience wrapper around triggers using the [pg_net](/docs/guides/database/extensions/pgnet) extension. This extension is asynchronous, and therefore will not block your database changes for long-running network requests.\n\nThis video demonstrates how you can create a new customer in Stripe each time a row is inserted into a `profiles` table:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating a webhook",
          "content": "1. Create a new [Database Webhook](https://supabase.com/dashboard/project/_/integrations/webhooks/overview) in the Dashboard.\n1. Give your Webhook a name.\n1. Select the table you want to hook into.\n1. Select one or more events (table inserts, updates, or deletes) you want to hook into.\n\nSince webhooks are just database triggers, you can also create one from SQL statement directly.\n\n```sql\ncreate trigger \"my_webhook\" after insert\non \"public\".\"my_table\" for each row\nexecute function \"supabase_functions\".\"http_request\"(\n 'http://host.docker.internal:3000',\n 'POST',\n '{\"Content-Type\":\"application/json\"}',\n '{}',\n '1000'\n);\n```\n\nWe currently support HTTP webhooks. These can be sent as `POST` or `GET` requests with a JSON payload.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Payload",
          "content": "The payload is automatically generated from the underlying table record:\n\n```typescript\ntype InsertPayload = {\n type: 'INSERT'\n table: string\n schema: string\n record: TableRecord\n old_record: null\n}\ntype UpdatePayload = {\n type: 'UPDATE'\n table: string\n schema: string\n record: TableRecord\n old_record: TableRecord\n}\ntype DeletePayload = {\n type: 'DELETE'\n table: string\n schema: string\n record: null\n old_record: TableRecord\n}\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Monitoring",
          "content": "Logging history of webhook calls is available under the `net` schema of your database. For more info, see the [GitHub Repo](https://github.com/supabase/pg_net).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Local development",
          "content": "When using Database Webhooks on your local Supabase instance, you need to be aware that the Postgres database runs inside a Docker container. This means that `localhost` or `127.0.0.1` in your webhook URL will refer to the container itself, not your host machine where your application is running.\n\nTo target services running on your host machine, use `host.docker.internal`. If that doesn't work, you may need to use your machine's local IP address instead.\n\nFor example, if you want to trigger an edge function when a webhook fires, your webhook URL would be:\n\n```\nhttp://host.docker.internal:54321/functions/v1/my-function-name\n```\n\nIf you're experiencing connection issues with webhooks locally, verify you're using the correct hostname instead of `localhost`.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [pg_net](/docs/guides/database/extensions/pgnet): an async networking extension for Postgres",
          "level": 2
        }
      ],
      "wordCount": 410,
      "characterCount": 2994
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-branching",
      "identifier": "deployment-branching",
      "name": "Branching",
      "description": "Use Supabase Branches to test and preview changes.",
      "category": "deployment",
      "url": "https://supabase.com/docs/guides/deployment/branching",
      "dateModified": "2025-06-13T12:45:11.296017",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/branching.mdx",
      "frontmatter": {
        "title": "Branching",
        "description": "Use Supabase Branches to test and preview changes.",
        "subtitle": "Use Supabase Branches to test and preview changes"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Use branching to safely experiment with changes to your Supabase project.\n\nSupabase branches work like Git branches. They let you create and test changes like new configurations, database schemas, or features in a separate, temporary instance without affecting your production setup.\n\nWhen you're ready to ship your changes, merge your branch to update your production instance with the new changes.\n\nIf you understand Git, you already understand Supabase Branching."
        },
        {
          "type": "section",
          "title": "How branching works",
          "content": "- **Separate Environments**: Each branch is a separate environment with its own Supabase instance and API credentials.\n- **Git Integration**: Branching works with Git, currently supporting GitHub repositories.\n- **Preview Branches**: You can create multiple Preview Branches for testing.\n- **Migrations and Seeding**: Branches run migrations from your repository and can seed data using a `seed.sql` file.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "- **Supabase Project**: You need an existing Supabase project.\n- **GitHub Repository**: Your project must be connected to a GitHub repository containing your Supabase directory.\n\nYou can run multiple Preview Branches for every Supabase project. Branches contain all the Supabase features with their own API credentials.\n\nPreview Environments auto-pause after branching.inactivity_period_in_minutes minutes of inactivity. Upon receiving a new request to your database or REST API, the paused branch will automatically resume to serve the request. The implications of this architecture means\n\n- `pg_cron` jobs will not execute in an auto-paused database.\n- Larger variance in request latency due to database cold starts.\n\nIf you need higher performance guarantees on your Preview Environment, you can switch individual branches to [persistent](/docs/guides/deployment/branching#persistent-branches) so they are not auto-paused.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Branching workflow",
          "content": "Preview Branch instances contain no data by default. You must include a seed file to seed your preview instance with sample data when the Preview Branch is created. Future versions of Branching may allow for automated data seeding and cloning after we are confident that we can provide safe data masking.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Git providers",
          "content": "To manage code changes, your Supabase project must be connected to a Git repository. At this stage, we only support [GitHub](#branching-with-github). If you are interested in other Git providers, join the [discussion](https://github.com/orgs/supabase/discussions/18936) for GitLab, Bitbucket, and non-Git based Branching.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Branching with GitHub",
          "content": "Supabase Branching uses the Supabase GitHub integration to read files from your GitHub repository. With this integration, Supabase watches all commits, branches, and pull requests of your GitHub repository.\n\nYou can create a corresponding Preview Branch for any Git branch in your repository. Each time a new Preview Branch is created and configured based on the [`config.toml`](/docs/guides/local-development/cli/config) configuration on this branch, the migrations from the corresponding Git branch are run on the Preview Branch.\n\nThe Preview Branch is also [seeded](/docs/guides/local-development/seeding-your-database) with sample data based on `./supabase/seed.sql` by default, if that file exists.\n\nSupabase Branching follows the [Trunk Based Development](https://trunkbaseddevelopment.com/) workflow, with one main Production branch and multiple development branches:\n\nWhen you merge your Git branch into the production branch, all new migrations will be applied to your Production environment. If you have declared Storage buckets or Edge Functions in `config.toml`, they will also be deployed automatically. All other configurations, including API, Auth, and seed files, will be ignored by default.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Preparing your Git repository",
          "content": "You can use the [Supabase CLI](/docs/guides/cli) to manage changes inside a local `./supabase` directory:\n\n If you don't have a `./supabase` directory, you can create one:\n\n ```markdown\n supabase init\n ```\n\n Pull your database changes using `supabase db pull`. To get your database connection string, go to your project dashboard, click [Connect](https://supabase.com/dashboard/project/_?showConnect=true) and look for the Session pooler connection string.\n\n ```markdown\n supabase db pull --db-url \n\n # Your Database connection string will look like this:\n # postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres\n ```\n \n If you're in an [IPv6 environment](https://github.com/orgs/supabase/discussions/27034) or have the IPv4 Add-On, you can use the direct connection string instead of Supavisor in Session mode.\n\n Commit the `supabase` directory to Git, and push your changes to your remote repository.\n\n ```bash\n git add supabase\n git commit -m \"Initial migration\"\n git push\n ```\n\nUse the Next.js example template to try out branching. This template includes sample migration and seed files to get you started. Run the following command in your terminal to clone the example:\n\n```bash\nnpx create-next-app -e with-supabase\n```\n\nPush your new project to a GitHub repo. For more information, see the GitHub guides on [creating](https://docs.github.com/en/get-started/quickstart/create-a-repo) and [pushing code](https://docs.github.com/en/get-started/using-git/pushing-commits-to-a-remote-repository) to a new repository.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Enable Supabase branching",
          "content": "Once your repository is [correctly prepared](#preparing-your-git-repository), you can enable branching from the Supabase dashboard.\n\nIf your repository doesn't have all the migration files, your production branch could run an incomplete set of migrations. Make sure your [GitHub repository is prepared](#preparing-your-git-repository).\n\n When clicking `Enable branching` you will see the following dialog:\n\n If you don't have the GitHub integration installed, click `Add new project connection`. The integration is required to run migration files and the optional database seed file.\n\n You're taken to the GitHub integration page. Click `Install`.\n\n Follow the instructions to link your Supabase project to its GitHub repository.\n\n Return to your project and re-click `Enable branching`.\n\n Type in the branch you want to use for production. The name of the branch will be validated to make sure it exists in your GitHub repository.\n\n To change your production branch, you need to disable branching and re-enable it with a different branch.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Open a pull request",
          "content": "When you open a pull request on GitHub, the Supabase integration automatically checks for a matching preview branch. If one doesn't exist, it gets created.\n\nA comment is added to your PR with the deployment status of your preview branch. Statuses are shown separately for Database, Services, and APIs.\n\nEvery time a new commit is pushed that changes the migration files in `./supabase/migrations`, the new migrations are run against the preview branch. You can check the status of these runs in the comment's Tasks table.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Preventing migration failures",
          "content": "We highly recommend turning on a 'required check' for the Supabase integration. You can do this from your GitHub repository settings. This prevents PRs from being merged when migration checks fail, and stops invalid migrations from being merged into your production branch.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Manually create a preview branch",
          "content": "Preview branches are automatically created for each pull request, but you can also manually create one.\n\n You need at least one other branch aside from your Supabase production branch.\n\n You can use the GitHub dashboard or command line to create a new branch. In this example, the new branch is called `feat/add-members`.\n\n In the Supabase dashboard, look for the branch dropdown on the right-hand side of the top bar. It should be set to your production branch by default. Open the dropdown and click [`Manage branches`](/dashboard/project/_/branches).\n\n Click `Create preview branch`.\n\n Type in the branch name you want to add. Click `Create branch` to confirm.\n\n Git branches from external contributors currently can't support a Preview Branch\n\nThe Git integration watches for changes in the `supabase` directory. This includes:\n\n- All SQL migration files, under the subdirectory `migrations`\n- An optional `seed.sql` file, used to seed preview instances with sample data\n\nYou can create new migrations either [locally](#develop-locally) or [remotely](#develop-remotely). Local development is recommended.\n\nThe Supabase CLI provides two options: [manual migrations](https://supabase.com/docs/guides/deployment/database-migrations) and [generated migrations](https://supabase.com/docs/guides/deployment/database-migrations#diffing-changes) using Supabase's local studio and the `supabase db-diff` command. Let's use the latter and push the change to our Preview Branch:\n\n Start Supabase locally:\n\n ```bash\n supabase start\n ```\n\n Then proceed to [localhost:54323](http://localhost:54323) to access your local Supabase dashboard.\n\n You can make changes in either the [Table Editor](http://localhost:54323/project/default/editor) or the [SQL Editor]((http://localhost:54323/project/default/sql)).\n\n Once you are finished making database changes, run `supabase db diff` to create a new migration file. For example:\n\n ```bash\n supabase db diff -f \"add_employees_table\"\n ```\n\n This will create a SQL file called `./supabase/migrations/[timestamp]add_employees_table.sql`. This file will reflect the changes that you made in your local dashboard.\n\n If you want to continue making changes, you can manually edit this migration file, then use the `db reset` command to pick up your edits:\n\n ```bash\n supabase db reset\n ```\n\n This will reset the database and run all the migrations again. The local dashboard at [localhost:54323](http://localhost:54323) will reflect the new changes you made.\n\n Commit and push your migration file to your remote GitHub repository. For example:\n\n ```bash\n git add supabase/migrations\n git commit -m \"Add employees table\"\n git push --set-upstream origin new-employee\n ```\n\n The Supabase integration detects the new migration and runs it on the remote Preview Branch. It can take up to 10 minutes for migrations to be applied. If you have a PR for your branch, errors are reflected in the GitHub check run status and in a PR comment.\n\n If you need to reset your database to a clean state (that is, discard any changes that aren't reflected in the migration files), run `supabase db reset` locally. Then, delete the preview branch and recreate it by closing, and reopening your pull request.\n\nAs an alternative to developing locally, you can make changes on your remote Supabase dashboard. You can then pull these changes to your local machine and commit them to GitHub.\n\nDashboard changes aren't automatically reflected in your Git repository. If you'd like to see automatic syncing in a future release, [join the discussion](https://github.com/orgs/supabase/discussions/18937).\n\n Select the branch you wish to use in your Supabase [project](https://supabase.com/dashboard/project/_).\n\n Make changes to your schema with either the [Table Editor](https://supabase.com/dashboard/project/_/editor) or the [SQL Editor]((https://supabase.com/dashboard/project/_/sql)).\n\n If you don't know the password, you must reset the database password so you know the password. Go to the [database setting page](https://supabase.com/dashboard/project/_/settings/database) and click `Reset database password`.\n\n Save the new password securely for future use.\n\n In your local terminal, navigate to the directory for your Supabase project and use the Supabase CLI to pull changes from your branch to your local migrations directory.\n\n Make sure to use the database URL for your branch:\n\n ```bash\n supabase db pull --db-url \"postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres\"\n ```\n\n No new migrations will be run on your remote Preview Branch after pushing your changes. This is expected, because your database is already up-to-date, based on the changes you made in the dashboard. But this ensures that your migration files are in GitHub, so they can be correctly merged into production.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Disable branching",
          "content": "You can disable branching at any time. Navigate to the [Branches](/dashboard/project/_/branches) page, which can be found via the Branches dropdown menu on the top navigation, then click \"Manage Branches\" in the menu. Click the 'Disable branching' button at the top of the Overview section.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Persistent branches",
          "content": "Persistent branches are the type of branches that will remain active even after the underlying PR is closed. Any PR based on a persistent branch will also have a corresponding preview branch created automatically.\n\nYou can change any branch to be persistent on the [Branches](/dashboard/project/_/branches) page by clicking the triple dots icon next to the branch you want to modify, and selecting \"Switch to persistent\".\n\nAll persistent branches can be toggled back to be an ephemeral branch in the exact same way.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Migration and seeding behavior",
          "content": "Migrations are run in sequential order. Each migration builds upon the previous one.\n\nThe preview branch has a record of which migrations have been applied, and only applies new migrations for each commit. This can create an issue when rolling back migrations.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using ORM or custom seed scripts",
          "content": "If you want to use your own ORM for managing migrations and seed scripts, you will need to run them in GitHub Actions after the preview branch is ready. The branch credentials can be fetched using the following example GHA workflow.\n\n```yaml\non:\n pull_request:\n types:\n - opened\n - reopened\n - synchronize\n branches:\n - main\n paths:\n - 'supabase/**'\n\njobs:\n wait:\n runs-on: ubuntu-latest\n outputs:\n status: ${{ steps.check.outputs.conclusion }}\n steps:\n - uses: fountainhead/action-wait-for-check@v1.2.0\n id: check\n with:\n checkName: Supabase Preview\n ref: ${{ github.event.pull_request.head.sha }}\n token: ${{ secrets.GITHUB_TOKEN }}\n\n migrate:\n needs:\n - wait\n if: ${{ needs.wait.outputs.status == 'success' }}\n runs-on: ubuntu-latest\n steps:\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n - run: supabase --experimental branches get \"$GITHUB_HEAD_REF\" -o env >> $GITHUB_ENV\n - name: Custom ORM migration\n run: psql \"$POSTGRES_URL_NON_POOLING\" -c 'select 1'\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Branch configuration with remotes",
          "content": "When Branching is enabled, your `config.toml` settings automatically sync to all ephemeral branches through a one-to-one mapping between your Git and Supabase branches.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Basic configuration",
          "content": "To update configuration for a Supabase branch, modify `config.toml` and push to git. The Supabase integration will detect the changes and apply them to the corresponding branch.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Remote-specific configuration",
          "content": "For persistent branches that need specific settings, you can use the `[remotes]` block in your `config.toml`. Each remote configuration must reference an existing project ID.\n\nHere's an example of configuring a separate seed script for a staging environment:\n\n```toml\n[remotes.staging]\nproject_id = \"your-project-ref\"\n\n[remotes.staging.db.seed]\nsql_paths = [\"./seeds/staging.sql\"]\n```\n\nSince the `project_id` field must reference an existing branch, you need to create the persistent branch before adding its configuration. Use the CLI to create a persistent branch first:\n\n```bash\nsupabase --experimental branches create --persistent",
          "level": 3
        },
        {
          "type": "section",
          "title": "Do you want to create a branch named develop? [Y/n]",
          "content": "```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Configuration merging",
          "content": "When merging a PR into a persistent branch, the Supabase integration:\n\n1. Checks for configuration changes\n2. Logs the changes\n3. Applies them to the target remote\n\nIf no remote is declared or the project ID is incorrect, the configuration step is skipped.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Available configuration options",
          "content": "All standard configuration options are available in the `[remotes]` block. This includes:\n\n- Database settings\n- API configurations\n- Authentication settings\n- Edge Functions configuration\n- And more\n\nYou can use this to maintain different configurations for different environments while keeping them all in version control.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Managing secrets for branches",
          "content": "For sensitive configuration like SMTP credentials or API keys, you can use the Supabase CLI to manage secrets for your branches. This is especially useful for custom SMTP setup or other services that require secure credentials.\n\nTo set secrets for a persistent branch:\n\n```bash",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set secrets from a .env file",
          "content": "supabase secrets set --env-file ./supabase/.env",
          "level": 1
        },
        {
          "type": "section",
          "title": "Or set individual secrets",
          "content": "supabase secrets set SMTP_HOST=smtp.example.com\nsupabase secrets set SMTP_USER=your-username\nsupabase secrets set SMTP_PASSWORD=your-password\n```\n\nThese secrets will be available to your branch's services and can be used in your configuration. For example, in your `config.toml`:\n\n```toml\n[auth.smtp]\nhost = \"env(SMTP_HOST)\"\nuser = \"env(SMTP_USER)\"\npassword = \"env(SMTP_PASSWORD)\"\n```\n\n Secrets set for one branch are not automatically available in other branches. You'll need to set\n them separately for each branch that needs them.",
          "level": 1
        },
        {
          "type": "section",
          "title": "Using dotenvx for git-based workflow",
          "content": "For managing environment variables across different branches, you can use [dotenvx](https://dotenvx.com/) to securely manage your configurations. This approach is particularly useful for teams working with Git branches and preview deployments.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Environment file structure",
          "content": "Following the conventions used in the [example repository](https://github.com/supabase/supabase/blob/master/examples/slack-clone/nextjs-slack-clone-dotenvx/README.md), environments are configured using dotenv files in the `supabase` directory:\n\n| File | Environment | `.gitignore` it? | Encrypted |\n| --------------- | ----------- | ---------------- | --------- |\n| .env.keys | All | Yes | No |\n| .env.local | Local | Yes | No |\n| .env.production | Production | No | Yes |\n| .env.preview | Branches | No | Yes |\n| .env | Any | Maybe | Yes |",
          "level": 5
        },
        {
          "type": "section",
          "title": "Setting up encrypted secrets",
          "content": "1. Generate key pair and encrypt your secrets:\n\n```bash\nnpx @dotenvx/dotenvx set SUPABASE_AUTH_EXTERNAL_GITHUB_SECRET \"\" -f supabase/.env.preview\n```\n\nThis creates a new encryption key in `supabase/.env.preview` and a new decryption key in `supabase/.env.keys`.\n\n2. Update project secrets:\n\n```bash\nnpx supabase secrets set --env-file supabase/.env.keys\n```\n\n3. Choose your configuration approach in `config.toml`:\n\nOption A: Use encrypted values directly:\n\n```toml\n[auth.external.github]\nenabled = true\nsecret = \"encrypted:\"\n```\n\nOption B: Use environment variables:\n\n```toml\n[auth.external.github]\nenabled = true\nclient_id = \"env(SUPABASE_AUTH_EXTERNAL_GITHUB_CLIENT_ID)\"\nsecret = \"env(SUPABASE_AUTH_EXTERNAL_GITHUB_SECRET)\"\n```\n\n The `encrypted:` syntax only works for designated \"secret\" fields in the configuration (like\n `secret` in auth providers). Using encrypted values in other fields will not be automatically\n decrypted and may cause issues. For non-secret fields, use environment variables with the `env()`\n syntax instead.",
          "level": 5
        },
        {
          "type": "section",
          "title": "Using with preview branches",
          "content": "When you commit your `.env.preview` file with encrypted values, the branching executor will automatically retrieve and use these values when deploying your branch. This allows you to maintain different configurations for different branches while keeping sensitive information secure.",
          "level": 5
        },
        {
          "type": "section",
          "title": "Rolling back migrations",
          "content": "You might want to roll back changes you've made in an earlier migration change. For example, you may have pushed a migration file containing schema changes you no longer want.\n\nTo fix this, push the latest changes, then delete the preview branch in Supabase and reopen it.\n\nThe new preview branch is reseeded from the `./supabase/seed.sql` file by default. Any additional data changes made on the old preview branch are lost. This is equivalent to running `supabase db reset` locally. All migrations are rerun in sequential order.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Seeding behavior",
          "content": "Your Preview Branches are seeded with sample data using the same as [local seeding behavior](/docs/guides/local-development/seeding-your-database).\n\nThe database is only seeded once, when the preview branch is created. To rerun seeding, delete the preview branch and recreate it by closing, and reopening your pull request.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Branching and hosting providers",
          "content": "Branching works with hosting providers that support preview deployments.\n\nWith the Supabase branching integration, you can sync the Git branch used by the hosting provider with the corresponding Supabase preview branch. This means that the preview deployment built by your hosting provider is matched to the correct database schema, edge functions, and other Supabase configurations.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Vercel",
          "content": "Install the Vercel integration:\n\n- From the [Vercel marketplace](https://vercel.com/integrations/supabase) or\n- By clicking the blue `Deploy` button in a Supabase example app's `README` file\n\nFor branching to work with Vercel, you also need the [Vercel GitHub integration](https://vercel.com/docs/deployments/git/vercel-for-github).\n\nAnd make sure you have [connected](/dashboard/org/_/integrations) your Supabase project to your Vercel project.\n\nSupabase automatically updates your Vercel project with the correct environment variables for the corresponding preview branches. The synchronization happens at the time of Pull Request being opened, not at the time of branch creation.\n\nAs branching integration is tied to the Preview Deployments feature in Vercel, there are possible race conditions between Supabase setting correct variables, and Vercel running a deployment process. Because of that, Supabase is always automatically re-deploying the most recent deployment of the given pull request.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Other Git providers",
          "content": "There are multiple alternative Git providers under consideration. If you're interested in branching for GitLab, Bitbucket, or some other provider, [join the GitHub discussion](https://github.com/orgs/supabase/discussions/18938).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Alternatives to branching",
          "content": "Under the hood, you can see Supabase branching as a way to programmatically \"duplicate\" your Supabase project via git flow. This allows spawning a new configured (via [`config.toml`](/docs/guides/local-development/cli/config)) and seeded instance of the database and the adjacent Supabase services (buckets, edge functions, etc.).\n\n1. A new project is deployed on behalf of the user on the Supabase side as the new \"branch\" if it doesn't already exist. This includes the database, storage, edge-function, and all Supabase-related services.\n2. The branch is cloned and the new project is configured based on the [`config.toml`](/docs/guides/local-development/cli/config) committed into this project branch.\n3. Migrations are applied and seeding scripts are run (the first time) for this branch.\n\nYou can make a similar setup with a distinct project for each environment. Or just have two environments, the localhost and the production one.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Pricing",
          "content": "Branching is available on the Pro Plan and above. The price is:\n\n- Each Preview branch costs per day\n- Each Preview branch is billed until it is removed",
          "level": 2
        },
        {
          "type": "section",
          "title": "Rolling back migrations",
          "content": "You might want to roll back changes you've made in an earlier migration change. For example, you may have pushed a migration file containing schema changes you no longer want.\n\nTo fix this, push the latest changes, then delete the preview branch in Supabase and reopen it.\n\nThe new preview branch is reseeded from the `./supabase/seed.sql` file by default. Any additional data changes made on the old preview branch are lost. This is equivalent to running `supabase db reset` locally. All migrations are rerun in sequential order.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deployment failures",
          "content": "A deployment might fail for various reasons, including invalid SQL statements and schema conflicts in migrations, errors within the `config.toml` config, or something else.\n\nTo check the error message, see the Supabase workflow run for your branch under the [View logs](/dashboard/project/_/branches) section.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Network restrictions",
          "content": "If you enable [network restrictions](/docs/guides/platform/network-restrictions) on your project, the branching cluster will be blocked from connecting to your project by default. This often results in database connection failures when migrating your production project after merging a development branch.\n\nThe workaround is to explicitly allow the IPv6 CIDR range of the branching cluster in your project's [database settings](https://supabase.com/dashboard/project/_/settings/database) page: `2600:1f18:2b7d:f600::/56`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Schema drift between preview branches",
          "content": "If multiple preview branches exist, each preview branch might contain different schema changes. This is similar to Git branches, where each branch might contain different code changes.\n\nWhen a preview branch is merged into the production branch, it creates a schema drift between the production branch and the preview branches that haven't been merged yet.\n\nThese conflicts can be resolved in the same way as normal Git Conflicts: merge or rebase from the production Git branch to the preview Git branch. Since migrations are applied sequentially, ensure that migration files are timestamped correctly after the rebase. Changes that build on top of earlier changes should always have later timestamps.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Changing production branch",
          "content": "It's not possible to change the Git branch used as the Production branch for Supabase Branching. The only way to change it is to disable and re-enable branching. See [Disable Branching](#disable-branching).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Feedback",
          "content": "Supabase branching is a new and exciting part of the Supabase development ecosystem. Feedback is welcome.\n\nYou can join the [conversation over in GitHub discussions](https://github.com/orgs/supabase/discussions/18937).",
          "level": 2
        }
      ],
      "wordCount": 3560,
      "characterCount": 25692
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-database-migrations",
      "identifier": "deployment-database-migrations",
      "name": "Database Migrations",
      "description": "How to manage schema migrations for your Supabase project.",
      "category": "database",
      "url": "https://supabase.com/docs/guides/deployment/database-migrations",
      "dateModified": "2025-06-13T12:45:11.296301",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/database-migrations.mdx",
      "frontmatter": {
        "id": "database-migrations",
        "title": "Database Migrations",
        "description": "How to manage schema migrations for your Supabase project.",
        "subtitle": "How to manage schema migrations for your Supabase project.",
        "video": "https://www.youtube-nocookie.com/v/Kx5nHBmIxyQ",
        "tocVideo": "Kx5nHBmIxyQ"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Database migrations are SQL statements that create, update, or delete your existing database schemas. They are a common way of tracking changes to your database over time."
        },
        {
          "type": "section",
          "title": "Schema migrations",
          "content": "For this guide, we'll create a table called `employees` and see how we can make changes to it.\n\nYou will need to [install](/docs/guides/local-development#quickstart) the Supabase CLI and start the local development stack.\n\n To get started, generate a [new migration](/docs/reference/cli/supabase-migration-new) to store the SQL needed to create our `employees` table.\n\n```bash name=Terminal\nsupabase migration new create_employees_table\n```\n\n This creates a new migration file in supabase/migrations directory.\n\n To that file, add the SQL to create this `employees` table.\n\n```sql name=supabase/migrations/_create_employees_table.sql\ncreate table if not exists employees (\n id bigint primary key generated always as identity,\n name text not null,\n email text,\n created_at timestamptz default now()\n);\n```\n\n Run this migration to create the `employees` table.\n\n Now you can visit your new `employees` table in the local Dashboard.\n\n```bash name=Terminal\nsupabase migration up\n```\n\n Next, modify your `employees` table by adding a column for `department`.\n\n```bash name=Terminal\nsupabase migration new add_department_column\n```\n\n To that new migration file, add the SQL to create a new `department` column.\n\n```sql name=supabase/migrations/_add_department_column.sql\nalter table if exists public.employees\nadd department text default 'Hooli';\n```\n\n Run this migration to update your existing `employees` table.\n\n```bash name=Terminal\nsupabase migration up\n```\n\nFinally, you should see the `department` column added to your `employees` table in the local Dashboard.\n\nView the [complete code](https://github.com/supabase/supabase/tree/master/examples/database/employees) for this example on GitHub.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Seeding data",
          "content": "Now that you are managing your database with migrations scripts, it would be great have some seed data to use every time you reset the database.\n\n Create a seed script in supabase/seed.sql.\n\n To that file, add the SQL to insert data into your `employees` table.\n\n```sql name=supabase/seed.sql\ninsert into public.employees\n (name)\nvalues\n ('Erlich Bachman'),\n ('Richard Hendricks'),\n ('Monica Hall');\n```\n\n Reset your database to reapply migrations and populate with seed data.\n\n```bash name=Terminal\nsupabase db reset\n```\n\nYou should now see the `employees` table, along with your seed data in the Dashboard! All of your database changes are captured in code, and you can reset to a known state at any time, complete with seed data.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Diffing changes",
          "content": "This workflow is great if you know SQL and are comfortable creating tables and columns. If not, you can still use the Dashboard to create tables and columns, and then use the CLI to diff your changes and create migrations.\n\n Create a new table called `cities`, with columns `id`, `name` and `population`.\n\n Then generate a [schema diff](/docs/reference/cli/supabase-db-diff).\n\n```bash name=Terminal\nsupabase db diff -f create_cities_table\n```\n\n A new migration file is created for you.\n\n Alternately, you can copy the table definitions directly from the Table Editor.\n\n```sql name=supabase/migrations/_create_cities_table.sql\ncreate table \"public\".\"cities\" (\n \"id\" bigint primary key generated always as identity,\n \"name\" text,\n \"population\" bigint\n);\n```\n\n Test your new migration file by resetting your local database.\n\n```bash name=Terminal\nsupabase db reset\n```\n\nThe last step is deploying these changes to a live Supabase project.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deploy your project",
          "content": "You've been developing your project locally, making changes to your tables via migrations. It's time to deploy your project to the Supabase Platform and start scaling up to millions of users!\n\nHead over to [Supabase](https://supabase.com/dashboard) and create a new project to deploy to.\n\n [Login](/docs/reference/cli/supabase-login) to the Supabase CLI using an auto-generated Personal Access Token.\n\n```bash name=Terminal\nsupabase login\n```\n\n [Link](/docs/reference/cli/supabase-link) to your remote project by selecting from the on-screen prompt.\n\n```bash name=Terminal\nsupabase link\n```\n\n [Push](/docs/reference/cli/supabase-db-push) your migrations to the remote database.\n\n```bash name=Terminal\nsupabase db push\n```\n\n [Push](/docs/reference/cli/supabase-db-push) your migrations and seed the remote database.\n\n```bash name=Terminal\nsupabase db push --include-seed\n```\n\nVisiting your live project on [Supabase](https://supabase.com/dashboard/project/_), you'll see a new `employees` table, complete with the `department` column you added in the second migration above.",
          "level": 2
        }
      ],
      "wordCount": 631,
      "characterCount": 4698
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-going-into-prod",
      "identifier": "deployment-going-into-prod",
      "name": "Production Checklist",
      "description": "Things to do before making your app publicly available",
      "category": "deployment",
      "url": "https://supabase.com/docs/guides/deployment/going-into-prod",
      "dateModified": "2025-06-13T12:45:11.296640",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/going-into-prod.mdx",
      "frontmatter": {
        "id": "going-into-prod",
        "title": "Production Checklist",
        "description": "Things to do before making your app publicly available"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "After developing your project and deciding it's Production Ready, you should run through this checklist to ensure that your project:\n\n- is secure\n- won't falter under the expected load\n- remains available whilst in production"
        },
        {
          "type": "section",
          "title": "Security",
          "content": "- Ensure RLS is enabled\n - Tables that do not have RLS enabled with reasonable policies allow any client to access and modify their data. This is unlikely to be what you want in the majority of cases.\n - [Learn more about RLS](/docs/guides/database/postgres/row-level-security).\n- Enable replication on tables containing sensitive data by enabling Row Level Security (RLS) and setting row security policies:\n - Go to the Authentication > Policies page in the Supabase Dashboard to enable RLS and create security policies.\n - Go to the Database > Publications page in the Supabase Dashboard to manage replication tables.\n- Turn on [SSL Enforcement](/docs/guides/platform/ssl-enforcement)\n- Enable [Network Restrictions](/docs/guides/platform/network-restrictions) for your database.\n- Ensure that your Supabase Account is protected with multi-factor authentication (MFA).\n - If using a GitHub signin, [enable 2FA on GitHub](https://docs.github.com/en/authentication/securing-your-account-with-two-factor-authentication-2fa/configuring-two-factor-authentication). Since your GitHub account gives you administrative rights to your Supabase org, you should protect it with a strong password and 2FA using a U2F key or a TOTP app.\n - If using email+password signin, set up [MFA for your Supabase account](https://supabase.com/docs/guides/platform/multi-factor-authentication#enable-mfa).\n- Consider [adding multiple owners on your Supabase org](https://supabase.com/dashboard/org/_/team). This ensures that if one of the owners is unreachable or loses access to their account, you still have Owner access to your org.\n- Ensure email confirmations are [enabled](https://supabase.com/dashboard/project/_/auth/providers) in the `Settings > Auth` page.\n- Ensure that you've [set the expiry](https://supabase.com/dashboard/project/_/auth/providers) for one-time passwords (OTPs) to a reasonable value that you are comfortable with. We recommend setting this to 3600 seconds (1 hour) or lower.\n- Increase the length of the OTP if you need a higher level of entropy.\n- If your application requires a higher level of security, consider setting up [multi-factor authentication](https://supabase.com/docs/guides/auth/auth-mfa) (MFA) for your users.\n- Use a custom SMTP server for auth emails so that your users can see that the mails are coming from a trusted domain (preferably the same domain that your app is hosted on). Grab SMTP credentials from any major email provider such as SendGrid, AWS SES, etc.\n- Think hard about how _you_ would abuse your service as an attacker, and mitigate.\n- Review these [common cybersecurity threats](https://auth0.com/docs/security/prevent-threats).\n- Check and review issues in your database using [Security Advisor](https://supabase.com/dashboard/project/_/database/security-advisor).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Performance",
          "content": "- Ensure that you have suitable indices to cater to your common query patterns\n - [Learn more about indexes in Postgres](https://www.enterprisedb.com/postgres-tutorials/overview-postgresql-indexes).\n - `pg_stat_statements` can help you [identify hot or slow queries](https://www.virtual-dba.com/blog/postgresql-performance-identifying-hot-and-slow-queries/).\n- Perform load testing (preferably on a staging env)\n - Tools like [k6](https://k6.io/) can simulate traffic from many different users.\n- Upgrade your database if you require more resources. If you need anything beyond what is listed, contact enterprise@supabase.io.\n- If you are expecting a surge in traffic (for a big launch) and are on a Team or Enterprise Plan, [contact support](https://supabase.com/dashboard/support/new) with more details about your launch and we'll help keep an eye on your project.\n- If you expect your database size to be > 4 GB, [enable](https://supabase.com/dashboard/project/_/settings/addons?panel=pitr) the Point in Time Recovery (PITR) add-on. Daily backups can take up resources from your database when the backup is in progress. PITR is more resource efficient, since only the changes to the database are backed up.\n- Check and review issues in your database using [Performance Advisor](https://supabase.com/dashboard/project/_/database/performance-advisor).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Availability",
          "content": "- Use your own SMTP credentials so that you have full control over the deliverability of your transactional auth emails (see Settings > Auth)\n - you can grab SMTP credentials from any major email provider such as SendGrid, AWS SES, etc. You can refer to our [SMTP guide](/docs/guides/auth/auth-smtp) for more details.\n - The default rate limit for auth emails when using a custom SMTP provider is 30 new users per hour, if doing a major public announcement you will likely require more than this.\n- Applications on the Free Plan that exhibit extremely low activity in a 7 day period may be paused by Supabase to save on server resources.\n - You can restore paused projects from the Supabase dashboard.\n - Upgrade to Pro to guarantee that your project will not be paused for inactivity.\n- Database backups are not available for download on the Free Plan.\n - You can set up your own backup systems using tools like [pg_dump](https://www.postgresqltutorial.com/postgresql-backup-database/) or [wal-g](https://github.com/wal-g/wal-g).\n - Nightly backups for Pro Plan projects are available on the Supabase dashboard for up to 7 days.\n - Point in Time Recovery (PITR) allows a project to be backed up at much shorter intervals. This provides users an option to restore to any chosen point of up to seconds in granularity. In terms of Recovery Point Objective (RPO), Daily Backups would be suitable for projects willing to lose up to 24 hours worth of data. If a lower RPO is required, enable PITR.\n- Supabase Projects use disks that offer 99.8-99.9% durability by default.\n - Use Read Replicas if you require availability resilience to a disk failure event\n - Use PITR if you require durability resilience to a disk failure event\n- Upgrading to the Supabase Pro Plan will give you [access to our support team](https://supabase.com/dashboard/support/new).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Rate limiting, resource allocation, & abuse prevention",
          "content": "- Supabase employs a number of safeguards against bursts of incoming traffic to prevent abuse and help maximize stability across the platform\n - If you're on a Team or Enterprise Plan and expect high load events, such as production launches, heavy load testing, or prolonged high resource usage, open a ticket via the [support form](https://supabase.help) for help. Provide at least 2 weeks notice.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Auth rate limits",
          "content": "- The table below shows the rate limit quotas on the following authentication endpoints. You can configure the auth rate limits for your project [here](/dashboard/project/_/auth/rate-limits).\n\n| Endpoint | Path | Limited By | Rate Limit |\n| ------------------------------------------------ | -------------------------------------------------------------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| All endpoints that send emails | `/auth/v1/signup` `/auth/v1/recover` `/auth/v1/user`[^1] | Sum of combined requests | As of 3 Sep 2024, this has been updated to auth.rate_limits.email.inbuilt_smtp_per_hour.value emails per hour. You can only change this with your own [custom SMTP setup](/docs/guides/auth/auth-smtp). |\n| All endpoints that send One-Time-Passwords (OTP) | `/auth/v1/otp` | Sum of combined requests | Defaults to 360 OTPs per hour. Is customizable. |\n| Send OTPs or magic links | `/auth/v1/otp` | Last request | Defaults to 60 seconds window before a new request is allowed. Is customizable. |\n| Signup confirmation request | `/auth/v1/signup` | Last request | Defaults to 60 seconds window before a new request is allowed. Is customizable. |\n| Password Reset Request | `/auth/v1/recover` | Last request | Defaults to 60 seconds window before a new request is allowed. Is customizable. |\n| Verification requests | `/auth/v1/verify` | IP Address | 360 requests per hour (with bursts up to 30 requests) |\n| Token refresh requests | `/auth/v1/token` | IP Address | 1800 requests per hour (with bursts up to 30 requests) |\n| Create or Verify an MFA challenge | `/auth/v1/factors/:id/challenge` `/auth/v1/factors/:id/verify` | IP Address | 15 requests per minute (with bursts up to 30 requests) |\n| Anonymous sign-ins | `/auth/v1/signup`[^2] | IP Address | 30 requests per hour (with bursts up to 30 requests) |",
          "level": 3
        },
        {
          "type": "section",
          "title": "Realtime quotas",
          "content": "- Review the [Realtime quotas](/docs/guides/realtime/quotas).\n- If you need quotas increased you can always [contact support](https://supabase.com/dashboard/support/new).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Abuse prevention",
          "content": "- Supabase provides CAPTCHA protection on the signup, sign-in and password reset endpoints. Refer to [our guide](/docs/guides/auth/auth-captcha) on how to protect against abuse using this method.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Email link validity",
          "content": "- When working with enterprise systems, email scanners may scan and make a `GET` request to the reset password link or sign up link in your email. Since links in Supabase Auth are single use, a user who opens an email post-scan to click on a link will receive an error. To get around this problem,\n consider altering the email template to replace the original magic link with a link to a domain you control. The domain can present the user with a \"Sign-in\" button which redirect the user to the original magic link URL when clicked.\n\n- When using a custom SMTP service, some services might have link tracking enabled which may overwrite or disform the email confirmation links sent by Supabase Auth. To prevent this from happening, we recommend that you disable link tracking when using a custom SMTP service.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Subscribe to Supabase status page",
          "content": "Stay informed about Supabase service status by subscribing to the [Status Page](https://status.supabase.com/). We recommend setting up Slack notifications through an RSS feed to ensure your team receives timely updates about service status changes.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Setting up Slack notifications",
          "content": "1. Install the RSS app in Slack:\n\n - Visit the [RSS app page](https://slack.com/marketplace/A0F81R7U7-rss) in the Slack marketplace\n - Click `Add to Slack` if not already installed\n - Otherwise you will get straight to next step, no need to reinstall the app\n\n2. Configure the Supabase status feed:\n\n - Create a channel (e.g., `#supabase-status-alerts`) for status updates\n - On the [RSS app page](https://slack.com/marketplace/A0F81R7U7-rss) go to _Add a Feed_ section and set Feed URL to `https://status.supabase.com/history.rss`\n - Select your designated channel and click \"Subscribe to this feed\"\n\nOnce configured, your team will receive automatic notifications in Slack whenever the Supabase Status Page is updated.\n\nFor detailed setup instructions, see the [Add RSS feeds to Slack](https://slack.com/intl/en-nz/help/articles/218688467-Add-RSS-feeds-to-Slack).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Next steps",
          "content": "This checklist is always growing so be sure to check back frequently, and also feel free to suggest additions and amendments by making a PR on [GitHub](https://github.com/supabase/supabase).",
          "level": 2
        }
      ],
      "wordCount": 1600,
      "characterCount": 11466
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-managing-environments",
      "identifier": "deployment-managing-environments",
      "name": "Managing Environments",
      "description": "Manage multiple environments using Database Migrations and GitHub Actions.",
      "category": "deployment",
      "url": "https://supabase.com/docs/guides/deployment/managing-environments",
      "dateModified": "2025-06-13T12:45:11.297109",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/managing-environments.mdx",
      "frontmatter": {
        "id": "managing-environments",
        "title": "Managing Environments",
        "description": "Manage multiple environments using Database Migrations and GitHub Actions.",
        "subtitle": "Manage multiple environments using Database Migrations and GitHub Actions.",
        "video": "https://www.youtube-nocookie.com/v/rOLyOsBR1Uc",
        "tocVideo": "rOLyOsBR1Uc"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "This guide shows you how to set up your local Supabase development environment that integrates with GitHub Actions to automatically test and release schema changes to staging and production Supabase projects."
        },
        {
          "type": "section",
          "title": "Set up a local environment",
          "content": "The first step is to set up your local repository with the Supabase CLI:\n\n```bash\nsupabase init\n```\n\nYou should see a new `supabase` directory. Then you need to link your local repository with your Supabase project:\n\n```bash\nsupabase login\nsupabase link --project-ref $PROJECT_ID\n```\n\nYou can get your `$PROJECT_ID` from your project's dashboard URL:\n\n```\nhttps://supabase.com/dashboard/project/\n```\n\nIf you're using an existing Supabase project, you might have made schema changes through the Dashboard.\nRun the following command to pull these changes before making local schema changes from the CLI:\n\n```sql\nsupabase db pull\n```\n\nThis command creates a new migration in `supabase/migrations/_remote_schema.sql` which reflects the schema changes you have made previously.\n\nNow commit your local changes to Git and run the local development setup:\n\n```bash\ngit add .\ngit commit -m \"init supabase\"\nsupabase start\n```\n\nYou are now ready to develop schema changes locally and create your first migration.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create a new migration",
          "content": "There are two ways to make schema changes:\n\n1. Manual migration: Write DDL statements manually into a migration file\n1. Auto schema diff: Make changes through Studio UI and auto generate a schema diff",
          "level": 2
        },
        {
          "type": "section",
          "title": "Manual migration",
          "content": "Create a new migration script by running:\n\n```bash\nsupabase migration new new_employee\n```\n\nYou should see a new file created: `supabase/migrations/_new_employee.sql`. You can then write SQL statements in this script using a text editor:\n\n```sql\ncreate table public.employees (\n id integer primary key generated always as identity,\n name text\n);\n```\n\nApply the new migration to your local database:\n\n```bash\nsupabase db reset\n```\n\nThis command recreates your local database from scratch and applies all migration scripts under `supabase/migrations` directory. Now your local database is up to date.\n\nThe new migration command also supports stdin as input. This allows you to pipe in an existing script from another file or stdout:\n\n`supabase migration new new_employee _new_employee.sql` is created. Open the file and verify that the generated DDL statements are the same as below.\n\n```sql\n-- This script was generated by the Schema Diff utility in pgAdmin 4\n-- For the circular dependencies, the order in which Schema Diff writes the objects is not very sophisticated\n-- and may require manual changes to the script to ensure changes are applied in the correct order.\n-- Please report an issue for any failure with the reproduction steps.\n\nCREATE TABLE IF NOT EXISTS public.employees\n(\n id integer NOT NULL GENERATED ALWAYS AS IDENTITY ( INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 2147483647 CACHE 1 ),\n name text COLLATE pg_catalog.\"default\",\n CONSTRAINT employees_pkey PRIMARY KEY (id)\n)\n\nTABLESPACE pg_default;\n\nALTER TABLE IF EXISTS public.employees\n OWNER to postgres;\n\nGRANT ALL ON TABLE public.employees TO anon;\n\nGRANT ALL ON TABLE public.employees TO authenticated;\n\nGRANT ALL ON TABLE public.employees TO postgres;\n\nGRANT ALL ON TABLE public.employees TO service_role;\n```\n\nYou may notice that the auto-generated migration script is more verbose than the manually written one.\nThis is because the default schema diff tool does not account for default privileges added by the initial schema.\n\nCommit the new migration script to git and you are ready to deploy.\n\nAlternatively, you may pass in the `--use-migra` experimental flag to generate a more concise migration using [`migra`](https://github.com/djrobstep/migra).\n\nWithout the `-f` file flag, the output is written to stdout by default.\n\n`supabase db diff --use-migra`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deploy a migration",
          "content": "In a production environment, we recommend using a CI/CD pipeline to deploy new migrations with GitHub Actions rather than deploying from your local machine.\n\nThis example uses two Supabase projects, one for production and one for staging.\n\nPrepare your environments by:\n\n- Creating separate Supabase projects for staging and production\n- Pushing your git repository to GitHub and enabling GitHub Actions\n\nYou need a _new_ project for staging. A project which has already been modified to reflect the production project's schema can't be used because the CLI would reapply these changes.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Configure GitHub Actions",
          "content": "The Supabase CLI requires a few environment variables to run in non-interactive mode.\n\n- `SUPABASE_ACCESS_TOKEN` is your personal access token\n- `SUPABASE_DB_PASSWORD` is your project specific database password\n- `SUPABASE_PROJECT_ID` is your project specific reference string\n\nWe recommend adding these as [encrypted secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets) to your GitHub Actions runners.\n\nCreate the following files inside the `.github/workflows` directory:\n\n```yaml .github/workflows/ci.yml\nname: CI\n\non:\n pull_request:\n workflow_dispatch:\n\njobs:\n test:\n runs-on: ubuntu-latest\n\n steps:\n - uses: actions/checkout@v4\n\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n\n - name: Start Supabase local development setup\n run: supabase db start\n\n - name: Verify generated types are checked in\n run: |\n supabase gen types typescript --local > types.gen.ts\n if ! git diff --ignore-space-at-eol --exit-code --quiet types.gen.ts; then\n echo \"Detected uncommitted changes after build. See status below:\"\n git diff\n exit 1\n fi\n```\n\n```yaml .github/workflows/staging.yml\nname: Deploy Migrations to Staging\n\non:\n push:\n branches:\n - develop\n workflow_dispatch:\n\njobs:\n deploy:\n runs-on: ubuntu-latest\n\n env:\n SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}\n SUPABASE_DB_PASSWORD: ${{ secrets.STAGING_DB_PASSWORD }}\n SUPABASE_PROJECT_ID: ${{ secrets.STAGING_PROJECT_ID }}\n\n steps:\n - uses: actions/checkout@v4\n\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n\n - run: supabase link --project-ref $SUPABASE_PROJECT_ID\n - run: supabase db push\n```\n\n```yaml .github/workflows/production.yml\nname: Deploy Migrations to Production\n\non:\n push:\n branches:\n - main\n workflow_dispatch:\n\njobs:\n deploy:\n runs-on: ubuntu-latest\n\n env:\n SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}\n SUPABASE_DB_PASSWORD: ${{ secrets.PRODUCTION_DB_PASSWORD }}\n SUPABASE_PROJECT_ID: ${{ secrets.PRODUCTION_PROJECT_ID }}\n\n steps:\n - uses: actions/checkout@v4\n\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n\n - run: supabase link --project-ref $SUPABASE_PROJECT_ID\n - run: supabase db push\n```\n\nThe full example code is available in the [demo repository](https://github.com/supabase/supabase-action-example).\n\nCommit these files to git and push to your `main` branch on GitHub. Update these environment variables to match your Supabase projects:\n\n- `SUPABASE_ACCESS_TOKEN`\n- `PRODUCTION_PROJECT_ID`\n- `PRODUCTION_DB_PASSWORD`\n- `STAGING_PROJECT_ID`\n- `STAGING_DB_PASSWORD`\n\nWhen configured correctly, your repository will have CI and Release workflows that trigger on new commits pushed to `main` and `develop` branches.\n\n![Correctly configured repo](/docs/img/guides/cli/ci-main.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Open a PR with new migration",
          "content": "Follow the [migration steps](#create-a-new-migration) to create a `supabase/migrations/_new_employee.sql` file.\n\nCheckout a new branch `feat/employee` from `develop` , commit the migration file, and push to GitHub.\n\n```bash\ngit checkout -b feat/employee\ngit add supabase/migrations/_new_employee.sql\ngit commit -m \"Add employee table\"\ngit push --set-upstream origin feat/employee\n```\n\nOpen a PR from `feat/employee` to the `develop` branch to see that the CI workflow has been triggered.\n\nOnce the test error is resolved, merge this PR and watch the deployment in action.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Release to production",
          "content": "After verifying your staging project has successfully migrated, create another PR from `develop` to `main` and merge it to deploy the migration to the production project.\n\nThe `release` job applies all new migration scripts merged in `supabase/migrations` directory to a linked Supabase project. You can control which project the job links to via `PROJECT_ID` environment variable.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Sync production project to staging",
          "content": "When setting up a new staging project, you might need to sync the initial schema with migrations previously applied to the production project.\n\nOne way is to leverage the Release workflow:\n\n- Create a new branch `develop` and choose `main` as the branch source\n- Push the `develop` branch to GitHub\n\nThe GitHub Actions runner will deploy your existing migrations to the staging project.\n\nAlternatively, you can also apply migrations through your local CLI to a linked remote database.\n\n```sql\nsupabase db push\n```\n\nOnce pushed, check that the migration version is up to date for both local and remote databases.\n\n```sql\nsupabase migration list\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Permission denied on `db pull`",
          "content": "If you have been using Supabase hosted projects for a long time, you might encounter the following permission error when executing `db pull`.\n\n```bash\nError: Error running pg_dump on remote database: pg_dump: error: query failed: ERROR: permission denied for table _type\n\npg_dump: error: query was: LOCK TABLE \"graphql\".\"_type\" IN ACCESS SHARE MODE\n```\n\nTo resolve this error, you need to grant `postgres` role permissions to `graphql` schema. You can do that by running the following query from Supabase dashboard's SQL Editor.\n\n```sql\ngrant all on all tables in schema graphql to postgres, anon, authenticated, service_role;\ngrant all on all functions in schema graphql to postgres, anon, authenticated, service_role;\ngrant all on all sequences in schema graphql to postgres, anon, authenticated, service_role;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Permission denied on `db push`",
          "content": "If you created a table through Supabase dashboard, and your new migration script contains `ALTER TABLE` statements, you might run into permission error when applying them on staging or production databases.\n\n```bash\nERROR: must be owner of table employees (SQLSTATE 42501); while executing migration \n```\n\nThis is because tables created through Supabase dashboard are owned by `supabase_admin` role while the migration scripts executed through CLI are under `postgres` role.\n\nOne way to solve this is to reassign the owner of those tables to `postgres` role. For example, if your table is named `users` in the public schema, you can run the following command to reassign owner.\n\n```sql\nALTER TABLE users OWNER TO postgres;\n```\n\nApart from tables, you also need to reassign owner of other entities using their respective commands, including [types](https://www.postgresql.org/docs/current/sql-altertype.html), [functions](https://www.postgresql.org/docs/current/sql-alterroutine.html), and [schemas](https://www.postgresql.org/docs/current/sql-alterschema.html).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Rebasing new migrations",
          "content": "Sometimes your teammate may merge a new migration file to git main branch, and now you need to rebase your local schema changes on top.\n\nWe can handle this scenario gracefully by renaming your old migration file with a new timestamp.\n\n```bash\ngit pull\nsupabase migration new dev_A",
          "level": 3
        },
        {
          "type": "section",
          "title": "Assume the new file is: supabase/migrations/_dev_A.sql",
          "content": "mv _dev_A.sql _dev_A.sql\nsupabase db reset\n```\n\nIn case [`reset`](/docs/reference/cli/usage#supabase-db-reset) fails, you can manually resolve conflicts by editing `_dev_A.sql` file.\n\nOnce validated locally, commit your changes to Git and push to GitHub.",
          "level": 1
        }
      ],
      "wordCount": 1601,
      "characterCount": 11498
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-maturity-model",
      "identifier": "deployment-maturity-model",
      "name": "Maturity Model",
      "description": "Growing with your application.",
      "category": "deployment",
      "url": "https://supabase.com/docs/guides/deployment/maturity-model",
      "dateModified": "2025-06-13T12:45:11.297278",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/maturity-model.mdx",
      "frontmatter": {
        "id": "maturity-model",
        "title": "Maturity Model",
        "description": "Growing with your application."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase is great for building something very fast _and_ for scaling up. However, it's important to note that as your application matures and your team expands, the practices you use for managing an application in production should not be the same as the practices you used for prototyping."
        },
        {
          "type": "section",
          "title": "Prototyping",
          "content": "The Dashboard is a quick and easy tool for building applications while you are prototyping. That said, we strongly recommend using [Migrations](/docs/guides/deployment/database-migrations) to manage your database changes. You can use our CLI to [capture any changes](/docs/reference/cli/supabase-db-diff) you have made on the Dashboard so that you can commit them a version control system, like git.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Collaborating",
          "content": "As soon as you start collaborating with team members, all project changes should be in version control. At this point we strongly recommend moving away from using the Dashboard for schema changes. Use migrations to manage your database, and check them into your version control system to track every change.\n\nResources:\n\n- [Database migrations](/docs/guides/deployment/database-migrations)\n- [Managing access on the Dashboard](/docs/guides/platform/access-control)\n- [PGAudit for Postgres](/docs/guides/database/extensions/pgaudit)",
          "level": 2
        },
        {
          "type": "section",
          "title": "In production",
          "content": "Once your application is live, you should never change your database using the Dashboard - everything should be done with [Migrations](/docs/guides/cli/managing-environments#create-a-new-migration). Some other important things to consider at this point include:\n\n- The Dashboard has various [access levels](/docs/guides/platform/access-control) that can prevent changes being made via the UI.\n- Design a [safe workflow](/docs/guides/platform/shared-responsibility-model#you-decide-your-own-workflow) for managing your database. We strongly recommend running [multiple environments](/docs/guides/cli/managing-environments) as part of your development workflow (`local` -> `staging` -> `prod`).\n- Do not share any production passwords with your team, _especially_ your `postgres` password. All changes should be made via version-controlled migrations which run via a bastion host or a CI platform (like [GitHub Actions](/docs/guides/cli/managing-environments#configure-github-actions). If you use GitHub Actions, use [approval workflows](https://docs.github.com/en/actions/managing-workflow-runs/reviewing-deployments) to prevent any migrations being run accidentally.\n- Restrict production access to your database using [Network Restrictions](/docs/guides/platform/network-restrictions).\n- As your database to grows, we strongly recommend moving to [Point-in-Time Recovery](/docs/guides/platform/backups#point-in-time-recovery). This is safer and has less impact on your database performance during maintenance windows.\n- Read the [Production Checklist](/docs/guides/platform/going-into-prod) and familiarize your team with the [Shared Responsibilities](/docs/guides/platform/shared-responsibility-model) between your organization and Supabase.\n\nResources:\n\n- [Database migrations](/docs/guides/deployment/database-migrations)\n- [Managing access on the Dashboard](/docs/guides/platform/access-control)\n- [PGAudit for Postgres](/docs/guides/database/extensions/pgaudit)\n- [Managing environments](/docs/guides/cli/managing-environments)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enterprise",
          "content": "For a more secure setup, consider running your workload across several organizations. It's a common pattern to have a Production organization which is restricted to only those team members who are qualified to have direct access to production databases.\n\nReach out to [growth](https://forms.supabase.com/enterprise) if you need help designing a secure development workflow for your organization.",
          "level": 2
        }
      ],
      "wordCount": 425,
      "characterCount": 3723
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:deployment-shared-responsibility-model",
      "identifier": "deployment-shared-responsibility-model",
      "name": "Shared Responsibility Model",
      "description": "Running databases is a shared responsibility between you and Supabase.",
      "category": "deployment",
      "url": "https://supabase.com/docs/guides/deployment/shared-responsibility-model",
      "dateModified": "2025-06-13T12:45:11.297520",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/deployment/shared-responsibility-model.mdx",
      "frontmatter": {
        "id": "shared-responsibility-model",
        "title": "Shared Responsibility Model",
        "description": "Running databases is a shared responsibility between you and Supabase."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Running databases is a shared responsibility between you and Supabase. There are some things that we can take care of for you, and some things that you are responsible for. This is by design: we want to give you the freedom to use your database however you want. While we _could_ put many more restrictions in place to ensure that you can’t do anything wrong, you will eventually find those restrictions prohibitive.\n\nTo summarize, you are always responsible for:\n\n- Your Supabase account\n- Access management (Supabase account, database, tables, etc)\n- Data\n- Applying security controls\n\nGenerally, we aim to reduce your burden of managing infrastructure and knowing about Postgres internals, minimizing configuration as much as we can. Here are a few things that you should know:"
        },
        {
          "type": "section",
          "title": "You share the security responsibility",
          "content": "We give you full access to the database. If you share that access with other people (either people on your team, or the public in general) then it is your responsibility to ensure that the access levels you provide are correctly managed.\n\nIf you have an inexperienced member on your team, then you probably shouldn’t give them access to Production. You should set internal workflows around what they should and should not be able to do, with restricted access to avoid anything that might be deemed dangerous.\n\nYou are also responsible for ensuring that tables with sensitive data have the right level of access. You are also responsible for managing your database secrets and API keys, storing them safely in an encrypted store.\n\nSupabase provides controls for [securing your data](/docs/guides/database/secure-data), and it is recommended that you always apply [Row Level Security](/docs/guides/database/postgres/row-level-security) (RLS).\n\nWe will also provide you with security alerts through [Security Advisor](https://supabase.com/dashboard/project/_/database/security-advisor) and applying the recommendations are your responsibility.",
          "level": 2
        },
        {
          "type": "section",
          "title": "You decide your own workflow",
          "content": "There are _many_ ways to work with Supabase.\n\nYou can use our Dashboard, our client libraries, external tools like Prisma and Drizzle, or migration tools like our CLI, Flyway, Sqitch, and anything else that is Postgres-compatible. You can develop directly on your database while you're getting started, run migrations from [local to production](/docs/guides/getting-started/local-development), or you can use [multiple environments](/docs/guides/cli/managing-environments).\n\nNone of these are right or wrong. It depends on the stage of your project. You _definitely_ shouldn’t be developing on your database directly when you’re in production - but that’s absolutely fine when you’re prototyping and don’t have users.",
          "level": 2
        },
        {
          "type": "section",
          "title": "You are responsible for your application architecture",
          "content": "Supabase isn't a silver-bullet for bad architectural decisions. A poorly designed database will run poorly, no matter where it’s hosted.\n\nYou can get away with a poorly-designed database for a while by adding compute. After a while, things will start to break. The database schema is the area you want to spend _the most_ time thinking about. That’s the benefit of Supabase - you can spend more time designing a scalable database system and less time thinking about the mundane tasks like implementing CRUD APIs.\n\nIf you don’t want to implement logic inside your database, that is 100% fine. You can use _any_ tools which work with Postgres.",
          "level": 2
        },
        {
          "type": "section",
          "title": "You are responsible for third-party services",
          "content": "Supabase offers a lot of opportunities for flexibly integrating with third-party services, such as:\n\n- OAuth and SAML login providers\n- SMTP and SMS sending APIs\n- Calls to external APIs within Postgres functions or triggers\n- Calls to external APIs within Edge Functions\n\nYou are free to use and integrate with any service, but you're also responsible for ensuring that the performance, availability, and security of the services you use match up with your application's requirements. We do not monitor for outages or performance issues within integrations with third-party services. Depending on the implementation, an issue with such an integration could also result in performance degradation or an outage for your Supabase project.\n\nIf your application architecture relies on such integrations, you should monitor the relevant logs and metrics to ensure optimal performance.",
          "level": 2
        },
        {
          "type": "section",
          "title": "You choose your level of comfort with Postgres",
          "content": "Our goal at Supabase is to make _all_ of Postgres easy to use. That doesn’t mean you have to use all of it. If you’re a Postgres veteran, you’ll probably love the tools that we offer. If you’ve never used Postgres before, then start smaller and grow into it. If you just want to treat Postgres like a simple table-store, that’s perfectly fine.",
          "level": 2
        },
        {
          "type": "section",
          "title": "You are in control of your database",
          "content": "Supabase places very few guard-rails around your database. That gives you a lot of control, but it also means you can break things. ”Break” is used liberally here. It refers to any situation that affects your application because of the way you're using the database.\n\nYou are responsible for using best-practices to optimize and manage your database: adding indexes, adding filters on large queries, using caching strategies, optimizing your database queries, and managing connections to the database.\n\nYou are responsible of provisioning enough compute to run the workload that your application requires. The Supabase Dashboard provides [observability tooling](https://supabase.com/dashboard/project/_/reports/database) to help with this.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Before going to production",
          "content": "We recommend reviewing and applying the recommendations offered in our [Production Checklist](/docs/guides/platform/going-into-prod). This checklist covers the responsibilities discussed here and a few additional general production readiness best practices.",
          "level": 2
        },
        {
          "type": "section",
          "title": "SOC 2 and compliance",
          "content": "Supabase provides a SOC 2 compliant environment for hosting and managing sensitive data. We recommend reviewing the [SOC 2 compliance responsibilities document](/docs/guides/security/soc-2-compliance) alongside the aforementioned production checklist.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Managing healthcare data",
          "content": "You can use Supabase to store and process Protected Health Information (PHI). You are responsible for the following\n\n- Signing a Business Associate Agreement (BAA) with Supabase. Submit a [HIPAA add-on request](https://forms.supabase.com/hipaa2) to get started. You will need to be at least on the [Team Plan](https://supabase.com/pricing) to sign a BAA with us.\n- [Marking specific projects as HIPAA projects](/docs/guides/platform/hipaa-projects) and addressing security issues raised by the advisor.\n- Ensuring [MFA is enabled](/docs/guides/platform/multi-factor-authentication) on all Supabase accounts.\n- Enabling [Point in Time Recovery](/docs/guides/platform/backups#point-in-time-recovery) which requires at least a [small compute add-on](/docs/guides/platform/compute-add-ons).\n- Turning on [SSL Enforcement](/docs/guides/platform/ssl-enforcement).\n- Enabling [Network Restrictions](/docs/guides/platform/network-restrictions).\n- Disabling data sharing for [Supabase AI editor](https://supabase.com/dashboard/org/_/general) in our dashboard.\n - Specifically, \"_Opt-in to sending anonymous data to OpenAI_\" should be disabled (Opt-out).\n- Complying with encryption requirements in the HIPAA Security Rule. Data is encrypted at rest and in transit by Supabase. You can consider encrypting the data at your application layer.\n- Not using [Edge functions](/docs/guides/functions) to process PHI.\n- Not storing PHI in [public Storage buckets](/docs/guides/storage/buckets/fundamentals#public-buckets).\n- Not [transferring projects](/docs/guides/platform/project-transfer) to a non-HIPAA organization.\n\nFor more information on the shared responsibilities and rules under HIPAA, review the [HIPAA compliance responsibilities document](/docs/guides/security/hipaa-compliance).",
          "level": 2
        }
      ],
      "wordCount": 1101,
      "characterCount": 7901
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-ai-models",
      "identifier": "functions-ai-models",
      "name": "Running AI Models",
      "description": "How to run AI models in Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/ai-models",
      "dateModified": "2025-06-13T12:45:11.297793",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/ai-models.mdx",
      "frontmatter": {
        "id": "function-ai-models",
        "title": "Running AI Models",
        "description": "How to run AI models in Edge Functions.",
        "subtitle": "How to run AI models in Edge Functions.",
        "tocVideo": "w4Rr_1whU-U"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[Supabase Edge Runtime](https://github.com/supabase/edge-runtime) has a built-in API for running AI models. You can use this API to generate embeddings, build conversational workflows, and do other AI related tasks in your Edge Functions."
        },
        {
          "type": "section",
          "title": "Setup",
          "content": "There are no external dependencies or packages to install to enable the API.\n\nYou can create a new inference session by doing:\n\n```ts\nconst model = new Supabase.ai.Session('model-name')\n```\n\nTo get type hints and checks for the API you can import types from `functions-js` at the top of your file:\n\n```ts\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts'\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Running a model inference",
          "content": "Once the session is instantiated, you can call it with inputs to perform inferences. Depending on the model you run, you may need to provide different options (discussed below).\n\n```ts\nconst output = await model.run(input, options)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "How to generate text embeddings",
          "content": "Now let's see how to write an Edge Function using the `Supabase.ai` API to generate text embeddings. Currently, `Supabase.ai` API only supports the [gte-small](https://huggingface.co/Supabase/gte-small) model.\n\n`gte-small` model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens. While you can provide inputs longer than 512 tokens, truncation may affect the accuracy.\n\n```ts\nconst model = new Supabase.ai.Session('gte-small')\n\nDeno.serve(async (req: Request) => {\n const params = new URL(req.url).searchParams\n const input = params.get('input')\n const output = await model.run(input, { mean_pool: true, normalize: true })\n return new Response(JSON.stringify(output), {\n headers: {\n 'Content-Type': 'application/json',\n Connection: 'keep-alive',\n },\n })\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using Large Language Models (LLM)",
          "content": "Inference via larger models is supported via [Ollama](https://ollama.com/) and [Mozilla Llamafile](https://github.com/Mozilla-Ocho/llamafile). In the first iteration, you can use it with a self-managed Ollama or [Llamafile server](https://www.docker.com/blog/a-quick-guide-to-containerizing-llamafile-with-docker-for-ai-applications/). We are progressively rolling out support for the hosted solution. To sign up for early access, fill up [this form](https://forms.supabase.com/supabase.ai-llm-early-access).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Running locally",
          "content": "[Install Ollama](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) and pull the Mistral model\n\n```bash\nollama pull mistral\n```\n\nRun the Ollama server locally\n\n```bash\nollama serve\n```\n\nSet a function secret called AI_INFERENCE_API_HOST to point to the Ollama server\n\n```bash\necho \"AI_INFERENCE_API_HOST=http://host.docker.internal:11434\" >> supabase/functions/.env\n```\n\nCreate a new function with the following code\n\n```bash\nsupabase functions new ollama-test\n```\n\n```ts supabase/functions/ollama-test/index.ts\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts'\nconst session = new Supabase.ai.Session('mistral')\n\nDeno.serve(async (req: Request) => {\n const params = new URL(req.url).searchParams\n const prompt = params.get('prompt') ?? ''\n\n // Get the output as a stream\n const output = await session.run(prompt, { stream: true })\n\n const headers = new Headers({\n 'Content-Type': 'text/event-stream',\n Connection: 'keep-alive',\n })\n\n // Create a stream\n const stream = new ReadableStream({\n async start(controller) {\n const encoder = new TextEncoder()\n\n try {\n for await (const chunk of output) {\n controller.enqueue(encoder.encode(chunk.response ?? ''))\n }\n } catch (err) {\n console.error('Stream error:', err)\n } finally {\n controller.close()\n }\n },\n })\n\n // Return the stream to the user\n return new Response(stream, {\n headers,\n })\n})\n```\n\nServe the function\n\n```bash\nsupabase functions serve --env-file supabase/functions/.env\n```\n\nExecute the function\n\n```bash\ncurl --get \"http://localhost:54321/functions/v1/ollama-test\" \\\n--data-urlencode \"prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj\" \\\n-H \"Authorization: $ANON_KEY\"\n```\n\nFollow the [Llamafile Quickstart](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart) to download an run a Llamafile locally on your machine.\n\nSince Llamafile provides an OpenAI API compatible server, you can either use it with `@supabase/functions-js` or with the official OpenAI Deno SDK.\n\nSet a function secret called `AI_INFERENCE_API_HOST` to point to the Llamafile server\n\n```bash\necho \"AI_INFERENCE_API_HOST=http://host.docker.internal:8080\" >> supabase/functions/.env\n```\n\nCreate a new function with the following code\n\n```bash\nsupabase functions new llamafile-test\n```\n\nNote that the model parameter doesn't have any effect here! The model depends on which Llamafile is currently running!\n\n```ts supabase/functions/llamafile-test/index.ts\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts'\nconst session = new Supabase.ai.Session('LLaMA_CPP')\n\nDeno.serve(async (req: Request) => {\n const params = new URL(req.url).searchParams\n const prompt = params.get('prompt') ?? ''\n\n // Get the output as a stream\n const output = await session.run(\n {\n messages: [\n {\n role: 'system',\n content:\n 'You are LLAMAfile, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.',\n },\n {\n role: 'user',\n content: prompt,\n },\n ],\n },\n {\n mode: 'openaicompatible', // Mode for the inference API host. (default: 'ollama')\n stream: false,\n }\n )\n\n console.log('done')\n return Response.json(output)\n})\n```\n\nSet the following function secrets to point the OpenAI SDK to the Llamafile server:\n\n```bash\necho \"OPENAI_BASE_URL=http://host.docker.internal:8080/v1\" >> supabase/functions/.env\necho \"OPENAI_BASE_URL=OPENAI_API_KEY=sk-XXXXXXXX\" >> supabase/functions/.env\n```\n\nCreate a new function with the following code\n\n```bash\nsupabase functions new llamafile-test\n```\n\nNote that the model parameter doesn't have any effect here! The model depends on which Llamafile is currently running!\n\n```ts supabase/functions/llamafile-test/index.ts\nimport OpenAI from 'https://deno.land/x/openai@v4.53.2/mod.ts'\n\nDeno.serve(async (req) => {\n const client = new OpenAI()\n const { prompt } = await req.json()\n const stream = true\n\n const chatCompletion = await client.chat.completions.create({\n model: 'LLaMA_CPP',\n stream,\n messages: [\n {\n role: 'system',\n content:\n 'You are LLAMAfile, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.',\n },\n {\n role: 'user',\n content: prompt,\n },\n ],\n })\n\n if (stream) {\n const headers = new Headers({\n 'Content-Type': 'text/event-stream',\n Connection: 'keep-alive',\n })\n\n // Create a stream\n const stream = new ReadableStream({\n async start(controller) {\n const encoder = new TextEncoder()\n\n try {\n for await (const part of chatCompletion) {\n controller.enqueue(encoder.encode(part.choices[0]?.delta?.content || ''))\n }\n } catch (err) {\n console.error('Stream error:', err)\n } finally {\n controller.close()\n }\n },\n })\n\n // Return the stream to the user\n return new Response(stream, {\n headers,\n })\n }\n\n return Response.json(chatCompletion)\n})\n```\n\nServe the function\n\n```bash\nsupabase functions serve --env-file supabase/functions/.env\n```\n\nExecute the function\n\n```bash\ncurl --get \"http://localhost:54321/functions/v1/llamafile-test\" \\\n --data-urlencode \"prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj\" \\\n -H \"Authorization: $ANON_KEY\"\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deploying to production",
          "content": "Once the function is working locally, it's time to deploy to production.\n\nDeploy an Ollama or Llamafile server and set a function secret called `AI_INFERENCE_API_HOST` to point to the deployed server\n\n```bash\nsupabase secrets set AI_INFERENCE_API_HOST=https://path-to-your-llm-server/\n```\n\nDeploy the Supabase function\n\n```bash\nsupabase functions deploy\n```\n\nExecute the function\n\n```bash\ncurl --get \"https://project-ref.supabase.co/functions/v1/ollama-test\" \\\n --data-urlencode \"prompt=write a short rap song about Supabase, the Postgres Developer platform, as sung by Nicki Minaj\" \\\n -H \"Authorization: $ANON_KEY\"\n```\n\nAs demonstrated in the video above, running Ollama locally is typically slower than running it in on a server with dedicated GPUs. We are collaborating with the Ollama team to improve local performance.\n\nIn the future, a hosted LLM API, will be provided as part of the Supabase platform. Supabase will scale and manage the API and GPUs for you. To sign up for early access, fill up [this form](https://forms.supabase.com/supabase.ai-llm-early-access).",
          "level": 3
        }
      ],
      "wordCount": 1068,
      "characterCount": 8532
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-auth",
      "identifier": "functions-auth",
      "name": "Integrating With Supabase Auth",
      "description": "Supabase Edge Functions and Auth.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/auth",
      "dateModified": "2025-06-13T12:45:11.297943",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/auth.mdx",
      "frontmatter": {
        "id": "auth",
        "title": "Integrating With Supabase Auth",
        "description": "Supabase Edge Functions and Auth.",
        "subtitle": "Supabase Edge Functions and Auth."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Functions work seamlessly with [Supabase Auth](/docs/guides/auth)."
        },
        {
          "type": "section",
          "title": "Auth context",
          "content": "When a user makes a request to an Edge Function, you can use the Authorization header to set the Auth context in the Supabase client:\n\n```js\nimport { createClient } from 'npm:@supabase/supabase-js@2'\n\nDeno.serve(async (req: Request) => {\n\n const supabaseClient = createClient(\n Deno.env.get('SUPABASE_URL') ?? '',\n Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n // Create client with Auth context of the user that called the function.\n // This way your row-level-security (RLS) policies are applied.\n {\n global: {\n headers: { Authorization: req.headers.get('Authorization')! },\n },\n }\n );\n\n // Get the session or user object\n const authHeader = req.headers.get('Authorization')!;\n const token = authHeader.replace('Bearer ', '');\n const { data } = await supabaseClient.auth.getUser(token);\n\n})\n```\n\nImportantly, this is done _inside_ the `Deno.serve()` callback argument, so that the Authorization header is set for each request.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Fetching the user",
          "content": "By getting the JWT from the `Authorization` header, you can provide the token to `getUser()` to fetch the user object to obtain metadata for the logged in user.\n\n```js\nimport { createClient } from 'npm:@supabase/supabase-js@2'\n\nDeno.serve(async (req: Request) => {\n\n const supabaseClient = createClient(\n Deno.env.get('SUPABASE_URL') ?? '',\n Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n {\n global: {\n headers: { Authorization: req.headers.get('Authorization') },\n },\n }\n )\n\n // Get the session or user object\n const authHeader = req.headers.get('Authorization')!\n const token = authHeader.replace('Bearer ', '')\n const { data } = await supabaseClient.auth.getUser(token)\n const user = data.user\n\n return new Response(JSON.stringify({ user }), {\n headers: { 'Content-Type': 'application/json' },\n status: 200,\n })\n\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Row Level Security",
          "content": "After initializing a Supabase client with the Auth context, all queries will be executed with the context of the user. For database queries, this means [Row Level Security](/docs/guides/database/postgres/row-level-security) will be enforced.\n\n```js\nimport { createClient } from 'npm:@supabase/supabase-js@2'\n\nDeno.serve(async (req: Request) => {\n\n const supabaseClient = createClient(\n Deno.env.get('SUPABASE_URL') ?? '',\n Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n // Create client with Auth context of the user that called the function.\n // This way your row-level-security (RLS) policies are applied.\n {\n global: {\n headers: { Authorization: req.headers.get('Authorization')! },\n },\n }\n );\n\n // Get the session or user object\n const authHeader = req.headers.get('Authorization')!;\n const token = authHeader.replace('Bearer ', '');\n const { data: userData } = await supabaseClient.auth.getUser(token);\n const { data, error } = await supabaseClient.from('profiles').select('*');\n\n return new Response(JSON.stringify({ data }), {\n headers: { 'Content-Type': 'application/json' },\n status: 200,\n })\n\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example code",
          "content": "See a full [example on GitHub](https://github.com/supabase/supabase/blob/master/examples/edge-functions/supabase/functions/select-from-table-with-auth-rls/index.ts).",
          "level": 2
        }
      ],
      "wordCount": 392,
      "characterCount": 3168
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-background-tasks",
      "identifier": "functions-background-tasks",
      "name": "Background Tasks",
      "description": "How to run background tasks in an Edge Function outside of the request handler",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/background-tasks",
      "dateModified": "2025-06-13T12:45:11.298185",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/background-tasks.mdx",
      "frontmatter": {
        "id": "function-background-tasks",
        "title": "Background Tasks",
        "description": "How to run background tasks in an Edge Function outside of the request handler",
        "subtitle": "How to run background tasks in an Edge Function outside of the request handler"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Function instances can process background tasks outside of the request handler. Background tasks are useful for asynchronous operations like uploading a file to Storage, updating a database, or sending events to a logging service. You can respond to the request immediately and leave the task running in the background."
        },
        {
          "type": "section",
          "title": "How it works",
          "content": "You can use `EdgeRuntime.waitUntil(promise)` to explicitly mark background tasks. The Function instance continues to run until the promise provided to `waitUntil` completes.\n\nThe maximum duration is capped based on the wall-clock, CPU, and memory limits. The Function will shutdown when it reaches one of these [limits](/docs/guides/functions/limits).\n\nYou can listen to the `beforeunload` event handler to be notified when Function invocation is about to be shut down.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Example",
          "content": "Here's an example of using `EdgeRuntime.waitUntil` to run a background task and using `beforeunload` event to be notified when the instance is about to be shut down.\n\n```ts\nasync function longRunningTask() {\n // do work here\n}\n\n// Mark the longRunningTask's returned promise as a background task.\n// note: we are not using await because we don't want it to block.\nEdgeRuntime.waitUntil(longRunningTask())\n\n// Use beforeunload event handler to be notified when function is about to shutdown\naddEventListener('beforeunload', (ev) => {\n console.log('Function will be shutdown due to', ev.detail?.reason)\n\n // save state or log the current progress\n})\n\n// Invoke the function using a HTTP request.\n// This will start the background task\nDeno.serve(async (req) => {\n return new Response('ok')\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Starting a background task in the request handler",
          "content": "You can call `EdgeRuntime.waitUntil` in the request handler too. This will not block the request.\n\n```ts\nasync function fetchAndLog(url: string) {\n const response = await fetch(url)\n console.log(response)\n}\n\nDeno.serve(async (req) => {\n // this will not block the request,\n // instead it will run in the background\n EdgeRuntime.waitUntil(fetchAndLog('https://httpbin.org/json'))\n\n return new Response('ok')\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Testing background tasks locally",
          "content": "When testing Edge Functions locally with Supabase CLI, the instances are terminated automatically after a request is completed. This will prevent background tasks from running to completion.\n\nTo prevent that, you can update the `supabase/config.toml` with the following settings:\n\n```toml\n[edge_runtime]\npolicy = \"per_worker\"\n```\n\nWhen running with `per_worker` policy, Function won't auto-reload on edits. You will need to manually restart it by running `supabase functions serve`.",
          "level": 3
        }
      ],
      "wordCount": 376,
      "characterCount": 2614
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-cicd-workflow",
      "identifier": "functions-cicd-workflow",
      "name": "Deploying with CI / CD pipelines",
      "description": "Use GitHub Actions, Bitbucket, and GitLab CI to deploy your Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/cicd-workflow",
      "dateModified": "2025-06-13T12:45:11.298294",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/cicd-workflow.mdx",
      "frontmatter": {
        "id": "cicd-workflow",
        "title": "Deploying with CI / CD pipelines",
        "description": "Use GitHub Actions, Bitbucket, and GitLab CI to deploy your Edge Functions.",
        "subtitle": "Use GitHub Actions, Bitbucket, and GitLab CI to deploy your Edge Functions.",
        "tocVideo": "6OMVWiiycLs"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You can use popular CI / CD tools like GitHub Actions, Bitbucket, and GitLab CI to automate Edge Function deployments."
        },
        {
          "type": "section",
          "title": "GitHub Actions",
          "content": "You can use the official [`setup-cli` GitHub Action](https://github.com/marketplace/actions/supabase-cli-action) to run Supabase CLI commands in your GitHub Actions.\n\nThe following GitHub Action deploys all Edge Functions any time code is merged into the `main` branch:\n\n```yaml\nname: Deploy Function\n\non:\n push:\n branches:\n - main\n workflow_dispatch:\n\njobs:\n deploy:\n runs-on: ubuntu-latest\n\n env:\n SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}\n PROJECT_ID: your-project-id\n\n steps:\n - uses: actions/checkout@v4\n\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n\n - run: supabase functions deploy --project-ref $PROJECT_ID\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "GitLab CI",
          "content": "Here is the sample pipeline configuration to deploy via GitLab CI.\n\n```yaml\nimage: node:20",
          "level": 2
        },
        {
          "type": "section",
          "title": "List of stages for jobs, and their order of execution",
          "content": "stages:\n - setup\n - deploy",
          "level": 1
        },
        {
          "type": "section",
          "title": "This job runs in the setup stage, which runs first.",
          "content": "setup-npm:\n stage: setup\n script:\n - npm i supabase\n cache:\n paths:\n - node_modules/\n artifacts:\n paths:\n - node_modules/",
          "level": 1
        },
        {
          "type": "section",
          "title": "This job runs in the deploy stage, which only starts when the job in the build stage completes successfully.",
          "content": "deploy-function:\n stage: deploy\n script:\n - npx supabase init\n - npx supabase functions deploy --debug\n services:\n - docker:dind\n variables:\n DOCKER_HOST: tcp://docker:2375\n```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Bitbucket Pipelines",
          "content": "Here is the sample pipeline configuration to deploy via Bitbucket.\n\n```yaml\nimage: node:20\n\npipelines:\n default:\n - step:\n name: Setup\n caches:\n - node\n script:\n - npm i supabase\n - parallel:\n - step:\n name: Functions Deploy\n script:\n - npx supabase init\n - npx supabase functions deploy --debug\n services:\n - docker\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Declarative configuration",
          "content": "Individual function configuration like [JWT verification](/docs/guides/cli/config#functions.function_name.verify_jwt) and [import map location](/docs/guides/cli/config#functions.function_name.import_map) can be set via the `config.toml` file.\n\n```toml\n[functions.hello-world]\nverify_jwt = false\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- See the [example on GitHub](https://github.com/supabase/supabase/blob/master/examples/edge-functions/.github/workflows/deploy.yaml).",
          "level": 2
        }
      ],
      "wordCount": 283,
      "characterCount": 2268
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-compression",
      "identifier": "functions-compression",
      "name": "Handling Compressed Requests",
      "description": "Handling Gzip compressed requests.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/compression",
      "dateModified": "2025-06-13T12:45:11.298377",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/compression.mdx",
      "frontmatter": {
        "id": "functions-compression",
        "title": "Handling Compressed Requests",
        "description": "Handling Gzip compressed requests.",
        "subtitle": "Handling Gzip compressed requests."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "To decompress Gzip bodies, you can use `gunzipSync` from the `node:zlib` API to decompress and then read the body.\n\n```ts\nimport { gunzipSync } from 'node:zlib'\n\nDeno.serve(async (req) => {\n try {\n // Check if the request body is gzip compressed\n const contentEncoding = req.headers.get('content-encoding')\n if (contentEncoding !== 'gzip') {\n return new Response('Request body is not gzip compressed', {\n status: 400,\n })\n }\n\n // Read the compressed body\n const compressedBody = await req.arrayBuffer()\n\n // Decompress the body\n const decompressedBody = gunzipSync(new Uint8Array(compressedBody))\n\n // Convert the decompressed body to a string\n const decompressedString = new TextDecoder().decode(decompressedBody)\n const data = JSON.parse(decompressedString)\n\n // Process the decompressed body as needed\n console.log(`Received: ${JSON.stringify(data)}`)\n\n return new Response('ok', {\n headers: { 'Content-Type': 'text/plain' },\n })\n } catch (error) {\n console.error('Error:', error)\n return new Response('Error processing request', { status: 500 })\n }\n})\n```\n\nEdge functions have a runtime memory limit of 150MB. Overly large compressed payloads may result in an out-of-memory error."
        }
      ],
      "wordCount": 155,
      "characterCount": 1184
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-connect-to-postgres",
      "identifier": "functions-connect-to-postgres",
      "name": "Connecting directly to Postgres",
      "description": "Connecting to Postgres from Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/connect-to-postgres",
      "dateModified": "2025-06-13T12:45:11.298525",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/connect-to-postgres.mdx",
      "frontmatter": {
        "id": "examples-postgres-on-the-edge",
        "title": "Connecting directly to Postgres",
        "description": "Connecting to Postgres from Edge Functions.",
        "subtitle": "Connecting to Postgres from Edge Functions.",
        "tocVideo": "cl7EuF1-RsY"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Connect to your Postgres database from an Edge Function by using the `supabase-js` client.\nYou can also use other Postgres clients like [Deno Postgres](https://deno.land/x/postgres)"
        },
        {
          "type": "section",
          "title": "Using supabase-js",
          "content": "The `supabase-js` client is a great option for connecting to your Supabase database since it handles authorization with Row Level Security, and it automatically formats your response as JSON.\n\n```ts index.ts\nimport { createClient } from 'npm:@supabase/supabase-js@2'\n\nDeno.serve(async (req) => {\n try {\n const supabase = createClient(\n Deno.env.get('SUPABASE_URL') ?? '',\n Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n { global: { headers: { Authorization: req.headers.get('Authorization')! } } }\n )\n\n const { data, error } = await supabase.from('countries').select('*')\n\n if (error) {\n throw error\n }\n\n return new Response(JSON.stringify({ data }), {\n headers: { 'Content-Type': 'application/json' },\n status: 200,\n })\n } catch (err) {\n return new Response(String(err?.message ?? err), { status: 500 })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using a Postgres client",
          "content": "Because Edge Functions are a server-side technology, it's safe to connect directly to your database using any popular Postgres client. This means you can run raw SQL from your Edge Functions.\n\nHere is how you can connect to the database using Deno Postgres driver and run raw SQL.\n\nCheck out the [full example](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/postgres-on-the-edge).\n\n```ts index.ts\nimport * as postgres from 'https://deno.land/x/postgres@v0.17.0/mod.ts'\n\n// Get the connection string from the environment variable \"SUPABASE_DB_URL\"\nconst databaseUrl = Deno.env.get('SUPABASE_DB_URL')!\n\n// Create a database pool with three connections that are lazily established\nconst pool = new postgres.Pool(databaseUrl, 3, true)\n\nDeno.serve(async (_req) => {\n try {\n // Grab a connection from the pool\n const connection = await pool.connect()\n\n try {\n // Run a query\n const result = await connection.queryObject`SELECT * FROM animals`\n const animals = result.rows // [{ id: 1, name: \"Lion\" }, ...]\n\n // Encode the result as pretty printed JSON\n const body = JSON.stringify(\n animals,\n (key, value) => (typeof value === 'bigint' ? value.toString() : value),\n 2\n )\n\n // Return the response with the correct content type header\n return new Response(body, {\n status: 200,\n headers: { 'Content-Type': 'application/json; charset=utf-8' },\n })\n } finally {\n // Release the connection back into the pool\n connection.release()\n }\n } catch (err) {\n console.error(err)\n return new Response(String(err?.message ?? err), { status: 500 })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using Drizzle",
          "content": "You can use Drizzle together with [Postgres.js](https://github.com/porsager/postgres). Both can be loaded directly from npm:\n\n```json supabase/functions/import_map.json\n{\n \"imports\": {\n \"drizzle-orm\": \"npm:drizzle-orm@0.29.1\",\n \"drizzle-orm/\": \"npm:/drizzle-orm@0.29.1/\",\n \"postgres\": \"npm:postgres@3.4.3\"\n }\n}\n```\n\n```ts supabase/functions/drizzle/index.ts\nimport { drizzle } from 'drizzle-orm/postgres-js'\nimport postgres from 'postgres'\nimport { countries } from '../_shared/schema.ts'\n\nconst connectionString = Deno.env.get('SUPABASE_DB_URL')!\n\nDeno.serve(async (_req) => {\n // Disable prefetch as it is not supported for \"Transaction\" pool mode\n const client = postgres(connectionString, { prepare: false })\n const db = drizzle(client)\n const allCountries = await db.select().from(countries)\n\n return Response.json(allCountries)\n})\n```\n\nYou can find the full example on [GitHub](https://github.com/thorwebdev/edgy-drizzle).",
          "level": 2
        },
        {
          "type": "section",
          "title": "SSL connections",
          "content": "Deployed edge functions are pre-configured to use SSL for connections to the Supabase database. You don't need to add any extra configurations.\n\nIf you want to use SSL connections during local development, follow these steps:\n\n- Download the SSL certificate from [Database settings](https://supabase.com/dashboard/project/_/settings/database)\n\n- In your [local .env file](https://supabase.com/docs/guides/functions/secrets), add these two variables:\n\n```bash\nSSL_CERT_FILE=/path/to/cert.crt # set the path to the downloaded cert\nDENO_TLS_CA_STORE=mozilla,system\n```",
          "level": 2
        }
      ],
      "wordCount": 523,
      "characterCount": 4160
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-cors",
      "identifier": "functions-cors",
      "name": "CORS (Cross-Origin Resource Sharing) support for Invoking from the browser",
      "description": "Add CORS headers to invoke Edge Functions from the browser.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/cors",
      "dateModified": "2025-06-13T12:45:11.298616",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/cors.mdx",
      "frontmatter": {
        "id": "functions-cors",
        "title": "CORS (Cross-Origin Resource Sharing) support for Invoking from the browser",
        "description": "Add CORS headers to invoke Edge Functions from the browser."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "To invoke edge functions from the browser, you need to handle [CORS Preflight](https://developer.mozilla.org/en-US/docs/Glossary/Preflight_request) requests.\n\nSee the [example on GitHub](https://github.com/supabase/supabase/blob/master/examples/edge-functions/supabase/functions/browser-with-cors/index.ts)."
        },
        {
          "type": "section",
          "title": "Recommended setup",
          "content": "We recommend adding a `cors.ts` file within a [`_shared` folder](/docs/guides/functions/quickstart#organizing-your-edge-functions) which makes it easy to reuse the CORS headers across functions:\n\n```ts cors.ts\nexport const corsHeaders = {\n 'Access-Control-Allow-Origin': '*',\n 'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',\n}\n```\n\nYou can then import and use the CORS headers within your functions:\n\n```ts index.ts\nimport { corsHeaders } from '../_shared/cors.ts'\n\nconsole.log(`Function \"browser-with-cors\" up and running!`)\n\nDeno.serve(async (req) => {\n // This is needed if you're planning to invoke your function from a browser.\n if (req.method === 'OPTIONS') {\n return new Response('ok', { headers: corsHeaders })\n }\n\n try {\n const { name } = await req.json()\n const data = {\n message: `Hello ${name}!`,\n }\n\n return new Response(JSON.stringify(data), {\n headers: { ...corsHeaders, 'Content-Type': 'application/json' },\n status: 200,\n })\n } catch (error) {\n return new Response(JSON.stringify({ error: error.message }), {\n headers: { ...corsHeaders, 'Content-Type': 'application/json' },\n status: 400,\n })\n }\n})\n```",
          "level": 3
        }
      ],
      "wordCount": 168,
      "characterCount": 1489
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-dart-edge",
      "identifier": "functions-dart-edge",
      "name": "Dart Edge",
      "description": "Write your functions using Dart.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/dart-edge",
      "dateModified": "2025-06-13T12:45:11.298678",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/dart-edge.mdx",
      "frontmatter": {
        "id": "functions-dart-edge",
        "title": "Dart Edge",
        "description": "Write your functions using Dart."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Be aware that the Dart Edge project is currently not actively maintained due to numerous breaking changes in Dart's development of (WASM) support.\n\n[Dart Edge](https://docs.dartedge.dev/) is an experimental project that enables you to write Supabase Edge Functions using Dart. It's built and maintained by [Invertase](https://invertase.io/).\n\nFor detailed information on how to set up and use Dart Edge with Supabase, refer to the [official Dart Edge documentation for Supabase](https://invertase.docs.page/dart_edge/platform/supabase)."
        }
      ],
      "wordCount": 68,
      "characterCount": 536
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-debugging-tools",
      "identifier": "functions-debugging-tools",
      "name": "Local Debugging with DevTools",
      "description": "How to use Chrome DevTools to debug Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/debugging-tools",
      "dateModified": "2025-06-13T12:45:11.298760",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/debugging-tools.mdx",
      "frontmatter": {
        "id": "functions-debugging-tools",
        "title": "Local Debugging with DevTools",
        "description": "How to use Chrome DevTools to debug Edge Functions.",
        "subtitle": "How to use Chrome DevTools to debug Edge Functions.",
        "tocVideo": "sOrtcoKg5zQ"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Since [v1.171.0](https://github.com/supabase/cli/releases/tag/v1.171.0) the Supabase CLI supports debugging Edge Functions via the v8 inspector protocol, allowing for debugging via [Chrome DevTools](https://developer.chrome.com/docs/devtools/) and other Chromium-based browsers."
        },
        {
          "type": "section",
          "title": "Inspect with Chrome Developer Tools",
          "content": "You can use the [Chrome DevTools](https://developer.chrome.com/docs/devtools/) to set breakpoints and inspect the execution of your Edge Functions.\n\n1. Serve your functions in [inspect mode](/docs/reference/cli/supabase-functions-serve): `supabase functions serve --inspect-mode brk`. This will set a breakpoint at the first line to pause script execution before any code runs.\n1. In your Chrome browser navigate to `chrome://inspect`.\n1. Click the \"Configure...\"\" button to the right of the Discover network targets checkbox.\n1. In the Target discovery settings dialog box that opens, enter `127.0.0.1:8083` in the blank space and click the \"Done\" button to exit the dialog box.\n1. Click \"Open dedicated DevTools for Node\" to complete the preparation for debugging. The opened DevTools window will now listen to any incoming requests to edge-runtime.\n1. Send a request to your function running locally, e.g. via curl or Postman. The DevTools window will now pause script execution at first line.\n1. In the \"Sources\" tab navigate to `file://` > `home/deno/functions//index.ts`.\n1. Use the DevTools to set breakpoints and inspect the execution of your Edge Function.\n\n![Debugging in Chrome DevTools.](/docs/img/guides/functions/debug-chrome-devtools.png)",
          "level": 3
        }
      ],
      "wordCount": 204,
      "characterCount": 1574
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-deno2",
      "identifier": "functions-deno2",
      "name": "Using Deno 2",
      "description": "Everything you need to know about the Deno 2 runtime",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/deno2",
      "dateModified": "2025-06-13T12:45:11.298900",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/deno2.mdx",
      "frontmatter": {
        "id": "deno2",
        "title": "Using Deno 2",
        "description": "Everything you need to know about the Deno 2 runtime",
        "subtitle": "Everything you need to know about the Deno 2 runtime"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "This feature is in Public Alpha. [Submit a support ticket](https://supabase.help) if you have any issues."
        },
        {
          "type": "section",
          "title": "What is Deno 2?",
          "content": "Deno 2 is a major upgrade to the Deno runtime that powers Supabase Edge Functions. It focuses on scalability and seamless ecosystem compatibility while maintaining Deno's core principles of security, simplicity, and developer experience.\n\n**Key improvements include**\n\n- **Node.js and npm compatibility**: Dramatically improved support for npm packages and Node.js code\n- **Better dependency management**: New tools like `deno install`, `deno add`, and `deno remove` for simplified package management\n- **Improved performance**: Enhanced runtime execution and startup times\n- **Workspace and monorepo support**: Better handling of complex project structures\n- **Framework compatibility**: Support for Next.js, SvelteKit, Remix, and other popular frameworks\n- **Full package.json support**: Works seamlessly with existing Node.js projects and npm workspaces\n\nWhile these improvements are exciting, they come with some changes that may affect your existing functions. We'll support Deno 1.x functions for a limited time, but we recommend migrating to Deno 2 within the next few months to ensure continued functionality.",
          "level": 3
        },
        {
          "type": "section",
          "title": "How to use Deno 2",
          "content": "Deno 2 will soon become the default choice for creating new functions. For now, Deno 2 is available in preview mode for local development.\n\nHere's how you can build and deploy a function with Deno 2:\n\n- [Install Deno 2.1](https://docs.deno.com/runtime/getting_started/installation/) or newer version on your machine\n\n- Go to your Supabase project. `cd my-supabase-project`\n\n- Open `supabase/config.toml` and set `deno_version = 2`\n\n```toml\n[edge_runtime]\ndeno_version = 2\n```\n\n- All your existing functions should work as before.\n\nTo scaffold a new function as a Deno 2 project:\n\n```bash\ndeno init --serve hello-world\n```\n\n- Open `supabase/config.toml` and add the following:\n\n```\n[functions.hello-world]\nentrypoint = \"./functions/hello-world/main.ts\"\n```\n\n- Open supabase/functions/hello-world/main.ts and modify line 10 to:\n\n```typescript\nif (url.pathname === \"/hello-world\") {\n```\n\n- Use `npx supabase@beta functions serve --no-verify-jwt` to start the dev server.\n\n- Visit http://localhost:54321/functions/v1/hello-world.\n\n- To run built-in tests, `cd supabase/functions/hello-world; deno test`",
          "level": 3
        },
        {
          "type": "section",
          "title": "How to migrate existing functions from Deno 1 to Deno 2",
          "content": "For a comprehensive migration guide, see the [official Deno 1.x to 2.x migration guide](https://docs.deno.com/runtime/reference/migration_guide/#content).\n\nMost Deno 1 Edge Functions will be compatible out of the box with Deno 2, and no action needs to be taken. When we upgrade our hosted runtime, your functions will automatically be deployed on a Deno 2 cluster.\n\nHowever, for a small number of functions, this may break existing functionality.\n\nThe most common issue to watch for is that some Deno 1 API calls are incompatible with Deno 2 runtime.\n\nFor instance if you are using:\n\n- `Deno.Closer`\n\nUse [`Closer`](https://jsr.io/@std/io/doc/types/~/Closer) from the Standard Library instead.\n\n```tsx\n+ import type { Closer } from \"jsr:@std/io/types\";\n- function foo(closer: Deno.Closer) {\n+ function foo(closer: Closer) {\n // ...\n}\n```\n\nThe best way to validate your APIs are up to date is to use the Deno lint, which has [rules to disallow deprecated APIs](https://docs.deno.com/lint/rules/no-deprecated-deno-api/).\n\n ```bash\n deno lint\n ```\n\nFor a full list of API changes, see the [official Deno 2 list](https://docs.deno.com/runtime/reference/migration_guide/#api-changes).",
          "level": 3
        }
      ],
      "wordCount": 502,
      "characterCount": 3611
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-dependencies",
      "identifier": "functions-dependencies",
      "name": "Managing dependencies",
      "description": "Managing packages and dependencies.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/dependencies",
      "dateModified": "2025-06-13T12:45:11.299132",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/dependencies.mdx",
      "frontmatter": {
        "id": "functions-import-maps",
        "title": "Managing dependencies",
        "description": "Managing packages and dependencies.",
        "subtitle": "Managing packages and dependencies.",
        "tocVideo": "ILr3cneZuFk"
      },
      "sections": [
        {
          "type": "section",
          "title": "Importing dependencies",
          "content": "Supabase Edge Functions support several ways to import dependencies:\n\n- JavaScript modules from npm (https://docs.deno.com/examples/npm/)\n- Built-in [Node APIs](https://docs.deno.com/runtime/manual/node/compatibility)\n- Modules published to [JSR](https://jsr.io/) or [deno.land/x](https://deno.land/x)",
          "level": 2
        },
        {
          "type": "section",
          "title": "NPM modules",
          "content": "You can import npm modules using the `npm:` specifier:\n\n```ts\nimport { createClient } from 'npm:@supabase/supabase-js@2'\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Node.js built-ins",
          "content": "For Node.js built-in APIs, use the `node:` specifier:\n\n```ts\nimport process from 'node:process'\n```\n\nLearn more about npm specifiers and Node built-in APIs in [Deno's documentation](https://docs.deno.com/runtime/manual/node/npm_specifiers).",
          "level": 3
        },
        {
          "type": "section",
          "title": "JSR",
          "content": "You can import JS modules published to [JSR](https://jsr.io/) (e.g.: Deno's standard library), using the `jsr:` specifier:\n\n```ts\nimport path from 'jsr:@std/path@1.0.8'\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Managing dependencies",
          "content": "Developing with Edge Functions is similar to developing with Node.js, but with a few key differences.\n\nIn the Deno ecosystem, each function should be treated as an independent project with its own set of dependencies and configurations. This \"isolation by design\" approach:\n\n- Ensures each function has explicit control over its dependencies\n- Prevents unintended side effects between functions\n- Makes deployments more predictable and maintainable\n- Allows for different versions of the same dependency across functions\n\nFor these reasons, we recommend maintaining separate configuration files (`deno.json`, `.npmrc`, or `import_map.json`) within each function's directory, even if it means duplicating some configurations.\n\nThere are two ways to manage your dependencies in Supabase Edge Functions:",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using deno.json (recommended)",
          "content": "This feature requires Supabase CLI version 1.215.0 or higher.\n\nEach function should have its own `deno.json` file to manage dependencies and configure Deno-specific settings. This ensures proper isolation between functions and is the recommended approach for deployment. For a complete list of supported options, see the [official Deno configuration documentation](https://docs.deno.com/runtime/manual/getting_started/configuration_file).\n\n```json supabase/functions/my-function/deno.json\n{\n \"imports\": {\n \"lodash\": \"https://cdn.skypack.dev/lodash\"\n }\n}\n```\n\nThe recommended file structure for deployment:\n\n```bash\n└── supabase\n ├── functions\n │ ├── function-one\n │ │ ├── index.ts\n │ │ ├─- deno.json # Function-specific Deno configuration\n │ │ └── .npmrc # Function-specific npm configuration (if needed)\n │ └── function-two\n │ ├── index.ts\n │ ├─- deno.json # Function-specific Deno configuration\n │ └── .npmrc # Function-specific npm configuration (if needed)\n └── config.toml\n```\n\n While it's possible to use a global `deno.json` in the `/supabase/functions` directory for local\n development, this approach is not recommended for deployment. Each function should maintain its\n own configuration to ensure proper isolation and dependency management.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using import maps (legacy)",
          "content": "Import Maps are a legacy way to manage dependencies, similar to a `package.json` file. While still supported, we recommend using `deno.json`. If both exist, `deno.json` takes precedence.\n\nEach function should have its own `import_map.json` file for proper isolation:\n\n```json supabase/functions/my-function/import_map.json\n{\n \"imports\": {\n \"lodash\": \"https://cdn.skypack.dev/lodash\"\n }\n}\n```\n\nThe recommended file structure:\n\n```bash\n└── supabase\n ├── functions\n │ ├── function-one\n │ │ ├── index.ts\n │ │ └── import_map.json # Function-specific import map\n │ └── function-two\n │ ├── index.ts\n │ └── import_map.json # Function-specific import map\n └── config.toml\n```\n\n While it's possible to use a global `import_map.json` in the `/supabase/functions` directory for\n local development, this approach is not recommended for deployment. Each function should maintain\n its own import map to ensure proper isolation.\n\nIf using import maps with VSCode, update your `.vscode/settings.json` to point to your function-specific import map:\n\n```json settings.json\n{\n \"deno.enable\": true,\n \"deno.unstable\": [\n \"bare-node-builtins\",\n \"byonm\"\n // ... other flags ...\n ],\n \"deno.importMap\": \"./supabase/functions/my-function/import_map.json\"\n}\n```\n\nYou can override the default import map location using the `--import-map ` flag with `serve` and `deploy` commands, or by setting the `import_map` property in your `config.toml` file:\n\n```toml supabase/config.toml\n[functions.my-function]\nimport_map = \"./supabase/functions/my-function/import_map.json\"\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Importing from private registries",
          "content": "This feature requires Supabase CLI version 1.207.9 or higher.\n\nTo use private npm packages, create a `.npmrc` file within your function directory. This ensures proper isolation and dependency management for each function.\n\n```bash\n└── supabase\n └── functions\n └── my-function\n ├── index.ts\n ├── deno.json\n └── .npmrc # Function-specific npm configuration\n```\n\nAdd your registry details in the `.npmrc` file. Follow [this guide](https://docs.npmjs.com/cli/v10/configuring-npm/npmrc) to learn more about the syntax of npmrc files.\n\n```plaintext\n@myorg:registry=https://npm.registryhost.com\n//npm.registryhost.com/:_authToken=VALID_AUTH_TOKEN\n```\n\n While it's possible to use a global `.npmrc` in the `/supabase/functions` directory for local\n development, we recommend using function-specific `.npmrc` files for deployment to maintain proper\n isolation.\n\nAfter configuring your `.npmrc`, you can import the private package in your function code:\n\n```ts\nimport MyPackage from 'npm:@myorg/private-package@v1.0.1'\n\n// use MyPackage\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using a custom NPM registry",
          "content": "This feature requires Supabase CLI version 2.2.8 or higher.\n\nSome organizations require a custom NPM registry for security and compliance purposes. In such instances, you can specify the custom NPM registry to use via `NPM_CONFIG_REGISTRY` environment variable.\n\nYou can define it in the project's `.env` file or directly specify it when running the deploy command:\n\n```bash\nNPM_CONFIG_REGISTRY=https://custom-registry/ supabase functions deploy my-function\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Importing types",
          "content": "If your [environment is set up properly](/docs/guides/functions/local-development) and the module you're importing is exporting types, the import will have types and autocompletion support.\n\nSome npm packages may not ship out of the box types and you may need to import them from a separate package. You can specify their types with a `@deno-types` directive:\n\n```ts\n// @deno-types=\"npm:@types/express@^4.17\"\nimport express from 'npm:express@^4.17'\n```\n\nTo include types for built-in Node APIs, add the following line to the top of your imports:\n\n```ts\n/// \n```",
          "level": 2
        }
      ],
      "wordCount": 858,
      "characterCount": 6758
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-deploy",
      "identifier": "functions-deploy",
      "name": "Deploy to Production",
      "description": "Deploy your Edge Functions to your remote Supabase Project.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/deploy",
      "dateModified": "2025-06-13T12:45:11.299259",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/deploy.mdx",
      "frontmatter": {
        "id": "functions-deploy",
        "title": "Deploy to Production",
        "description": "Deploy your Edge Functions to your remote Supabase Project.",
        "subtitle": "Deploy your Edge Functions to your remote Supabase Project.",
        "tocVideo": "5OWH9c4u68M"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Once you have developed your Edge Functions locally, you can deploy them to your Supabase project."
        },
        {
          "type": "section",
          "title": "Login to the CLI",
          "content": "Log in to the Supabase CLI if necessary:\n\n```bash\nsupabase login\n```\n\nSee the [CLI Docs](/docs/guides/cli) to learn how to install the Supabase CLI on your local machine.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Get your project ID",
          "content": "Get the project ID associated with your function by running:\n\n```bash\nsupabase projects list\n```\n\nIf you haven't yet created a Supabase project, you can do so by visiting [database.new](https://database.new).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Link your local project",
          "content": "[Link](/docs/reference/cli/usage#supabase-link) your local project to your remote Supabase project using the ID you just retrieved:\n\n```bash\nsupabase link --project-ref your-project-id\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy your Edge Functions",
          "content": "Since Supabase CLI version 1.123.4, you must have [Docker Desktop](https://docs.docker.com/desktop/) installed to deploy Edge Functions.\n\nYou can deploy all of your Edge Functions with a single command:\n\n```bash\nsupabase functions deploy\n```\n\nYou can deploy individual Edge Functions by specifying the name of the function in the deploy command:\n\n```bash\nsupabase functions deploy hello-world\n```\n\nBy default, Edge Functions require a valid JWT in the authorization header. If you want to use Edge Functions without Authorization checks (commonly used for Stripe webhooks), you can pass the `--no-verify-jwt` flag when deploying your Edge Functions.\n\n```bash\nsupabase functions deploy hello-world --no-verify-jwt\n```\n\nBe careful when using this flag, as it will allow anyone to invoke your Edge Function without a valid JWT. The Supabase client libraries automatically handle authorization.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Invoking remote functions",
          "content": "You can now invoke your Edge Function using the project's `ANON_KEY`, which can be found in the [API settings](https://supabase.com/dashboard/project/_/settings/api) of the Supabase Dashboard.\n\n```bash name=cURL\ncurl --request POST 'https://.supabase.co/functions/v1/hello-world' \\\n --header 'Authorization: Bearer ANON_KEY' \\\n --header 'Content-Type: application/json' \\\n --data '{ \"name\":\"Functions\" }'\n```\n\n```js name=JavaScript\nimport { createClient } from '@supabase/supabase-js'\n\n// Create a single supabase client for interacting with your database\nconst supabase = createClient('https://xyzcompany.supabase.co', 'public-anon-key')\n\nconst { data, error } = await supabase.functions.invoke('hello-world', {\n body: { name: 'Functions' },\n})\n```\n\nYou should receive the response `{ \"message\":\"Hello Functions!\" }`.",
          "level": 2
        }
      ],
      "wordCount": 339,
      "characterCount": 2516
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-development-tips",
      "identifier": "functions-development-tips",
      "name": "Development tips",
      "description": "Tips for getting started with Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/development-tips",
      "dateModified": "2025-06-13T12:45:11.299423",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/development-tips.mdx",
      "frontmatter": {
        "id": "functions-development-tips",
        "title": "Development tips",
        "description": "Tips for getting started with Edge Functions.",
        "subtitle": "Tips for getting started with Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Here are a few recommendations when you first start developing Edge Functions."
        },
        {
          "type": "section",
          "title": "Skipping authorization checks",
          "content": "By default, Edge Functions require a valid JWT in the authorization header. If you want to use Edge Functions without Authorization checks (commonly used for Stripe webhooks), you can pass the `--no-verify-jwt` flag when serving your Edge Functions locally.\n\n```bash\nsupabase functions serve hello-world --no-verify-jwt\n```\n\nBe careful when using this flag, as it will allow anyone to invoke your Edge Function without a valid JWT. The Supabase client libraries automatically handle authorization.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using HTTP methods",
          "content": "Edge Functions support `GET`, `POST`, `PUT`, `PATCH`, `DELETE`, and `OPTIONS`. A Function can be designed to perform different actions based on a request's HTTP method. See the [example on building a RESTful service](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/restful-tasks) to learn how to handle different HTTP methods in your Function.\n\nHTML content is not supported. `GET` requests that return `text/html` will be rewritten to `text/plain`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Naming Edge Functions",
          "content": "We recommend using hyphens to name functions because hyphens are the most URL-friendly of all the naming conventions (snake_case, camelCase, PascalCase).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Organizing your Edge Functions",
          "content": "We recommend developing \"fat functions\". This means that you should develop few large functions, rather than many small functions. One common pattern when developing Functions is that you need to share code between two or more Functions. To do this, you can store any shared code in a folder prefixed with an underscore (`_`). We also recommend a separate folder for [Unit Tests](/docs/guides/functions/unit-test) including the name of the function followed by a `-test` suffix.\nWe recommend this folder structure:\n\n```bash\n└── supabase\n ├── functions\n │ ├── import_map.json # A top-level import map to use across functions.\n │ ├── _shared\n │ │ ├── supabaseAdmin.ts # Supabase client with SERVICE_ROLE key.\n │ │ └── supabaseClient.ts # Supabase client with ANON key.\n │ │ └── cors.ts # Reusable CORS headers.\n │ ├── function-one # Use hyphens to name functions.\n │ │ └── index.ts\n │ └── function-two\n │ │ └── index.ts\n │ └── tests\n │ └── function-one-test.ts\n │ └── function-two-test.ts\n ├── migrations\n └── config.toml\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using config.toml",
          "content": "Individual function configuration like [JWT verification](/docs/guides/cli/config#functions.function_name.verify_jwt) and [import map location](/docs/guides/cli/config#functions.function_name.import_map) can be set via the `config.toml` file.\n\n```toml supabase/config.toml\n[functions.hello-world]\nverify_jwt = false\nimport_map = './import_map.json'\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Not using TypeScript",
          "content": "When you create a new Edge Function, it will use TypeScript by default. However, it is possible to write and deploy Edge Functions using pure JavaScript.\n\nSave your Function as a JavaScript file (e.g. `index.js`) and then update the `supabase/config.toml` as follows:\n\n`entrypoint` is available only in Supabase CLI version 1.215.0 or higher.\n\n```toml supabase/config.toml\n[functions.hello-world]",
          "level": 3
        },
        {
          "type": "section",
          "title": "other entries",
          "content": "entrypoint = './functions/hello-world/index.js' # path must be relative to config.toml\n```\n\nYou can use any `.ts`, `.js`, `.tsx`, `.jsx` or `.mjs` file as the `entrypoint` for a Function.",
          "level": 1
        },
        {
          "type": "section",
          "title": "Error handling",
          "content": "The `supabase-js` library provides several error types that you can use to handle errors that might occur when invoking Edge Functions:\n\n```js\nimport { FunctionsHttpError, FunctionsRelayError, FunctionsFetchError } from '@supabase/supabase-js'\n\nconst { data, error } = await supabase.functions.invoke('hello', {\n headers: { 'my-custom-header': 'my-custom-header-value' },\n body: { foo: 'bar' },\n})\n\nif (error instanceof FunctionsHttpError) {\n const errorMessage = await error.context.json()\n console.log('Function returned an error', errorMessage)\n} else if (error instanceof FunctionsRelayError) {\n console.log('Relay error:', error.message)\n} else if (error instanceof FunctionsFetchError) {\n console.log('Fetch error:', error.message)\n}\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Database Functions vs Edge Functions",
          "content": "For data-intensive operations we recommend using [Database Functions](/docs/guides/database/functions), which are executed within your database and can be called remotely using the [REST and GraphQL API](/docs/guides/api).\n\nFor use-cases which require low-latency we recommend [Edge Functions](/docs/guides/functions), which are globally-distributed and can be written in TypeScript.",
          "level": 3
        }
      ],
      "wordCount": 603,
      "characterCount": 4571
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-ephemeral-storage",
      "identifier": "functions-ephemeral-storage",
      "name": "Ephemeral Storage",
      "description": "Read and write from temporary directory",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/ephemeral-storage",
      "dateModified": "2025-06-13T12:45:11.299527",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/ephemeral-storage.mdx",
      "frontmatter": {
        "id": "function-ephemeral-storage",
        "title": "Ephemeral Storage",
        "description": "Read and write from temporary directory",
        "subtitle": "Read and write from temporary directory"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Functions provides ephemeral file storage. You can read and write files to the `/tmp` directory.\n\nEphemeral storage will reset on each function invocation. This means the files you write during an invocation can only be read within the same invocation."
        },
        {
          "type": "section",
          "title": "Use cases",
          "content": "Here are some use cases where ephemeral storage can be useful:\n\n- Unzip an archive of CSVs and then add them as records to the DB\n- Custom image manipulation workflows (using [`magick-wasm`](https://supabase.com/docs/guides/functions/examples/image-manipulation))\n\nYou can use [Background Tasks](https://supabase.com/docs/guides/functions/background-tasks) to handle slow file processing outside of a request.",
          "level": 3
        },
        {
          "type": "section",
          "title": "How to use",
          "content": "You can use [Deno File System APIs](https://docs.deno.com/api/deno/file-system) or the [`node:fs` module](https://docs.deno.com/api/node/fs/) to access the `/tmp` path.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Example",
          "content": "Here is an example of how to write a user-uploaded zip file into temporary storage for further processing.\n\n```js\nDeno.serve(async (req) => {\n if (req.headers.get('content-type') !== 'application/zip') {\n return new Response('file must be a zip file', {\n status: 400,\n })\n }\n\n const uploadId = crypto.randomUUID()\n await Deno.writeFile('/tmp/' + uploadId, req.body)\n\n // do something with the written zip file\n\n return new Response('ok')\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Unavailable APIs",
          "content": "Currently, the synchronous APIs (e.g. `Deno.writeFileSync` or `Deno.mkdirSync`) for creating or writing files are not supported.\n\nYou can use sync variations of read APIs (e.g. `Deno.readFileSync`).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Limits",
          "content": "In the hosted platform, a free project can write up to 256MB of data to ephemeral storage. A paid project can write up to 512MB.",
          "level": 3
        }
      ],
      "wordCount": 232,
      "characterCount": 1692
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-amazon-bedrock-image-generator",
      "identifier": "functions-examples-amazon-bedrock-image-generator",
      "name": "Generate Images with Amazon Bedrock",
      "description": "Generate images with Amazon Bedrock and store them in Supabase Storage.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/amazon-bedrock-image-generator",
      "dateModified": "2025-06-13T12:45:11.299690",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/amazon-bedrock-image-generator.mdx",
      "frontmatter": {
        "id": "examples-amazon-bedrock-image-generator",
        "title": "Generate Images with Amazon Bedrock",
        "description": "Generate images with Amazon Bedrock and store them in Supabase Storage.",
        "tocVideo": "KIwN2TmkTlg"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[Amazon Bedrock](https://aws.amazon.com/bedrock) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.\n\nThis guide will walk you through an example using the Amazon Bedrock JavaScript SDK in Supabase Edge Functions to generate images using the [Amazon Titan Image Generator G1](https://aws.amazon.com/blogs/machine-learning/use-amazon-titan-models-for-image-generation-editing-and-searching/) model."
        },
        {
          "type": "section",
          "title": "Setup",
          "content": "- In your AWS console, navigate to Amazon Bedrock and under \"Request model access\", select the Amazon Titan Image Generator G1 model.\n- In your Supabase project, create a `.env` file in the `supabase` directory with the following contents:\n\n```txt\nAWS_DEFAULT_REGION=\"\"\nAWS_ACCESS_KEY_ID=\"\"\nAWS_SECRET_ACCESS_KEY=\"\"\nAWS_SESSION_TOKEN=\"\"",
          "level": 2
        },
        {
          "type": "section",
          "title": "Mocked config files",
          "content": "AWS_SHARED_CREDENTIALS_FILE=\"./aws/credentials\"\nAWS_CONFIG_FILE=\"./aws/config\"\n```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Configure Storage",
          "content": "- [locally] Run `supabase start`\n- Open Studio URL: [locally](http://127.0.0.1:54323/project/default/storage/buckets) | [hosted](https://app.supabase.com/project/_/storage/buckets)\n- Navigate to Storage\n- Click \"New bucket\"\n- Create a new public bucket called \"images\"",
          "level": 3
        },
        {
          "type": "section",
          "title": "Code",
          "content": "Create a new function in your project:\n\n```bash\nsupabase functions new amazon-bedrock\n```\n\nAnd add the code to the `index.ts` file:\n\n```ts index.ts\n// We need to mock the file system for the AWS SDK to work.\nimport { prepareVirtualFile } from 'https://deno.land/x/mock_file@v1.1.2/mod.ts'\n\nimport { BedrockRuntimeClient, InvokeModelCommand } from 'npm:@aws-sdk/client-bedrock-runtime'\nimport { createClient } from 'npm:@supabase/supabase-js'\nimport { decode } from 'npm:base64-arraybuffer'\n\nconsole.log('Hello from Amazon Bedrock!')\n\nDeno.serve(async (req) => {\n prepareVirtualFile('./aws/config')\n prepareVirtualFile('./aws/credentials')\n\n const client = new BedrockRuntimeClient({\n region: Deno.env.get('AWS_DEFAULT_REGION') ?? 'us-west-2',\n credentials: {\n accessKeyId: Deno.env.get('AWS_ACCESS_KEY_ID') ?? '',\n secretAccessKey: Deno.env.get('AWS_SECRET_ACCESS_KEY') ?? '',\n sessionToken: Deno.env.get('AWS_SESSION_TOKEN') ?? '',\n },\n })\n\n const { prompt, seed } = await req.json()\n console.log(prompt)\n const input = {\n contentType: 'application/json',\n accept: '*/*',\n modelId: 'amazon.titan-image-generator-v1',\n body: JSON.stringify({\n taskType: 'TEXT_IMAGE',\n textToImageParams: { text: prompt },\n imageGenerationConfig: {\n numberOfImages: 1,\n quality: 'standard',\n cfgScale: 8.0,\n height: 512,\n width: 512,\n seed: seed ?? 0,\n },\n }),\n }\n\n const command = new InvokeModelCommand(input)\n const response = await client.send(command)\n console.log(response)\n\n if (response.$metadata.httpStatusCode === 200) {\n const { body, $metadata } = response\n\n const textDecoder = new TextDecoder('utf-8')\n const jsonString = textDecoder.decode(body.buffer)\n const parsedData = JSON.parse(jsonString)\n console.log(parsedData)\n const image = parsedData.images[0]\n\n const supabaseClient = createClient(\n // Supabase API URL - env var exported by default.\n Deno.env.get('SUPABASE_URL')!,\n // Supabase API ANON KEY - env var exported by default.\n Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n )\n\n const { data: upload, error: uploadError } = await supabaseClient.storage\n .from('images')\n .upload(`${$metadata.requestId ?? ''}.png`, decode(image), {\n contentType: 'image/png',\n cacheControl: '3600',\n upsert: false,\n })\n if (!upload) {\n return Response.json(uploadError)\n }\n const { data } = supabaseClient.storage.from('images').getPublicUrl(upload.path!)\n return Response.json(data)\n }\n\n return Response.json(response)\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Run the function locally",
          "content": "1. Run `supabase start` (see: https://supabase.com/docs/reference/cli/supabase-start)\n2. Start with env: `supabase functions serve --env-file supabase/.env`\n3. Make an HTTP request:\n\n```bash\n curl -i --location --request POST 'http://127.0.0.1:54321/functions/v1/amazon-bedrock' \\\n --header 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0' \\\n --header 'Content-Type: application/json' \\\n --data '{\"prompt\":\"A beautiful picture of a bird\"}'\n```\n\n4. Navigate back to your storage bucket. You might have to hit the refresh button to see the uploaded image.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy to your hosted project",
          "content": "```bash\nsupabase link\nsupabase functions deploy amazon-bedrock\nsupabase secrets set --env-file supabase/.env\n```\n\nYou've now deployed a serverless function that uses AI to generate and upload images to your Supabase storage bucket.",
          "level": 2
        }
      ],
      "wordCount": 539,
      "characterCount": 4872
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-auth-send-email-hook-react-email-resend",
      "identifier": "functions-examples-auth-send-email-hook-react-email-resend",
      "name": "Custom Auth Emails with React Email and Resend",
      "description": "Use the send email hook to send custom auth emails with React Email and Resend in Supabase Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/auth-send-email-hook-react-email-resend",
      "dateModified": "2025-06-13T12:45:11.299917",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/auth-send-email-hook-react-email-resend.mdx",
      "frontmatter": {
        "title": "Custom Auth Emails with React Email and Resend",
        "description": "Use the send email hook to send custom auth emails with React Email and Resend in Supabase Edge Functions.",
        "tocVideo": "tlA7BomSCgU"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Use the [send email hook](/docs/guides/auth/auth-hooks/send-email-hook?queryGroups=language&language=http) to send custom auth emails with [React Email](https://react.email/) and [Resend](https://resend.com/) in Supabase Edge Functions.\n\nPrefer to jump straight to the code? [Check out the example on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/auth-hook-react-email-resend)."
        },
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "To get the most out of this guide, you’ll need to:\n\n- [Create a Resend API key](https://resend.com/api-keys)\n- [Verify your domain](https://resend.com/domains)\n\nMake sure you have the latest version of the [Supabase CLI](https://supabase.com/docs/guides/cli#installation) installed.",
          "level": 3
        },
        {
          "type": "section",
          "title": "1. Create Supabase function",
          "content": "Create a new function locally:\n\n```bash\nsupabase functions new send-email\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "2. Edit the handler function",
          "content": "Paste the following code into the `index.ts` file:\n\n```tsx supabase/functions/send-email/index.ts\nimport React from 'npm:react@18.3.1'\nimport { Webhook } from 'https://esm.sh/standardwebhooks@1.0.0'\nimport { Resend } from 'npm:resend@4.0.0'\nimport { renderAsync } from 'npm:@react-email/components@0.0.22'\nimport { MagicLinkEmail } from './_templates/magic-link.tsx'\n\nconst resend = new Resend(Deno.env.get('RESEND_API_KEY') as string)\nconst hookSecret = Deno.env.get('SEND_EMAIL_HOOK_SECRET') as string\n\nDeno.serve(async (req) => {\n if (req.method !== 'POST') {\n return new Response('not allowed', { status: 400 })\n }\n\n const payload = await req.text()\n const headers = Object.fromEntries(req.headers)\n const wh = new Webhook(hookSecret)\n try {\n const {\n user,\n email_data: { token, token_hash, redirect_to, email_action_type },\n } = wh.verify(payload, headers) as {\n user: {\n email: string\n }\n email_data: {\n token: string\n token_hash: string\n redirect_to: string\n email_action_type: string\n site_url: string\n token_new: string\n token_hash_new: string\n }\n }\n\n const html = await renderAsync(\n React.createElement(MagicLinkEmail, {\n supabase_url: Deno.env.get('SUPABASE_URL') ?? '',\n token,\n token_hash,\n redirect_to,\n email_action_type,\n })\n )\n\n const { error } = await resend.emails.send({\n from: 'welcome ',\n to: [user.email],\n subject: 'Supa Custom MagicLink!',\n html,\n })\n if (error) {\n throw error\n }\n } catch (error) {\n console.log(error)\n return new Response(\n JSON.stringify({\n error: {\n http_code: error.code,\n message: error.message,\n },\n }),\n {\n status: 401,\n headers: { 'Content-Type': 'application/json' },\n }\n )\n }\n\n const responseHeaders = new Headers()\n responseHeaders.set('Content-Type', 'application/json')\n return new Response(JSON.stringify({}), {\n status: 200,\n headers: responseHeaders,\n })\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "3. Create React Email templates",
          "content": "Create a new folder `_templates` and create a new file `magic-link.tsx` with the following code:\n\n```tsx supabase/functions/send-email/_templates/magic-link.tsx\nimport {\n Body,\n Container,\n Head,\n Heading,\n Html,\n Link,\n Preview,\n Text,\n} from 'npm:@react-email/components@0.0.22'\nimport * as React from 'npm:react@18.3.1'\n\ninterface MagicLinkEmailProps {\n supabase_url: string\n email_action_type: string\n redirect_to: string\n token_hash: string\n token: string\n}\n\nexport const MagicLinkEmail = ({\n token,\n supabase_url,\n email_action_type,\n redirect_to,\n token_hash,\n}: MagicLinkEmailProps) => (\n\n Log in with this magic link\n\n Login\n \n Click here to log in with this magic link\n\n Or, copy and paste this temporary login code:\n \n {token}\n \n If you didn&apos;t try to login, you can safely ignore this email.\n\n ACME Corp\n \n , the famouse demo corp.\n\n)\n\nexport default MagicLinkEmail\n\nconst main = {\n backgroundColor: '#ffffff',\n}\n\nconst container = {\n paddingLeft: '12px',\n paddingRight: '12px',\n margin: '0 auto',\n}\n\nconst h1 = {\n color: '#333',\n fontFamily:\n \"-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif\",\n fontSize: '24px',\n fontWeight: 'bold',\n margin: '40px 0',\n padding: '0',\n}\n\nconst link = {\n color: '#2754C5',\n fontFamily:\n \"-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif\",\n fontSize: '14px',\n textDecoration: 'underline',\n}\n\nconst text = {\n color: '#333',\n fontFamily:\n \"-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif\",\n fontSize: '14px',\n margin: '24px 0',\n}\n\nconst footer = {\n color: '#898989',\n fontFamily:\n \"-apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue', sans-serif\",\n fontSize: '12px',\n lineHeight: '22px',\n marginTop: '12px',\n marginBottom: '24px',\n}\n\nconst code = {\n display: 'inline-block',\n padding: '16px 4.5%',\n width: '90.5%',\n backgroundColor: '#f4f4f4',\n borderRadius: '5px',\n border: '1px solid #eee',\n color: '#333',\n}\n```\n\nYou can find a selection of React Email templates in the [React Email Examples](https://react.email/examples).",
          "level": 3
        },
        {
          "type": "section",
          "title": "4. Deploy the Function",
          "content": "Deploy function to Supabase:\n\n```bash\nsupabase functions deploy send-email --no-verify-jwt\n```\n\nNote down the function URL, you will need it in the next step!",
          "level": 3
        },
        {
          "type": "section",
          "title": "5. Configure the Send Email Hook",
          "content": "- Go to the [Auth Hooks](/dashboard/project/_/auth/hooks) section of the Supabase dashboard and create a new \"Send Email hook\".\n- Select HTTPS as the hook type.\n- Paste the function URL in the \"URL\" field.\n- Click \"Generate Secret\" to generate your webhook secret and note it down.\n- Click \"Create\" to save the hook configuration.\n\nStore these secrets in your `.env` file.\n\n```bash supabase/functions/.env\nRESEND_API_KEY=your_resend_api_key\nSEND_EMAIL_HOOK_SECRET=\n```\n\nYou can generate the secret in the [Auth Hooks](/dashboard/project/_/auth/hooks) section of the Supabase dashboard. Make sure to remove the `v1,whsec_` prefix!\n\nSet the secrets from the `.env` file:\n\n```bash\nsupabase secrets set --env-file supabase/functions/.env\n```\n\nNow your Supabase Edge Function will be triggered anytime an Auth Email needs to be sent to the user!",
          "level": 3
        },
        {
          "type": "section",
          "title": "More resources",
          "content": "- [Send Email Hooks](/docs/guides/auth/auth-hooks/send-email-hook)\n- [Auth Hooks](/docs/guides/auth/auth-hooks)",
          "level": 2
        }
      ],
      "wordCount": 759,
      "characterCount": 6290
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-cloudflare-turnstile",
      "identifier": "functions-examples-cloudflare-turnstile",
      "name": "CAPTCHA support with Cloudflare Turnstile",
      "description": "Protecting Forms with Cloudflare Turnstile.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/cloudflare-turnstile",
      "dateModified": "2025-06-13T12:45:11.300032",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/cloudflare-turnstile.mdx",
      "frontmatter": {
        "id": "examples-cloudflare-turnstile",
        "title": "CAPTCHA support with Cloudflare Turnstile",
        "description": "Protecting Forms with Cloudflare Turnstile.",
        "tocVideo": "OwW0znboh60"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[Cloudflare Turnstile](https://www.cloudflare.com/products/turnstile/) is a friendly, free CAPTCHA replacement, and it works seamlessly with Supabase Edge Functions to protect your forms. [View on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/cloudflare-turnstile)."
        },
        {
          "type": "section",
          "title": "Setup",
          "content": "- Follow these steps to set up a new site: https://developers.cloudflare.com/turnstile/get-started/\n- Add the Cloudflare Turnstile widget to your site: https://developers.cloudflare.com/turnstile/get-started/client-side-rendering/",
          "level": 2
        },
        {
          "type": "section",
          "title": "Code",
          "content": "Create a new function in your project:\n\n```bash\nsupabase functions new cloudflare-turnstile\n```\n\nAnd add the code to the `index.ts` file:\n\n```ts index.ts\nimport { corsHeaders } from '../_shared/cors.ts'\n\nconsole.log('Hello from Cloudflare Trunstile!')\n\nfunction ips(req: Request) {\n return req.headers.get('x-forwarded-for')?.split(/\\s*,\\s*/)\n}\n\nDeno.serve(async (req) => {\n // This is needed if you're planning to invoke your function from a browser.\n if (req.method === 'OPTIONS') {\n return new Response('ok', { headers: corsHeaders })\n }\n\n const { token } = await req.json()\n const clientIps = ips(req) || ['']\n const ip = clientIps[0]\n\n // Validate the token by calling the\n // \"/siteverify\" API endpoint.\n let formData = new FormData()\n formData.append('secret', Deno.env.get('CLOUDFLARE_SECRET_KEY') ?? '')\n formData.append('response', token)\n formData.append('remoteip', ip)\n\n const url = 'https://challenges.cloudflare.com/turnstile/v0/siteverify'\n const result = await fetch(url, {\n body: formData,\n method: 'POST',\n })\n\n const outcome = await result.json()\n console.log(outcome)\n if (outcome.success) {\n return new Response('success', { headers: corsHeaders })\n }\n return new Response('failure', { headers: corsHeaders })\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy the server-side validation Edge Functions",
          "content": "- https://developers.cloudflare.com/turnstile/get-started/server-side-validation/\n\n```bash\nsupabase functions deploy cloudflare-turnstile\nsupabase secrets set CLOUDFLARE_SECRET_KEY=your_secret_key\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Invoke the function from your site",
          "content": "```js\nconst { data, error } = await supabase.functions.invoke('cloudflare-turnstile', {\n body: { token },\n})\n```",
          "level": 2
        }
      ],
      "wordCount": 243,
      "characterCount": 2218
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-discord-bot",
      "identifier": "functions-examples-discord-bot",
      "name": "Building a Discord Bot",
      "description": "Building a Slash Command Discord Bot with Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/discord-bot",
      "dateModified": "2025-06-13T12:45:11.300216",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/discord-bot.mdx",
      "frontmatter": {
        "id": "examples-discord-bot",
        "title": "Building a Discord Bot",
        "description": "Building a Slash Command Discord Bot with Edge Functions.",
        "video": "https://www.youtube.com/v/J24Bvo_m7DM"
      },
      "sections": [
        {
          "type": "section",
          "title": "Create an application on Discord Developer portal",
          "content": "1. Go to [https://discord.com/developers/applications](https://discord.com/developers/applications) (login using your discord account if required).\n2. Click on **New Application** button available at left side of your profile picture.\n3. Name your application and click on **Create**.\n4. Go to **Bot** section, click on **Add Bot**, and finally on **Yes, do it!** to confirm.\n\nA new application is created which will hold our Slash Command. Don't close the tab as we need information from this application page throughout our development.\n\nBefore we can write some code, we need to curl a discord endpoint to register a Slash Command in our app.\n\nFill `DISCORD_BOT_TOKEN` with the token available in the **Bot** section and `CLIENT_ID` with the ID available on the **General Information** section of the page and run the command on your terminal.\n\n```bash\nBOT_TOKEN='replace_me_with_bot_token'\nCLIENT_ID='replace_me_with_client_id'\ncurl -X POST \\\n-H 'Content-Type: application/json' \\\n-H \"Authorization: Bot $BOT_TOKEN\" \\\n-d '{\"name\":\"hello\",\"description\":\"Greet a person\",\"options\":[{\"name\":\"name\",\"description\":\"The name of the person\",\"type\":3,\"required\":true}]}' \\\n\"https://discord.com/api/v8/applications/$CLIENT_ID/commands\"\n```\n\nThis will register a Slash Command named `hello` that accepts a parameter named `name` of type string.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Code",
          "content": "```ts index.ts\n// Sift is a small routing library that abstracts away details like starting a\n// listener on a port, and provides a simple function (serve) that has an API\n// to invoke a function for a specific path.\nimport { json, serve, validateRequest } from 'https://deno.land/x/sift@0.6.0/mod.ts'\n// TweetNaCl is a cryptography library that we use to verify requests\n// from Discord.\nimport nacl from 'https://cdn.skypack.dev/tweetnacl@v1.0.3?dts'\n\nenum DiscordCommandType {\n Ping = 1,\n ApplicationCommand = 2,\n}\n\n// For all requests to \"/\" endpoint, we want to invoke home() handler.\nserve({\n '/discord-bot': home,\n})\n\n// The main logic of the Discord Slash Command is defined in this function.\nasync function home(request: Request) {\n // validateRequest() ensures that a request is of POST method and\n // has the following headers.\n const { error } = await validateRequest(request, {\n POST: {\n headers: ['X-Signature-Ed25519', 'X-Signature-Timestamp'],\n },\n })\n if (error) {\n return json({ error: error.message }, { status: error.status })\n }\n\n // verifySignature() verifies if the request is coming from Discord.\n // When the request's signature is not valid, we return a 401 and this is\n // important as Discord sends invalid requests to test our verification.\n const { valid, body } = await verifySignature(request)\n if (!valid) {\n return json(\n { error: 'Invalid request' },\n {\n status: 401,\n }\n )\n }\n\n const { type = 0, data = { options: [] } } = JSON.parse(body)\n // Discord performs Ping interactions to test our application.\n // Type 1 in a request implies a Ping interaction.\n if (type === DiscordCommandType.Ping) {\n return json({\n type: 1, // Type 1 in a response is a Pong interaction response type.\n })\n }\n\n // Type 2 in a request is an ApplicationCommand interaction.\n // It implies that a user has issued a command.\n if (type === DiscordCommandType.ApplicationCommand) {\n const { value } = data.options.find(\n (option: { name: string; value: string }) => option.name === 'name'\n )\n return json({\n // Type 4 responds with the below message retaining the user's\n // input at the top.\n type: 4,\n data: {\n content: `Hello, ${value}!`,\n },\n })\n }\n\n // We will return a bad request error as a valid Discord request\n // shouldn't reach here.\n return json({ error: 'bad request' }, { status: 400 })\n}\n\n/** Verify whether the request is coming from Discord. */\nasync function verifySignature(request: Request): Promise {\n const PUBLIC_KEY = Deno.env.get('DISCORD_PUBLIC_KEY')!\n // Discord sends these headers with every request.\n const signature = request.headers.get('X-Signature-Ed25519')!\n const timestamp = request.headers.get('X-Signature-Timestamp')!\n const body = await request.text()\n const valid = nacl.sign.detached.verify(\n new TextEncoder().encode(timestamp + body),\n hexToUint8Array(signature),\n hexToUint8Array(PUBLIC_KEY)\n )\n\n return { valid, body }\n}\n\n/** Converts a hexadecimal string to Uint8Array. */\nfunction hexToUint8Array(hex: string) {\n return new Uint8Array(hex.match(/.{1,2}/g)!.map((val) => parseInt(val, 16)))\n}\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy the slash command handler",
          "content": "```bash\nsupabase functions deploy discord-bot --no-verify-jwt\nsupabase secrets set DISCORD_PUBLIC_KEY=your_public_key\n```\n\nNavigate to your Function details in the Supabase Dashboard to get your Endpoint URL.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Configure Discord application to use our URL as interactions endpoint URL",
          "content": "1. Go back to your application (Greeter) page on Discord Developer Portal\n2. Fill **INTERACTIONS ENDPOINT URL** field with the URL and click on **Save Changes**.\n\nThe application is now ready. Let's proceed to the next section to install it.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Install the slash command on your Discord server",
          "content": "So to use the `hello` Slash Command, we need to install our Greeter application on our Discord server. Here are the steps:\n\n1. Go to **OAuth2** section of the Discord application page on Discord Developer Portal\n2. Select `applications.commands` scope and click on the **Copy** button below.\n3. Now paste and visit the URL on your browser. Select your server and click on **Authorize**.\n\nOpen Discord, type `/Promise` and press **Enter**.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Run locally",
          "content": "```bash\nsupabase functions serve discord-bot --no-verify-jwt --env-file ./supabase/.env.local\nngrok http 54321\n```",
          "level": 2
        }
      ],
      "wordCount": 810,
      "characterCount": 5653
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-elevenlabs-generate-speech-stream",
      "identifier": "functions-examples-elevenlabs-generate-speech-stream",
      "name": "Streaming Speech with ElevenLabs",
      "description": "",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/elevenlabs-generate-speech-stream",
      "dateModified": "2025-06-13T12:45:11.300465",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/elevenlabs-generate-speech-stream.mdx",
      "frontmatter": {
        "title": "Streaming Speech with ElevenLabs",
        "subtitle": "Generate and stream speech through Supabase Edge Functions. Store speech in Supabase Storage and cache responses via built-in CDN.",
        "tocVideo": "4Roog4PAmZ8"
      },
      "sections": [
        {
          "type": "section",
          "title": "Introduction",
          "content": "In this tutorial you will learn how to build an edge API to generate, stream, store, and cache speech using Supabase Edge Functions, Supabase Storage, and [ElevenLabs text to speech API](https://elevenlabs.io/text-to-speech).\n\n Find the [example project on\n GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/supabase/stream-and-cache-storage).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Requirements",
          "content": "- An ElevenLabs account with an [API key](/app/settings/api-keys).\n- A [Supabase](https://supabase.com) account (you can sign up for a free account via [database.new](https://database.new)).\n- The [Supabase CLI](https://supabase.com/docs/guides/local-development) installed on your machine.\n- The [Deno runtime](https://docs.deno.com/runtime/getting_started/installation/) installed on your machine and optionally [setup in your favourite IDE](https://docs.deno.com/runtime/getting_started/setup_your_environment).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create a Supabase project locally",
          "content": "After installing the [Supabase CLI](https://supabase.com/docs/guides/local-development), run the following command to create a new Supabase project locally:\n\n```bash\nsupabase init\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Configure the storage bucket",
          "content": "You can configure the Supabase CLI to automatically generate a storage bucket by adding this configuration in the `config.toml` file:\n\n```toml ./supabase/config.toml\n[storage.buckets.audio]\npublic = false\nfile_size_limit = \"50MiB\"\nallowed_mime_types = [\"audio/mp3\"]\nobjects_path = \"./audio\"\n```\n\n Upon running `supabase start` this will create a new storage bucket in your local Supabase\n project. Should you want to push this to your hosted Supabase project, you can run `supabase seed\n buckets --linked`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Configure background tasks for Supabase Edge Functions",
          "content": "To use background tasks in Supabase Edge Functions when developing locally, you need to add the following configuration in the `config.toml` file:\n\n```toml ./supabase/config.toml\n[edge_runtime]\npolicy = \"per_worker\"\n```\n\n When running with `per_worker` policy, Function won't auto-reload on edits. You will need to\n manually restart it by running `supabase functions serve`.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a Supabase Edge Function for speech generation",
          "content": "Create a new Edge Function by running the following command:\n\n```bash\nsupabase functions new text-to-speech\n```\n\nIf you're using VS Code or Cursor, select `y` when the CLI prompts \"Generate VS Code settings for Deno? [y/N]\"!",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set up the environment variables",
          "content": "Within the `supabase/functions` directory, create a new `.env` file and add the following variables:\n\n```env supabase/functions/.env",
          "level": 3
        },
        {
          "type": "section",
          "title": "Find / create an API key at https://elevenlabs.io/app/settings/api-keys",
          "content": "ELEVENLABS_API_KEY=your_api_key\n```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Dependencies",
          "content": "The project uses a couple of dependencies:\n\n- The [@supabase/supabase-js](https://supabase.com/docs/reference/javascript) library to interact with the Supabase database.\n- The ElevenLabs [JavaScript SDK](/docs/quickstart) to interact with the text-to-speech API.\n- The open-source [object-hash](https://www.npmjs.com/package/object-hash) to generate a hash from the request parameters.\n\nSince Supabase Edge Function uses the [Deno runtime](https://deno.land/), you don't need to install the dependencies, rather you can [import](https://docs.deno.com/examples/npm/) them via the `npm:` prefix.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Code the Supabase Edge Function",
          "content": "In your newly created `supabase/functions/text-to-speech/index.ts` file, add the following code:\n\n```ts supabase/functions/text-to-speech/index.ts\n// Setup type definitions for built-in Supabase Runtime APIs\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts'\nimport { createClient } from 'npm:@supabase/supabase-js@2'\nimport { ElevenLabsClient } from 'npm:elevenlabs@1.52.0'\nimport * as hash from 'npm:object-hash'\n\nconst supabase = createClient(\n Deno.env.get('SUPABASE_URL')!,\n Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n)\n\nconst client = new ElevenLabsClient({\n apiKey: Deno.env.get('ELEVENLABS_API_KEY'),\n})\n\n// Upload audio to Supabase Storage in a background task\nasync function uploadAudioToStorage(stream: ReadableStream, requestHash: string) {\n const { data, error } = await supabase.storage\n .from('audio')\n .upload(`${requestHash}.mp3`, stream, {\n contentType: 'audio/mp3',\n })\n\n console.log('Storage upload result', { data, error })\n}\n\nDeno.serve(async (req) => {\n // To secure your function for production, you can for example validate the request origin,\n // or append a user access token and validate it with Supabase Auth.\n console.log('Request origin', req.headers.get('host'))\n const url = new URL(req.url)\n const params = new URLSearchParams(url.search)\n const text = params.get('text')\n const voiceId = params.get('voiceId') ?? 'JBFqnCBsd6RMkjVDRZzb'\n\n const requestHash = hash.MD5({ text, voiceId })\n console.log('Request hash', requestHash)\n\n // Check storage for existing audio file\n const { data } = await supabase.storage.from('audio').createSignedUrl(`${requestHash}.mp3`, 60)\n\n if (data) {\n console.log('Audio file found in storage', data)\n const storageRes = await fetch(data.signedUrl)\n if (storageRes.ok) return storageRes\n }\n\n if (!text) {\n return new Response(JSON.stringify({ error: 'Text parameter is required' }), {\n status: 400,\n headers: { 'Content-Type': 'application/json' },\n })\n }\n\n try {\n console.log('ElevenLabs API call')\n const response = await client.textToSpeech.convertAsStream(voiceId, {\n output_format: 'mp3_44100_128',\n model_id: 'eleven_multilingual_v2',\n text,\n })\n\n const stream = new ReadableStream({\n async start(controller) {\n for await (const chunk of response) {\n controller.enqueue(chunk)\n }\n controller.close()\n },\n })\n\n // Branch stream to Supabase Storage\n const [browserStream, storageStream] = stream.tee()\n\n // Upload to Supabase Storage in the background\n EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash))\n\n // Return the streaming response immediately\n return new Response(browserStream, {\n headers: {\n 'Content-Type': 'audio/mpeg',\n },\n })\n } catch (error) {\n console.log('error', { error })\n return new Response(JSON.stringify({ error: error.message }), {\n status: 500,\n headers: { 'Content-Type': 'application/json' },\n })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Run locally",
          "content": "To run the function locally, run the following commands:\n\n```bash\nsupabase start\n```\n\nOnce the local Supabase stack is up and running, run the following command to start the function and observe the logs:\n\n```bash\nsupabase functions serve\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Try it out",
          "content": "Navigate to `http://127.0.0.1:54321/functions/v1/text-to-speech?text=hello%20world` to hear the function in action.\n\nAfterwards, navigate to `http://127.0.0.1:54323/project/default/storage/buckets/audio` to see the audio file in your local Supabase Storage bucket.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deploy to Supabase",
          "content": "If you haven't already, create a new Supabase account at [database.new](https://database.new) and link the local project to your Supabase account:\n\n```bash\nsupabase link\n```\n\nOnce done, run the following command to deploy the function:\n\n```bash\nsupabase functions deploy\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Set the function secrets",
          "content": "Now that you have all your secrets set locally, you can run the following command to set the secrets in your Supabase project:\n\n```bash\nsupabase secrets set --env-file supabase/functions/.env\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Test the function",
          "content": "The function is designed in a way that it can be used directly as a source for an `` element.\n\n```html\n\n```\n\nYou can find an example frontend implementation in the complete code example on [GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/supabase/stream-and-cache-storage/src/pages/Index.tsx).",
          "level": 2
        }
      ],
      "wordCount": 903,
      "characterCount": 7622
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-elevenlabs-transcribe-speech",
      "identifier": "functions-examples-elevenlabs-transcribe-speech",
      "name": "Transcription Telegram Bot",
      "description": "",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/elevenlabs-transcribe-speech",
      "dateModified": "2025-06-13T12:45:11.300753",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/elevenlabs-transcribe-speech.mdx",
      "frontmatter": {
        "title": "Transcription Telegram Bot",
        "subtitle": "Build a Telegram bot that transcribes audio and video messages in 99 languages using TypeScript with Deno in Supabase Edge Functions.",
        "tocVideo": "CE4iPp7kd7Q"
      },
      "sections": [
        {
          "type": "section",
          "title": "Introduction",
          "content": "In this tutorial you will learn how to build a Telegram bot that transcribes audio and video messages in 99 languages using TypeScript and the ElevenLabs Scribe model via the [speech to text API](https://elevenlabs.io/speech-to-text).\n\nTo check out what the end result will look like, you can test out the [t.me/ElevenLabsScribeBot](https://t.me/ElevenLabsScribeBot)\n\n Find the [example project on\n GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/speech-to-text/telegram-transcription-bot).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Requirements",
          "content": "- An ElevenLabs account with an [API key](/app/settings/api-keys).\n- A [Supabase](https://supabase.com) account (you can sign up for a free account via [database.new](https://database.new)).\n- The [Supabase CLI](https://supabase.com/docs/guides/local-development) installed on your machine.\n- The [Deno runtime](https://docs.deno.com/runtime/getting_started/installation/) installed on your machine and optionally [setup in your favourite IDE](https://docs.deno.com/runtime/getting_started/setup_your_environment).\n- A [Telegram](https://telegram.org) account.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Register a Telegram bot",
          "content": "Use the [BotFather](https://t.me/BotFather) to create a new Telegram bot. Run the `/newbot` command and follow the instructions to create a new bot. At the end, you will receive your secret bot token. Note it down securely for the next step.\n\n![BotFather](/docs/img/guides/functions/elevenlabs/bot-father.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a Supabase project locally",
          "content": "After installing the [Supabase CLI](https://supabase.com/docs/guides/local-development), run the following command to create a new Supabase project locally:\n\n```bash\nsupabase init\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a database table to log the transcription results",
          "content": "Next, create a new database table to log the transcription results:\n\n```bash\nsupabase migrations new init\n```\n\nThis will create a new migration file in the `supabase/migrations` directory. Open the file and add the following SQL:\n\n```sql supabase/migrations/init.sql\nCREATE TABLE IF NOT EXISTS transcription_logs (\n id BIGSERIAL PRIMARY KEY,\n file_type VARCHAR NOT NULL,\n duration INTEGER NOT NULL,\n chat_id BIGINT NOT NULL,\n message_id BIGINT NOT NULL,\n username VARCHAR,\n transcript TEXT,\n language_code VARCHAR,\n created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n error TEXT\n);\n\nALTER TABLE transcription_logs ENABLE ROW LEVEL SECURITY;\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a Supabase Edge Function to handle Telegram webhook requests",
          "content": "Next, create a new Edge Function to handle Telegram webhook requests:\n\n```bash\nsupabase functions new scribe-bot\n```\n\nIf you're using VS Code or Cursor, select `y` when the CLI prompts \"Generate VS Code settings for Deno? [y/N]\"!",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set up the environment variables",
          "content": "Within the `supabase/functions` directory, create a new `.env` file and add the following variables:\n\n```env supabase/functions/.env",
          "level": 3
        },
        {
          "type": "section",
          "title": "Find / create an API key at https://elevenlabs.io/app/settings/api-keys",
          "content": "ELEVENLABS_API_KEY=your_api_key",
          "level": 1
        },
        {
          "type": "section",
          "title": "The bot token you received from the BotFather.",
          "content": "TELEGRAM_BOT_TOKEN=your_bot_token",
          "level": 1
        },
        {
          "type": "section",
          "title": "A random secret chosen by you to secure the function.",
          "content": "FUNCTION_SECRET=random_secret\n```",
          "level": 1
        },
        {
          "type": "section",
          "title": "Dependencies",
          "content": "The project uses a couple of dependencies:\n\n- The open-source [grammY Framework](https://grammy.dev/) to handle the Telegram webhook requests.\n- The [@supabase/supabase-js](https://supabase.com/docs/reference/javascript) library to interact with the Supabase database.\n- The ElevenLabs [JavaScript SDK](/docs/quickstart) to interact with the speech-to-text API.\n\nSince Supabase Edge Function uses the [Deno runtime](https://deno.land/), you don't need to install the dependencies, rather you can [import](https://docs.deno.com/examples/npm/) them via the `npm:` prefix.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Code the Telegram bot",
          "content": "In your newly created `scribe-bot/index.ts` file, add the following code:\n\n```ts supabase/functions/scribe-bot/index.ts\nimport { Bot, webhookCallback } from 'https://deno.land/x/grammy@v1.34.0/mod.ts'\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts'\nimport { createClient } from 'npm:@supabase/supabase-js@2'\nimport { ElevenLabsClient } from 'npm:elevenlabs@1.50.5'\n\nconsole.log(`Function \"elevenlabs-scribe-bot\" up and running!`)\n\nconst elevenLabsClient = new ElevenLabsClient({\n apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',\n})\n\nconst supabase = createClient(\n Deno.env.get('SUPABASE_URL') || '',\n Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''\n)\n\nasync function scribe({\n fileURL,\n fileType,\n duration,\n chatId,\n messageId,\n username,\n}: {\n fileURL: string\n fileType: string\n duration: number\n chatId: number\n messageId: number\n username: string\n}) {\n let transcript: string | null = null\n let languageCode: string | null = null\n let errorMsg: string | null = null\n try {\n const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer())\n const sourceBlob = new Blob([sourceFileArrayBuffer], {\n type: fileType,\n })\n\n const scribeResult = await elevenLabsClient.speechToText.convert({\n file: sourceBlob,\n model_id: 'scribe_v1',\n tag_audio_events: false,\n })\n\n transcript = scribeResult.text\n languageCode = scribeResult.language_code\n\n // Reply to the user with the transcript\n await bot.api.sendMessage(chatId, transcript, {\n reply_parameters: { message_id: messageId },\n })\n } catch (error) {\n errorMsg = error.message\n console.log(errorMsg)\n await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {\n reply_parameters: { message_id: messageId },\n })\n }\n // Write log to Supabase.\n const logLine = {\n file_type: fileType,\n duration,\n chat_id: chatId,\n message_id: messageId,\n username,\n language_code: languageCode,\n error: errorMsg,\n }\n console.log({ logLine })\n await supabase.from('transcription_logs').insert({ ...logLine, transcript })\n}\n\nconst telegramBotToken = Deno.env.get('TELEGRAM_BOT_TOKEN')\nconst bot = new Bot(telegramBotToken || '')\nconst startMessage = `Welcome to the ElevenLabs Scribe Bot\\\\! I can transcribe speech in 99 languages with super high accuracy\\\\!\n \\nTry it out by sending or forwarding me a voice message, video, or audio file\\\\!\n \\n[Learn more about Scribe](https://elevenlabs.io/speech-to-text) or [build your own bot](https://elevenlabs.io/docs/cookbooks/speech-to-text/telegram-bot)\\\\!\n `\nbot.command('start', (ctx) => ctx.reply(startMessage.trim(), { parse_mode: 'MarkdownV2' }))\n\nbot.on([':voice', ':audio', ':video'], async (ctx) => {\n try {\n const file = await ctx.getFile()\n const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`\n const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio\n\n if (!fileMeta) {\n return ctx.reply('No video|audio|voice metadata found. Please try again.')\n }\n\n // Run the transcription in the background.\n EdgeRuntime.waitUntil(\n scribe({\n fileURL,\n fileType: fileMeta.mime_type!,\n duration: fileMeta.duration,\n chatId: ctx.chat.id,\n messageId: ctx.message?.message_id!,\n username: ctx.from?.username || '',\n })\n )\n\n // Reply to the user immediately to let them know we received their file.\n return ctx.reply('Received. Scribing...')\n } catch (error) {\n console.error(error)\n return ctx.reply(\n 'Sorry, there was an error getting the file. Please try again with a smaller file!'\n )\n }\n})\n\nconst handleUpdate = webhookCallback(bot, 'std/http')\n\nDeno.serve(async (req) => {\n try {\n const url = new URL(req.url)\n if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {\n return new Response('not allowed', { status: 405 })\n }\n\n return await handleUpdate(req)\n } catch (err) {\n console.error(err)\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy to Supabase",
          "content": "If you haven't already, create a new Supabase account at [database.new](https://database.new) and link the local project to your Supabase account:\n\n```bash\nsupabase link\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Apply the database migrations",
          "content": "Run the following command to apply the database migrations from the `supabase/migrations` directory:\n\n```bash\nsupabase db push\n```\n\nNavigate to the [table editor](https://supabase.com/dashboard/project/_/editor) in your Supabase dashboard and you should see and empty `transcription_logs` table.\n\n![Empty table](/docs/img/guides/functions/elevenlabs/supa-empty-table.png)\n\nLastly, run the following command to deploy the Edge Function:\n\n```bash\nsupabase functions deploy --no-verify-jwt scribe-bot\n```\n\nNavigate to the [Edge Functions view](https://supabase.com/dashboard/project/_/functions) in your Supabase dashboard and you should see the `scribe-bot` function deployed. Make a note of the function URL as you'll need it later, it should look something like `https://.functions.supabase.co/scribe-bot`.\n\n![Edge Function deployed](/docs/img/guides/functions/elevenlabs/supa-edge-function-deployed.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set up the webhook",
          "content": "Set your bot's webhook URL to `https://.functions.supabase.co/telegram-bot` (Replacing `` with respective values). In order to do that, run a GET request to the following URL (in your browser, for example):\n\n```\nhttps://api.telegram.org/bot/setWebhook?url=https://.supabase.co/functions/v1/scribe-bot?secret=\n```\n\nNote that the `FUNCTION_SECRET` is the secret you set in your `.env` file.\n\n![Set webhook](/docs/img/guides/functions/elevenlabs/set-webhook.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Set the function secrets",
          "content": "Now that you have all your secrets set locally, you can run the following command to set the secrets in your Supabase project:\n\n```bash\nsupabase secrets set --env-file supabase/functions/.env\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Test the bot",
          "content": "Finally you can test the bot by sending it a voice message, audio or video file.\n\n![Test the bot](/docs/img/guides/functions/elevenlabs/test-bot.png)\n\nAfter you see the transcript as a reply, navigate back to your table editor in the Supabase dashboard and you should see a new row in your `transcription_logs` table.\n\n![New row in table](/docs/img/guides/functions/elevenlabs/supa-new-row.png)",
          "level": 2
        }
      ],
      "wordCount": 1150,
      "characterCount": 9849
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-github-actions",
      "identifier": "functions-examples-github-actions",
      "name": "GitHub Actions",
      "description": "Deploying Edge Functions with GitHub Actions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/github-actions",
      "dateModified": "2025-06-13T12:45:11.300851",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/github-actions.mdx",
      "frontmatter": {
        "id": "examples-github-actions",
        "title": "GitHub Actions",
        "description": "Deploying Edge Functions with GitHub Actions.",
        "video": "https://www.youtube.com/v/l2KlzGrhB6w"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Use the Supabase CLI together with GitHub Actions to automatically deploy our Supabase Edge Functions. [View on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/github-action-deploy).\n\n```yaml deploy.yaml\nname: Deploy Function\n\non:\n push:\n branches:\n - main\n workflow_dispatch:\n\njobs:\n deploy:\n runs-on: ubuntu-latest\n\n env:\n SUPABASE_ACCESS_TOKEN: YOUR_SUPABASE_ACCESS_TOKEN\n PROJECT_ID: YOUR_SUPABASE_PROJECT_ID\n\n steps:\n - uses: actions/checkout@v4\n\n - uses: supabase/setup-cli@v1\n with:\n version: latest\n\n - run: supabase functions deploy --project-ref $PROJECT_ID\n```\n\nSince Supabase CLI [v1.62.0](https://github.com/supabase/cli/releases/tag/v1.62.0) you can deploy all functions with a single command.\n\nIndividual function configuration like [JWT verification](/docs/guides/cli/config#functions.function_name.verify_jwt) and [import map location](/docs/guides/cli/config#functions.function_name.import_map) can be set via the `config.toml` file.\n\n```toml\n[functions.hello-world]\nverify_jwt = false\n```"
        }
      ],
      "wordCount": 92,
      "characterCount": 1059
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-image-manipulation",
      "identifier": "functions-examples-image-manipulation",
      "name": "Image Manipulation",
      "description": "How to optimize and transform images using Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/image-manipulation",
      "dateModified": "2025-06-13T12:45:11.300952",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/image-manipulation.mdx",
      "frontmatter": {
        "title": "Image Manipulation",
        "description": "How to optimize and transform images using Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Storage has [out-of-the-box support](https://supabase.com/docs/guides/storage/serving/image-transformations?queryGroups=language&language=js) for the most common image transformations and optimizations you need.\nIf you need to do anything custom beyond what Supabase Storage provides, you can use Edge Functions to write custom image manipulation scripts.\n\nIn this example, we will use [`magick-wasm`](https://github.com/dlemstra/magick-wasm) to perform image manipulations. `magick-wasm` is the WebAssembly port of the popular ImageMagick library and supports processing over 100 file formats.\n\nEdge Functions currently doesn't support image processing libraries such as `Sharp`, which depend on native libraries. Only WASM-based libraries are supported."
        },
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "Make sure you have the latest version of the [Supabase CLI](https://supabase.com/docs/guides/cli#installation) installed.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create the Edge Function",
          "content": "Create a new function locally:\n\n```bash\nsupabase functions new image-blur\n\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Write the function",
          "content": "In this example, we are implementing a function allowing users to upload an image and get a blurred thumbnail.\n\nHere's the implementation in `index.ts` file:",
          "level": 3
        },
        {
          "type": "section",
          "title": "Test it locally",
          "content": "You can test the function locally by running:\n\n```bash\nsupabase start\nsupabase functions serve --no-verify-jwt\n\n```\n\nThen, make a request using `curl` or your favorite API testing tool.\n\n```bash\ncurl --location '' \\\\\n--form 'file=@\"/path/to/image.png\"'\n--output '/path/to/output.png'\n\n```\n\nIf you open the `output.png` file you will find a transformed version of your original image.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deploy to your hosted project",
          "content": "Now, let's deploy the function to your Supabase project.\n\n```bash\nsupabase link\nsupabase functions deploy image-blur\n\n```\n\nHosted Edge Functions have [limits](https://supabase.com/docs/guides/functions/limits) on memory and CPU usage.\n\nIf you try to perform complex image processing or handle large images (> 5MB) your function may return a resource limit exceeded error.",
          "level": 3
        }
      ],
      "wordCount": 260,
      "characterCount": 2013
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-og-image",
      "identifier": "functions-examples-og-image",
      "name": "Generating OG Images",
      "description": "Generate Open Graph images with Deno and Supabase Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/og-image",
      "dateModified": "2025-06-13T12:45:11.301026",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/og-image.mdx",
      "frontmatter": {
        "id": "examples-og-image",
        "title": "Generating OG Images",
        "description": "Generate Open Graph images with Deno and Supabase Edge Functions.",
        "video": "https://www.youtube.com/v/jZgyOJGWayQ"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Generate Open Graph images with Deno and Supabase Edge Functions. [View on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/opengraph)."
        },
        {
          "type": "section",
          "title": "Code",
          "content": "Create a `handler.tsx` file to construct the OG image in React:\n\n```tsx handler.tsx\nimport React from 'https://esm.sh/react@18.2.0'\nimport { ImageResponse } from 'https://deno.land/x/og_edge@0.0.4/mod.ts'\n\nexport default function handler(req: Request) {\n return new ImageResponse(\n (\n \n Hello OG Image!\n \n )\n )\n}\n```\n\nCreate an `index.ts` file to execute the handler on incoming requests:\n\n```ts index.ts\nimport handler from './handler.tsx'\n\nconsole.log('Hello from og-image Function!')\n\nDeno.serve(handler)\n```",
          "level": 2
        }
      ],
      "wordCount": 78,
      "characterCount": 708
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-push-notifications",
      "identifier": "functions-examples-push-notifications",
      "name": "Sending Push Notifications",
      "description": "Send Push Notifications to your React Native iOS and Android apps using Expo.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/push-notifications",
      "dateModified": "2025-06-13T12:45:11.301377",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/push-notifications.mdx",
      "frontmatter": {
        "title": "Sending Push Notifications",
        "description": "Send Push Notifications to your React Native iOS and Android apps using Expo.",
        "tocVideo": "xYRbYG77M_o"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Push notifications are an important part of any mobile app. They allow you to send notifications to your users even when they are not using your app. This guide will show you how to send push notifications to different mobile app frameworks from your Supabase edge functions.\n\n [Expo](https://docs.expo.dev/push-notifications/overview/) makes implementing push notifications easy. All the hassle with device information and communicating with Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNs) is done behind the scenes. This allows you to treat Android and iOS notifications in the same way and save time both on the frontend and backend.\n\n Find the example code on [GitHub](https://github.com/supabase/supabase/blob/master/examples/user-management/expo-push-notifications/).\n\n ## Supabase setup\n\n - [Create a new Supabase project](https://database.new).\n - Link your project: `supabase link --project-ref your-supabase-project-ref`\n - Start Supabase locally: `supabase start`\n - Push up the schema: `supabase db push` (schema is defined in [supabase/migrations](https://github.com/supabase/supabase/blob/master/examples/user-management/expo-push-notifications/supabase/migrations/))\n\n ## Expo setup\n\n To utilize Expo's push notification service, you must configure your app by installing a set of libraries, implementing functions to handle notifications, and setting up credentials for Android and iOS. Follow the official [Expo Push Notifications Setup Guide](https://docs.expo.dev/push-notifications/push-notifications-setup/) to get the credentials for Android and iOS. This project uses [Expo's EAS build](https://docs.expo.dev/build/introduction/) service to simplify this part.\n\n 1. Install the dependencies: `npm i`\n 1. Create a [new Expo project](https://expo.dev/accounts/_/projects)\n 1. Link this app to your project: `npm install --global eas-cli && eas init --id your-expo-project-id`\n 1. [Create a build for your physical device](https://docs.expo.dev/develop/development-builds/create-a-build/#create-a-build-for-the-device)\n 1. Start the development server for your project: `npx expo start --dev-client`\n 1. Scan the QR code shown in the terminal with your physical device.\n 1. Sign up/in to create a user in Supabase Auth.\n\n ## Enhanced security for push notifications\n\n 1. Navigate to your [Expo Access Token Settings](https://expo.dev/accounts/_/settings/access-tokens).\n 1. Create a new token for usage in Supabase Edge Functions.\n 1. Toggle on \"Enhanced Security for Push Notifications\".\n 1. Create the local `.env` file: `cp .env.local.example .env.local`\n 1. In the newly created `.env.local` file, set your `EXPO_ACCESS_TOKEN` value.\n\n ## Deploy the Supabase Edge Function\n\n The database webhook handler to send push notifications is located in [supabase/functions/push/index.ts](https://github.com/supabase/supabase/blob/master/examples/user-management/expo-push-notifications/supabase/functions/push/index.ts). Deploy the function to your linked project and set the `EXPO_ACCESS_TOKEN` secret.\n\n 1. `supabase functions deploy push`\n 1. `supabase secrets set --env-file .env.local`\n\n ```ts supabase/functions/push/index.ts\n import { createClient } from 'npm:@supabase/supabase-js@2'\n\n console.log('Hello from Functions!')\n\n interface Notification {\n id: string\n user_id: string\n body: string\n }\n\n interface WebhookPayload {\n type: 'INSERT' | 'UPDATE' | 'DELETE'\n table: string\n record: Notification\n schema: 'public'\n old_record: null | Notification\n }\n\n const supabase = createClient(\n Deno.env.get('SUPABASE_URL')!,\n Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n )\n\n Deno.serve(async (req) => {\n const payload: WebhookPayload = await req.json()\n const { data } = await supabase\n .from('profiles')\n .select('expo_push_token')\n .eq('id', payload.record.user_id)\n .single()\n\n const res = await fetch('https://exp.host/--/api/v2/push/send', {\n method: 'POST',\n headers: {\n 'Content-Type': 'application/json',\n Authorization: `Bearer ${Deno.env.get('EXPO_ACCESS_TOKEN')}`,\n },\n body: JSON.stringify({\n to: data?.expo_push_token,\n sound: 'default',\n body: payload.record.body,\n }),\n }).then((res) => res.json())\n\n return new Response(JSON.stringify(res), {\n headers: { 'Content-Type': 'application/json' },\n })\n })\n ```\n\n ## Create the database webhook\n\n Navigate to the [Database Webhooks settings](https://supabase.com/dashboard/project/_/integrations/webhooks/overview) in your Supabase Dashboard.\n\n 1. Enable and create a new hook.\n 1. Conditions to fire webhook: Select the `notifications` table and tick the `Insert` event.\n 1. Webhook configuration: Supabase Edge Functions.\n 1. Edge Function: Select the `push` edge function and leave the method as `POST` and timeout as `1000`.\n 1. HTTP Headers: Click \"Add new header\" > \"Add auth header with service key\" and leave Content-type: `application/json`.\n 1. Click \"Create webhook\".\n\n ## Send push notification\n\n 1. Navigate to the [table editor](https://supabase.com/dashboard/project/_/editor) in your Supabase Dashboard.\n 1. In your `notifications` table, insert a new row.\n 1. Watch the magic happen 🪄\n\n Firebase Cloud Messaging (FCM) is a push notification service offered by Google that allows you to send push notifications to your users' devices on iOS, Android, and Web.\n\n This guide will show you how to send push notifications to your app when a new row is inserted into a table using FCM, Supabase Edge Functions, and database web hooks.\n\n ## Supabase setup\n\n We will create two tables. One to store the user's FCM token and a `notifications` table. The edge function will be triggered when a new row is inserted into the `notifications` table and sends a push notification to the user.\n\n Create a `notifications` table. Also create a `profiles` table if you don't already have one:\n\n ```sql\n create table public.profiles (\n id uuid references auth.users(id) not null primary key,\n fcm_token text\n );\n\n create table public.notifications (\n id uuid not null default gen_random_uuid(),\n user_id uuid references auth.users(id) not null,\n created_at timestamp with time zone not null default now(),\n body text not null\n );\n ```\n\n If you already have a `profiles` table, alter it to include an `fcm_token` column:\n\n ```sql\n ALTER TABLE public.profiles\n ADD COLUMN fcm_token text;\n ```\n\n With the tables created, we can now create the edge function that will be triggered by database webhook when a notification is inserted.\n\n Create the function using the following command:\n ```bash\n # Initialize Supabase in your working directory\n supabase init\n # Create the push edge function\n supabase functions new push\n ```\n\n Add the following code to `supabase/functions/push/index.ts`:\n\n ```ts supabase/functions/push/index.ts\n import { createClient } from 'npm:@supabase/supabase-js@2'\n import { JWT } from 'npm:google-auth-library@9'\n import serviceAccount from '../service-account.json' with { type: 'json' }\n\n interface Notification {\n id: string\n user_id: string\n body: string\n }\n\n interface WebhookPayload {\n type: 'INSERT'\n table: string\n record: Notification\n schema: 'public'\n }\n\n const supabase = createClient(\n Deno.env.get('SUPABASE_URL')!,\n Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n )\n\n Deno.serve(async (req) => {\n const payload: WebhookPayload = await req.json()\n\n const { data } = await supabase\n .from('profiles')\n .select('fcm_token')\n .eq('id', payload.record.user_id)\n .single()\n\n const fcmToken = data!.fcm_token as string\n\n const accessToken = await getAccessToken({\n clientEmail: serviceAccount.client_email,\n privateKey: serviceAccount.private_key,\n })\n\n const res = await fetch(\n `https://fcm.googleapis.com/v1/projects/${serviceAccount.project_id}/messages:send`,\n {\n method: 'POST',\n headers: {\n 'Content-Type': 'application/json',\n Authorization: `Bearer ${accessToken}`,\n },\n body: JSON.stringify({\n message: {\n token: fcmToken,\n notification: {\n title: `Notification from Supabase`,\n body: payload.record.body,\n },\n },\n }),\n }\n )\n\n const resData = await res.json()\n if (res.status => {\n return new Promise((resolve, reject) => {\n const jwtClient = new JWT({\n email: clientEmail,\n key: privateKey,\n scopes: ['https://www.googleapis.com/auth/firebase.messaging'],\n })\n jwtClient.authorize((err, tokens) => {\n if (err) {\n reject(err)\n return\n }\n resolve(tokens!.access_token!)\n })\n })\n }\n ```\n\n ## FCM setup\n\n 1. Follow the official [FCM Setup Guide](https://firebase.google.com/docs/cloud-messaging) to set up FCM for your client side application.\n 1. Generate a new service account private key from the Firebase console `Project Settings > Service Accounts > Generate new private key`.\n 1. Save the service account private key as `service-account.json` under `supabase/functions` directory.\n\n ## Deploy the function\n\n Deploy the function with the following command:\n\n ```bash\n # Link your local Supabase project to the remote Supabase project\n supabase link\n # Deploy the function\n supabase functions deploy push --no-verify-jwt\n ```\n\n ## Create the database webhook\n\n Navigate to the [Database Webhooks settings](https://supabase.com/dashboard/project/_/database/hooks) in your Supabase Dashboard.\n\n 1. Enable and create a new hook.\n 1. Conditions to fire webhook: Select the `public.notifications` table and tick the `Insert` event.\n 1. Webhook configuration: Supabase Edge Functions.\n 1. Edge Function: Select the `push` edge function and leave the method as `POST` and timeout as `1000`.\n 1. Click \"Create webhook\".\n\n ## Send push notification\n\n 1. Make sure you have a user with an FCM token in the `profiles` table.\n 1. Navigate to the [table editor](https://supabase.com/dashboard/project/_/editor) in your Supabase Dashboard.\n 1. In your `notifications` table, insert a new row.\n 1. Watch the magic happen 🪄"
        }
      ],
      "wordCount": 1195,
      "characterCount": 9744
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-rate-limiting",
      "identifier": "functions-examples-rate-limiting",
      "name": "Rate Limiting Edge Functions",
      "description": "Rate Limiting Edge Functions with Upstash Redis.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/rate-limiting",
      "dateModified": "2025-06-13T12:45:11.301480",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/rate-limiting.mdx",
      "frontmatter": {
        "title": "Rate Limiting Edge Functions",
        "description": "Rate Limiting Edge Functions with Upstash Redis."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "[Redis](https://redis.io/docs/about/) is an open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. It is optimized for atomic operations like incrementing a value, for example for a view counter or rate limiting. We can even rate limit based on the user ID from Supabase Auth!\n\n[Upstash](https://upstash.com/) provides an HTTP/REST based Redis client which is ideal for serverless use-cases and therefore works well with Supabase Edge Functions.\n\nFind the code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/upstash-redis-ratelimit)."
        }
      ],
      "wordCount": 79,
      "characterCount": 659
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-screenshots",
      "identifier": "functions-examples-screenshots",
      "name": "Taking Screenshots with Puppeteer",
      "description": "Take screenshots in Edge Functions with Puppeteer and Browserless.io.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/screenshots",
      "dateModified": "2025-06-13T12:45:11.301539",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/screenshots.mdx",
      "frontmatter": {
        "title": "Taking Screenshots with Puppeteer",
        "description": "Take screenshots in Edge Functions with Puppeteer and Browserless.io."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "[Puppeteer](https://pptr.dev/) is a handy tool to programmatically take screenshots and generate PDFs. However, trying to do so in Edge Functions can be challenging due to the size restrictions. Luckily there is a [serverless browser offering available](https://www.browserless.io/) that we can connect to via WebSockets.\n\nFind the code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/puppeteer)."
        }
      ],
      "wordCount": 48,
      "characterCount": 452
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-semantic-search",
      "identifier": "functions-examples-semantic-search",
      "name": "Semantic Search",
      "description": "Semantic Search with pgvector and Supabase Edge Functions",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/semantic-search",
      "dateModified": "2025-06-13T12:45:11.301785",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/semantic-search.mdx",
      "frontmatter": {
        "id": "function-ai-models",
        "title": "Semantic Search",
        "description": "Semantic Search with pgvector and Supabase Edge Functions",
        "subtitle": "Semantic Search with pgvector and Supabase Edge Functions",
        "tocVideo": "w4Rr_1whU-U"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "[Semantic search](/docs/guides/ai/semantic-search) interprets the meaning behind user queries rather than exact [keywords](/docs/guides/ai/keyword-search). It uses machine learning to capture the intent and context behind the query, handling language nuances like synonyms, phrasing variations, and word relationships.\n\nSince Supabase Edge Runtime [v1.36.0](https://github.com/supabase/edge-runtime/releases/tag/v1.36.0) you can run the [`gte-small` model](https://huggingface.co/Supabase/gte-small) natively within Supabase Edge Functions without any external dependencies! This allows you to generate text embeddings without calling any external APIs!\n\nIn this tutorial you're implementing three parts:\n\n1. A [`generate-embedding`](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions/supabase/functions/generate-embedding/index.ts) database webhook edge function which generates embeddings when a content row is added (or updated) in the [`public.embeddings`](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions/supabase/migrations/20240408072601_embeddings.sql) table.\n2. A [`query_embeddings` Postgres function](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions/supabase/migrations/20240410031515_vector-search.sql) which allows us to perform similarity search from an Edge Function via [Remote Procedure Call (RPC)](https://supabase.com/docs/guides/database/functions?language=js).\n3. A [`search` edge function](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions/supabase/functions/search/index.ts) which generates the embedding for the search term, performs the similarity search via RPC function call, and returns the result.\n\nYou can find the complete example code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/edge-functions)"
        },
        {
          "type": "section",
          "title": "Create the database table and webhook",
          "content": "Given the [following table definition](https://github.com/supabase/supabase/blob/master/examples/ai/edge-functions/supabase/migrations/20240408072601_embeddings.sql):\n\n```sql\ncreate extension if not exists vector with schema extensions;\n\ncreate table embeddings (\n id bigint primary key generated always as identity,\n content text not null,\n embedding vector (384)\n);\nalter table embeddings enable row level security;\n\ncreate index on embeddings using hnsw (embedding vector_ip_ops);\n```\n\nYou can deploy the [following edge function](https://github.com/supabase/supabase/blob/master/examples/ai/edge-functions/supabase/functions/generate-embedding/index.ts) as a [database webhook](/docs/guides/database/webhooks) to generate the embeddings for any text content inserted into the table:\n\n```tsx\nconst model = new Supabase.ai.Session('gte-small')\n\nDeno.serve(async (req) => {\n const payload: WebhookPayload = await req.json()\n const { content, id } = payload.record\n\n // Generate embedding.\n const embedding = await model.run(content, {\n mean_pool: true,\n normalize: true,\n })\n\n // Store in database.\n const { error } = await supabase\n .from('embeddings')\n .update({ embedding: JSON.stringify(embedding) })\n .eq('id', id)\n if (error) console.warn(error.message)\n\n return new Response('ok')\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a Database Function and RPC",
          "content": "With the embeddings now stored in your Postgres database table, you can query them from Supabase Edge Functions by utilizing [Remote Procedure Calls (RPC)](/docs/guides/database/functions?language=js).\n\nGiven the [following Postgres Function](https://github.com/supabase/supabase/blob/master/examples/ai/edge-functions/supabase/migrations/20240410031515_vector-search.sql):\n\n```sql\n-- Matches document sections using vector similarity search on embeddings\n--\n-- Returns a setof embeddings so that we can use PostgREST resource embeddings (joins with other tables)\n-- Additional filtering like limits can be chained to this function call\ncreate or replace function query_embeddings(embedding vector(384), match_threshold float)\nreturns setof embeddings\nlanguage plpgsql\nas $$\nbegin\n return query\n select *\n from embeddings\n\n -- The inner product is negative, so we negate match_threshold\n where embeddings.embedding embedding embedding;\nend;\n$$;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Query vectors in Supabase Edge Functions",
          "content": "You can use `supabase-js` to first generate the embedding for the search term and then invoke the Postgres function to find the relevant results from your stored embeddings, right from your [Supabase Edge Function](https://github.com/supabase/supabase/blob/master/examples/ai/edge-functions/supabase/functions/search/index.ts):\n\n```tsx\nconst model = new Supabase.ai.Session('gte-small')\n\nDeno.serve(async (req) => {\n const { search } = await req.json()\n if (!search) return new Response('Please provide a search param!')\n // Generate embedding for search term.\n const embedding = await model.run(search, {\n mean_pool: true,\n normalize: true,\n })\n\n // Query embeddings.\n const { data: result, error } = await supabase\n .rpc('query_embeddings', {\n embedding,\n match_threshold: 0.8,\n })\n .select('content')\n .limit(3)\n if (error) {\n return Response.json(error)\n }\n\n return Response.json({ search, result })\n})\n```\n\nYou now have AI powered semantic search set up without any external dependencies! Just you, pgvector, and Supabase Edge Functions!",
          "level": 2
        }
      ],
      "wordCount": 548,
      "characterCount": 5278
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-send-emails",
      "identifier": "functions-examples-send-emails",
      "name": "Sending Emails",
      "description": "Sending emails from Edge Functions using the Resend API.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/send-emails",
      "dateModified": "2025-06-13T12:45:11.301892",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/send-emails.mdx",
      "frontmatter": {
        "title": "Sending Emails",
        "description": "Sending emails from Edge Functions using the Resend API.",
        "tocVideo": "Qf7XvL1fjvo"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Sending emails from Edge Functions using the [Resend API](https://resend.com/)."
        },
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "To get the most out of this guide, you’ll need to:\n\n- [Create an API key](https://resend.com/api-keys)\n- [Verify your domain](https://resend.com/domains)\n\nMake sure you have the latest version of the [Supabase CLI](https://supabase.com/docs/guides/cli#installation) installed.",
          "level": 3
        },
        {
          "type": "section",
          "title": "1. Create Supabase function",
          "content": "Create a new function locally:\n\n```bash\nsupabase functions new resend\n```\n\nStore the `RESEND_API_KEY` in your `.env` file.",
          "level": 3
        },
        {
          "type": "section",
          "title": "2. Edit the handler function",
          "content": "Paste the following code into the `index.ts` file:\n\n```tsx\nconst RESEND_API_KEY = Deno.env.get('RESEND_API_KEY')\n\nconst handler = async (_request: Request): Promise => {\n const res = await fetch('https://api.resend.com/emails', {\n method: 'POST',\n headers: {\n 'Content-Type': 'application/json',\n Authorization: `Bearer ${RESEND_API_KEY}`,\n },\n body: JSON.stringify({\n from: 'onboarding@resend.dev',\n to: 'delivered@resend.dev',\n subject: 'hello world',\n html: 'it works!',\n }),\n })\n\n const data = await res.json()\n\n return new Response(JSON.stringify(data), {\n status: 200,\n headers: {\n 'Content-Type': 'application/json',\n },\n })\n}\n\nDeno.serve(handler)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "3. Deploy and send email",
          "content": "Run function locally:\n\n```bash\nsupabase start\nsupabase functions serve --no-verify-jwt --env-file .env\n```\n\nTest it: http://localhost:54321/functions/v1/resend\n\nDeploy function to Supabase:\n\n```bash\nsupabase functions deploy resend --no-verify-jwt\n```\n\nWhen you deploy to Supabase, make sure that your `RESEND_API_KEY` is set in [Edge Function Secrets Management](https://supabase.com/dashboard/project/_/settings/functions)\n\nOpen the endpoint URL to send an email:",
          "level": 3
        },
        {
          "type": "section",
          "title": "4. Try it yourself",
          "content": "Find the complete example on [GitHub](https://github.com/resendlabs/resend-supabase-edge-functions-example).",
          "level": 3
        }
      ],
      "wordCount": 213,
      "characterCount": 1858
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-sentry-monitoring",
      "identifier": "functions-examples-sentry-monitoring",
      "name": "Monitoring with Sentry",
      "description": "Monitor Edge Functions with the Sentry Deno SDK.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/sentry-monitoring",
      "dateModified": "2025-06-13T12:45:11.302008",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/sentry-monitoring.mdx",
      "frontmatter": {
        "title": "Monitoring with Sentry",
        "description": "Monitor Edge Functions with the Sentry Deno SDK."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Add the [Sentry Deno SDK](https://docs.sentry.io/platforms/javascript/guides/deno/) to your Supabase Edge Functions to track exceptions and get notified of errors or performance issues."
        },
        {
          "type": "section",
          "title": "Prerequisites",
          "content": "- [Create a Sentry account](https://sentry.io/signup/).\n- Make sure you have the latest version of the [Supabase CLI](https://supabase.com/docs/guides/cli#installation) installed.",
          "level": 3
        },
        {
          "type": "section",
          "title": "1. Create Supabase function",
          "content": "Create a new function locally:\n\n```bash\nsupabase functions new sentryfied\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "2. Add the Sentry Deno SDK",
          "content": "Handle exceptions within your function and send them to Sentry.\n\n```tsx\nimport * as Sentry from 'https://deno.land/x/sentry/index.mjs'\n\nSentry.init({\n // https://docs.sentry.io/product/sentry-basics/concepts/dsn-explainer/#where-to-find-your-dsn\n dsn: SENTRY_DSN,\n defaultIntegrations: false,\n // Performance Monitoring\n tracesSampleRate: 1.0,\n // Set sampling rate for profiling - this is relative to tracesSampleRate\n profilesSampleRate: 1.0,\n})\n\n// Set region and execution_id as custom tags\nSentry.setTag('region', Deno.env.get('SB_REGION'))\nSentry.setTag('execution_id', Deno.env.get('SB_EXECUTION_ID'))\n\nDeno.serve(async (req) => {\n try {\n const { name } = await req.json()\n // This will throw, as `name` in our example call will be `undefined`\n const data = {\n message: `Hello ${name}!`,\n }\n\n return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })\n } catch (e) {\n Sentry.captureException(e)\n return new Response(JSON.stringify({ msg: 'error' }), {\n status: 500,\n headers: { 'Content-Type': 'application/json' },\n })\n }\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "3. Deploy and test",
          "content": "Run function locally:\n\n```bash\nsupabase start\nsupabase functions serve --no-verify-jwt\n```\n\nTest it: http://localhost:54321/functions/v1/sentryfied\n\nDeploy function to Supabase:\n\n```bash\nsupabase functions deploy sentryfied --no-verify-jwt\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "4. Try it yourself",
          "content": "Find the complete example on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/sentryfied/index.ts).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Working with scopes",
          "content": "Sentry Deno SDK currently do not support `Deno.serve` instrumentation, which means that there is no scope separation between requests.\nBecause of that, when the Edge Functions runtime is reused between multiple requests, all globally captured breadcrumbs and contextual data\nwill be shared, which is not the desired behavior. To work around this, all default integrations in the example code above are disabled,\nand you should be relying on [`withScope`](https://docs.sentry.io/platforms/javascript/enriching-events/scopes/#using-withscope) to encapsulate\nall Sentry SDK API calls, or [pass context directly](https://docs.sentry.io/platforms/javascript/enriching-events/context/#passing-context-directly)\nto the `captureException` or `captureMessage` calls.",
          "level": 2
        }
      ],
      "wordCount": 318,
      "characterCount": 2832
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-slack-bot-mention",
      "identifier": "functions-examples-slack-bot-mention",
      "name": "Slack Bot Mention Edge Function",
      "description": "Building a Slack Bot that Handles Mentions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/slack-bot-mention",
      "dateModified": "2025-06-13T12:45:11.302110",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/slack-bot-mention.mdx",
      "frontmatter": {
        "id": "slack-bot-mention",
        "title": "Slack Bot Mention Edge Function",
        "description": "Building a Slack Bot that Handles Mentions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The Slack Bot Mention Edge Function allows you to process mentions in Slack and respond accordingly."
        },
        {
          "type": "section",
          "title": "Configuring Slack apps",
          "content": "For your bot to seamlessly interact with Slack, you'll need to configure Slack Apps:\n\n1. Navigate to the Slack Apps page.\n1. Under \"Event Subscriptions,\" add the URL of the `slack-bot-mention` function and click to verify the URL.\n1. The Edge function will respond, confirming that everything is set up correctly.\n1. Add `app-mention` in the events the bot will subscribe to.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Creating the Edge Function",
          "content": "Deploy the following code as an Edge function using the CLI:\n\n```bash\nsupabase --project-ref nacho_slacker secrets \\\nset SLACK_TOKEN=\n```\n\nHere's the code of the Edge Function, you can change the response to handle the text received:\n\n```ts index.ts\nimport { WebClient } from 'https://deno.land/x/slack_web_api@6.7.2/mod.js'\n\nconst slackBotToken = Deno.env.get('SLACK_TOKEN') ?? ''\nconst botClient = new WebClient(slackBotToken)\n\nconsole.log(`Slack URL verification function up and running!`)\nDeno.serve(async (req) => {\n try {\n const reqBody = await req.json()\n console.log(JSON.stringify(reqBody, null, 2))\n const { token, challenge, type, event } = reqBody\n\n if (type == 'url_verification') {\n return new Response(JSON.stringify({ challenge }), {\n headers: { 'Content-Type': 'application/json' },\n status: 200,\n })\n } else if (event.type == 'app_mention') {\n const { user, text, channel, ts } = event\n // Here you should process the text received and return a response:\n const response = await botClient.chat.postMessage({\n channel: channel,\n text: `Hello !`,\n thread_ts: ts,\n })\n return new Response('ok', { status: 200 })\n }\n } catch (error) {\n return new Response(JSON.stringify({ error: error.message }), {\n headers: { 'Content-Type': 'application/json' },\n status: 500,\n })\n }\n})\n```",
          "level": 2
        }
      ],
      "wordCount": 263,
      "characterCount": 1828
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-stripe-webhooks",
      "identifier": "functions-examples-stripe-webhooks",
      "name": "Handling Stripe Webhooks",
      "description": "Handling signed Stripe Webhooks with Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/stripe-webhooks",
      "dateModified": "2025-06-13T12:45:11.302166",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/stripe-webhooks.mdx",
      "frontmatter": {
        "title": "Handling Stripe Webhooks",
        "description": "Handling signed Stripe Webhooks with Edge Functions."
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Handling signed Stripe Webhooks with Edge Functions. [View on GitHub](https://github.com/supabase/supabase/blob/master/examples/edge-functions/supabase/functions/stripe-webhooks/index.ts)."
        }
      ],
      "wordCount": 10,
      "characterCount": 188
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-telegram-bot",
      "identifier": "functions-examples-telegram-bot",
      "name": "Building a Telegram Bot",
      "description": "Building a Telegram Bot with Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/telegram-bot",
      "dateModified": "2025-06-13T12:45:11.302221",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/telegram-bot.mdx",
      "frontmatter": {
        "id": "examples-telegram-bot",
        "title": "Building a Telegram Bot",
        "description": "Building a Telegram Bot with Edge Functions.",
        "video": "https://www.youtube.com/v/AWfE3a9J_uo"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Handle Telegram Bot Webhooks with the [grammY framework](https://grammy.dev/). grammY is an open source Telegram Bot Framework which makes it easy to handle and respond to incoming messages. [View on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/telegram-bot)."
        }
      ],
      "wordCount": 30,
      "characterCount": 314
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-examples-upstash-redis",
      "identifier": "functions-examples-upstash-redis",
      "name": "Upstash Redis",
      "description": "Build an Edge Functions Counter with Upstash Redis.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/examples/upstash-redis",
      "dateModified": "2025-06-13T12:45:11.302325",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/examples/upstash-redis.mdx",
      "frontmatter": {
        "title": "Upstash Redis",
        "description": "Build an Edge Functions Counter with Upstash Redis."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "A Redis counter example that stores a [hash](https://redis.io/commands/hincrby/) of function invocation count per region. Find the code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/upstash-redis-counter)."
        },
        {
          "type": "section",
          "title": "Redis database setup",
          "content": "Create a Redis database using the [Upstash Console](https://console.upstash.com/) or [Upstash CLI](https://github.com/upstash/cli).\n\nSelect the `Global` type to minimize the latency from all edge locations. Copy the `UPSTASH_REDIS_REST_URL` and `UPSTASH_REDIS_REST_TOKEN` to your .env file.\n\nYou'll find them under **Details > REST API > .env**.\n\n```bash\ncp supabase/functions/upstash-redis-counter/.env.example supabase/functions/upstash-redis-counter/.env\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Code",
          "content": "Make sure you have the latest version of the [Supabase CLI installed](/docs/guides/cli#installation).\n\nCreate a new function in your project:\n\n```bash\nsupabase functions new upstash-redis-counter\n```\n\nAnd add the code to the `index.ts` file:\n\n```ts index.ts\nimport { Redis } from 'https://deno.land/x/upstash_redis@v1.19.3/mod.ts'\n\nconsole.log(`Function \"upstash-redis-counter\" up and running!`)\n\nDeno.serve(async (_req) => {\n try {\n const redis = new Redis({\n url: Deno.env.get('UPSTASH_REDIS_REST_URL')!,\n token: Deno.env.get('UPSTASH_REDIS_REST_TOKEN')!,\n })\n\n const deno_region = Deno.env.get('DENO_REGION')\n if (deno_region) {\n // Increment region counter\n await redis.hincrby('supa-edge-counter', deno_region, 1)\n } else {\n // Increment localhost counter\n await redis.hincrby('supa-edge-counter', 'localhost', 1)\n }\n\n // Get all values\n const counterHash: Record | null = await redis.hgetall('supa-edge-counter')\n const counters = Object.entries(counterHash!)\n .sort(([, a], [, b]) => b - a) // sort desc\n .reduce((r, [k, v]) => ({ total: r.total + v, regions: { ...r.regions, [k]: v } }), {\n total: 0,\n regions: {},\n })\n\n return new Response(JSON.stringify({ counters }), { status: 200 })\n } catch (error) {\n return new Response(JSON.stringify({ error: error.message }), { status: 200 })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Run locally",
          "content": "```bash\nsupabase start\nsupabase functions serve --no-verify-jwt --env-file supabase/functions/upstash-redis-counter/.env\n```\n\nNavigate to http://localhost:54321/functions/v1/upstash-redis-counter.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy",
          "content": "```bash\nsupabase functions deploy upstash-redis-counter --no-verify-jwt\nsupabase secrets set --env-file supabase/functions/upstash-redis-counter/.env\n```",
          "level": 2
        }
      ],
      "wordCount": 266,
      "characterCount": 2446
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-kysely-postgres",
      "identifier": "functions-kysely-postgres",
      "name": "Type-Safe SQL with Kysely",
      "description": "Combining Kysely with Deno Postgres gives you a convenient developer experience for interacting directly with your Postgres database.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/kysely-postgres",
      "dateModified": "2025-06-13T12:45:11.302523",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/kysely-postgres.mdx",
      "frontmatter": {
        "id": "kysely-postgres",
        "title": "Type-Safe SQL with Kysely",
        "description": "Combining Kysely with Deno Postgres gives you a convenient developer experience for interacting directly with your Postgres database."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Edge Functions can [connect directly to your Postgres database](/docs/guides/functions/connect-to-postgres) to execute SQL queries. [Kysely](https://github.com/kysely-org/kysely#kysely) is a type-safe and autocompletion-friendly typescript SQL query builder.\n\nCombining Kysely with Deno Postgres gives you a convenient developer experience for interacting directly with your Postgres database."
        },
        {
          "type": "section",
          "title": "Code",
          "content": "Find the example on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/kysely-postgres)\n\nGet your database connection credentials from your [Supabase Dashboard](https://supabase.com/dashboard/project/_/settings/database) and store them in an `.env` file:\n\n```bash .env\nDB_HOSTNAME=\nDB_PASSWORD=\nDB_SSL_CERT=\"-----BEGIN CERTIFICATE-----\nGET YOUR CERT FROM YOUR PROJECT DASHBOARD\n-----END CERTIFICATE-----\"\n```\n\nCreate a `DenoPostgresDriver.ts` file to manage the connection to Postgres via [deno-postgres](https://deno-postgres.com/):\n\n```ts DenoPostgresDriver.ts\nimport {\n CompiledQuery,\n DatabaseConnection,\n Driver,\n PostgresCursorConstructor,\n QueryResult,\n TransactionSettings,\n} from 'https://esm.sh/kysely@0.23.4'\nimport { freeze, isFunction } from 'https://esm.sh/kysely@0.23.4/dist/esm/util/object-utils.js'\nimport { extendStackTrace } from 'https://esm.sh/kysely@0.23.4/dist/esm/util/stack-trace-utils.js'\nimport { Pool, PoolClient } from 'https://deno.land/x/postgres@v0.17.0/mod.ts'\n\nexport interface PostgresDialectConfig {\n pool: Pool | (() => Promise)\n cursor?: PostgresCursorConstructor\n onCreateConnection?: (connection: DatabaseConnection) => Promise\n}\n\nconst PRIVATE_RELEASE_METHOD = Symbol()\n\nexport class PostgresDriver implements Driver {\n readonly #config: PostgresDialectConfig\n readonly #connections = new WeakMap()\n #pool?: Pool\n\n constructor(config: PostgresDialectConfig) {\n this.#config = freeze({ ...config })\n }\n\n async init(): Promise {\n this.#pool = isFunction(this.#config.pool) ? await this.#config.pool() : this.#config.pool\n }\n\n async acquireConnection(): Promise {\n const client = await this.#pool!.connect()\n let connection = this.#connections.get(client)\n\n if (!connection) {\n connection = new PostgresConnection(client, {\n cursor: this.#config.cursor ?? null,\n })\n this.#connections.set(client, connection)\n\n // The driver must take care of calling `onCreateConnection` when a new\n // connection is created. The `pg` module doesn't provide an async hook\n // for the connection creation. We need to call the method explicitly.\n if (this.#config?.onCreateConnection) {\n await this.#config.onCreateConnection(connection)\n }\n }\n\n return connection\n }\n\n async beginTransaction(\n connection: DatabaseConnection,\n settings: TransactionSettings\n ): Promise {\n if (settings.isolationLevel) {\n await connection.executeQuery(\n CompiledQuery.raw(`start transaction isolation level ${settings.isolationLevel}`)\n )\n } else {\n await connection.executeQuery(CompiledQuery.raw('begin'))\n }\n }\n\n async commitTransaction(connection: DatabaseConnection): Promise {\n await connection.executeQuery(CompiledQuery.raw('commit'))\n }\n\n async rollbackTransaction(connection: DatabaseConnection): Promise {\n await connection.executeQuery(CompiledQuery.raw('rollback'))\n }\n\n async releaseConnection(connection: PostgresConnection): Promise {\n connection[PRIVATE_RELEASE_METHOD]()\n }\n\n async destroy(): Promise {\n if (this.#pool) {\n const pool = this.#pool\n this.#pool = undefined\n await pool.end()\n }\n }\n}\n\ninterface PostgresConnectionOptions {\n cursor: PostgresCursorConstructor | null\n}\n\nclass PostgresConnection implements DatabaseConnection {\n #client: PoolClient\n #options: PostgresConnectionOptions\n\n constructor(client: PoolClient, options: PostgresConnectionOptions) {\n this.#client = client\n this.#options = options\n }\n\n async executeQuery(compiledQuery: CompiledQuery): Promise> {\n try {\n const result = await this.#client.queryObject(compiledQuery.sql, [\n ...compiledQuery.parameters,\n ])\n\n if (\n result.command === 'INSERT' ||\n result.command === 'UPDATE' ||\n result.command === 'DELETE'\n ) {\n const numAffectedRows = BigInt(result.rowCount || 0)\n\n return {\n numUpdatedOrDeletedRows: numAffectedRows,\n numAffectedRows,\n rows: result.rows ?? [],\n } as any\n }\n\n return {\n rows: result.rows ?? [],\n }\n } catch (err) {\n throw extendStackTrace(err, new Error())\n }\n }\n\n async *streamQuery(\n _compiledQuery: CompiledQuery,\n chunkSize: number\n ): AsyncIterableIterator> {\n if (!this.#options.cursor) {\n throw new Error(\n \"'cursor' is not present in your postgres dialect config. It's required to make streaming work in postgres.\"\n )\n }\n\n if (!Number.isInteger(chunkSize) || chunkSize \n animal: string\n created_at: Date\n}\n\n// Keys of this interface are table names.\ninterface Database {\n animals: AnimalTable\n}\n\n// Create a database pool with one connection.\nconst pool = new Pool(\n {\n tls: { caCertificates: [Deno.env.get('DB_SSL_CERT')!] },\n database: 'postgres',\n hostname: Deno.env.get('DB_HOSTNAME'),\n user: 'postgres',\n port: 5432,\n password: Deno.env.get('DB_PASSWORD'),\n },\n 1\n)\n\n// You'd create one of these when you start your app.\nconst db = new Kysely({\n dialect: {\n createAdapter() {\n return new PostgresAdapter()\n },\n createDriver() {\n return new PostgresDriver({ pool })\n },\n createIntrospector(db: Kysely) {\n return new PostgresIntrospector(db)\n },\n createQueryCompiler() {\n return new PostgresQueryCompiler()\n },\n },\n})\n\nserve(async (_req) => {\n try {\n // Run a query\n const animals = await db.selectFrom('animals').select(['id', 'animal', 'created_at']).execute()\n\n // Neat, it's properly typed \\o/\n console.log(animals[0].created_at.getFullYear())\n\n // Encode the result as pretty printed JSON\n const body = JSON.stringify(\n animals,\n (key, value) => (typeof value === 'bigint' ? value.toString() : value),\n 2\n )\n\n // Return the response with the correct content type header\n return new Response(body, {\n status: 200,\n headers: {\n 'Content-Type': 'application/json; charset=utf-8',\n },\n })\n } catch (err) {\n console.error(err)\n return new Response(String(err?.message ?? err), { status: 500 })\n }\n})\n```",
          "level": 2
        }
      ],
      "wordCount": 650,
      "characterCount": 6100
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-limits",
      "identifier": "functions-limits",
      "name": "Limits",
      "description": "Limits applied Edge Functions in Supabase's hosted platform.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/limits",
      "dateModified": "2025-06-13T12:45:11.302618",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/limits.mdx",
      "frontmatter": {
        "id": "functions-limits",
        "title": "Limits",
        "description": "Limits applied Edge Functions in Supabase's hosted platform.",
        "subtitle": "Limits applied Edge Functions in Supabase's hosted platform."
      },
      "sections": [
        {
          "type": "section",
          "title": "Runtime limits",
          "content": "- Maximum Memory: 256MB\n- Maximum Duration (Wall clock limit):\n This is the duration an Edge Function worker will stay active. During this period, a worker can serve multiple requests or process background tasks.\n - Free plan: 150s\n - Paid plans: 400s\n- Maximum CPU Time: 2s (Amount of actual time spent on the CPU per request - does not include async I/O.)\n- Request idle timeout: 150s (If an Edge Function doesn't send a response before the timeout, 504 Gateway Timeout will be returned)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Platform limits",
          "content": "- Maximum Function Size: 20MB (After bundling using CLI)\n- Maximum no. of Functions per project:\n - Free: 100\n - Pro: 500\n - Team: 1000\n - Enterprise: Unlimited\n- Maximum log message length: 10,000 characters\n- Log event threshold: 100 events per 10 seconds",
          "level": 2
        },
        {
          "type": "section",
          "title": "Other limits & restrictions",
          "content": "- Outgoing connections to ports `25` and `587` are not allowed.\n- Serving of HTML content is only supported with [custom domains](/docs/reference/cli/supabase-domains) (Otherwise `GET` requests that return `text/html` will be rewritten to `text/plain`).\n- Web Worker API (or Node `vm` API) are not available.\n- Node Libraries that require multithreading are not supported. Examples: [`libvips`](https://github.com/libvips/libvips), [sharp](https://github.com/lovell/sharp).",
          "level": 2
        }
      ],
      "wordCount": 196,
      "characterCount": 1294
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-local-development",
      "identifier": "functions-local-development",
      "name": "Local development",
      "description": "Setup local development environment for Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/local-development",
      "dateModified": "2025-06-13T12:45:11.302731",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/local-development.mdx",
      "frontmatter": {
        "id": "functions-local-development",
        "title": "Local development",
        "description": "Setup local development environment for Edge Functions.",
        "subtitle": "Setup local development environment for Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "We recommend installing the Deno CLI and related tools for local development."
        },
        {
          "type": "section",
          "title": "Deno support",
          "content": "You can follow the [Deno guide](https://deno.com/manual@v1.32.5/getting_started/setup_your_environment) for setting up your development environment with your favorite editor/IDE.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deno with Visual Studio Code",
          "content": "When using VSCode, you should install both the Deno CLI and the the Deno language server [via this link](vscode:extension/denoland.vscode-deno) or by browsing the extensions in VSCode and choosing to install the _Deno_ extension.\n\nThe Supabase CLI can automatically create helpful Deno settings when running `supabase init`. Select `y` when prompted \"Generate VS Code settings for Deno? [y/N]\"!",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deno support in subfolders",
          "content": "You can enable the Deno language server for specific sub-paths in a workspace, while using VSCode's built-in JavaScript/TypeScript language server for all other files.\n\nFor example if you have a project like this:\n\n```\nproject\n├── app\n└── supabase\n └── functions\n```\n\nTo enable the Deno language server only for the `supabase/functions` folder, add `./supabase/functions` to the list of _Deno: Enable Paths_ in the configuration. In your `.vscode/settings.json` file add:\n\n```json\n{\n \"deno.enablePaths\": [\"./supabase/functions\"],\n \"deno.importMap\": \"./supabase/functions/import_map.json\"\n}\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Multi-root workspaces in VSCode",
          "content": "We recommend using `deno.enablePaths` mentioned above as it's easier to manage, however if you like [multi-root workspaces](https://code.visualstudio.com/docs/editor/workspaces#_multiroot-workspaces) you can use these as an alternative.\n\nFor example, see this `edge-functions.code-workspace` configuration for a CRA (create react app) client with Supabase Edge Functions. You can find the complete example on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions).\n\n```json\n{\n \"folders\": [\n {\n \"name\": \"project-root\",\n \"path\": \"./\"\n },\n {\n \"name\": \"client\",\n \"path\": \"app\"\n },\n {\n \"name\": \"supabase-functions\",\n \"path\": \"supabase/functions\"\n }\n ],\n \"settings\": {\n \"files.exclude\": {\n \"node_modules/\": true,\n \"app/\": true,\n \"supabase/functions/\": true\n },\n \"deno.importMap\": \"./supabase/functions/import_map.json\"\n }\n}\n```",
          "level": 2
        }
      ],
      "wordCount": 270,
      "characterCount": 2216
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-local-quickstart",
      "identifier": "functions-local-quickstart",
      "name": "Developing Edge Functions locally",
      "description": "Get started with Edge Functions on your local machine.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/local-quickstart",
      "dateModified": "2025-06-13T12:45:11.302864",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/local-quickstart.mdx",
      "frontmatter": {
        "id": "functions-local-quickstart",
        "title": "Developing Edge Functions locally",
        "description": "Get started with Edge Functions on your local machine.",
        "subtitle": "Get started with Edge Functions on your local machine.",
        "tocVideo": "5OWH9c4u68M"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Let's create a basic Edge Function on your local machine and then invoke it using the Supabase CLI."
        },
        {
          "type": "section",
          "title": "Initialize a project",
          "content": "Create a new Supabase project in a folder on your local machine:\n\n```bash\nsupabase init\n```\n\nCheck out the [CLI Docs](/docs/guides/cli) to learn how to install the Supabase CLI on your local machine.\n\nIf you're using VS code you can have the CLI automatically create helpful Deno settings when running `supabase init`. Select `y` when prompted \"Generate VS Code settings for Deno? [y/N]\"!\n\nIf you're using an IntelliJ IDEA editor such as WebStorm, you can use the `--with-intellij-settings` flag with `supabase init` to create an auto generated Deno config.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create an Edge Function",
          "content": "Let's create a new Edge Function called `hello-world` inside your project:\n\n```bash\nsupabase functions new hello-world\n```\n\nThis creates a function stub in your `supabase` folder:\n\n```bash\n└── supabase\n ├── functions\n │ └── hello-world\n │ │ └── index.ts ## Your function code\n └── config.toml\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "How to write the code",
          "content": "The generated function uses native [Deno.serve](https://docs.deno.com/runtime/manual/runtime/http_server_apis) to handle requests. It gives you access to `Request` and `Response` objects.\n\nHere's the generated Hello World Edge Function, that accepts a name in the `Request` and responds with a greeting:\n\n```tsx\nDeno.serve(async (req) => {\n const { name } = await req.json()\n const data = {\n message: `Hello ${name}!`,\n }\n\n return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Running Edge Functions locally",
          "content": "You can run your Edge Function locally using [`supabase functions serve`](/docs/reference/cli/usage#supabase-functions-serve):\n\n```bash\nsupabase start # start the supabase stack\nsupabase functions serve # start the Functions watcher\n```\n\nThe `functions serve` command has hot-reloading capabilities. It will watch for any changes to your files and restart the Deno server.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Invoking Edge Functions locally",
          "content": "While serving your local Edge Function, you can invoke it using curl or one of the client libraries.\nTo call the function from a browser you need to handle CORS requests. See [CORS](/docs/guides/functions/cors).\n\n```bash name=cURL\ncurl --request POST 'http://localhost:54321/functions/v1/hello-world' \\\n --header 'Authorization: Bearer SUPABASE_ANON_KEY' \\\n --header 'Content-Type: application/json' \\\n --data '{ \"name\":\"Functions\" }'\n```\n\n```js name=JavaScript\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY)\n\nconst { data, error } = await supabase.functions.invoke('hello-world', {\n body: { name: 'Functions' },\n})\n```\n\nRun `supabase status` to see your local credentials.\n\nYou should see the response `{ \"message\":\"Hello Functions!\" }`.\n\nIf you execute the function with a different payload, the response will change.\n\nModify the `--data '{\"name\":\"Functions\"}'` line to `--data '{\"name\":\"World\"}'` and try invoking the command again.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Next steps",
          "content": "Check out the [Deploy to Production](/docs/guides/functions/deploy) guide to make your Edge Function available to the world.\n\nSee the [development tips](/docs/guides/functions/development-tips) for best practices.",
          "level": 2
        }
      ],
      "wordCount": 448,
      "characterCount": 3268
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-logging",
      "identifier": "functions-logging",
      "name": "Logging",
      "description": "How to access logs for your Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/logging",
      "dateModified": "2025-06-13T12:45:11.303033",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/logging.mdx",
      "frontmatter": {
        "id": "functions-logging",
        "title": "Logging",
        "description": "How to access logs for your Edge Functions.",
        "subtitle": "How to access logs for your Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Logs are provided for each function invocation, locally and in hosted environments."
        },
        {
          "type": "section",
          "title": "Hosted",
          "content": "You can access both tools from the [Functions section](https://supabase.com/dashboard/project/_/functions) of the Dashboard. Select your function from the list, and click `Invocations` or `Logs`:\n\n- **Invocations**: shows the Request and Response for each execution. You can see the headers, body, status code, and duration of each invocation. You can also filter the invocations by date, time, or status code.\n- **Logs**: shows any platform events, uncaught exceptions, and custom log events. You can see the timestamp, level, and message of each log event. You can also filter the log events by date, time, or level.\n\n![Function invocations.](/docs/img/guides/functions/function-logs.png)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Local",
          "content": "When [developing locally](/docs/guides/functions/local-development) you will see error messages and console log statements printed to your local terminal window.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Events that get logged",
          "content": "- **Uncaught exceptions**: Uncaught exceptions thrown by a function during execution are automatically logged. You can see the error message and stack trace in the Logs tool.\n- **Custom log events**: You can use `console.log`, `console.error`, and `console.warn` in your code to emit custom log events. These events also appear in the Logs tool.\n- **Boot and Shutdown Logs**: The Logs tool extends its coverage to include logs for the boot and shutdown of functions.\n\n A custom log message can contain up to 10,000 characters. A function can log up to 100 events\n within a 10 second period.\n\nHere is an example of how to use custom logs events in your function:\n\n```typescript\nDeno.serve(async (req) => {\n try {\n const { name } = await req.json()\n\n if (!name) {\n console.warn('Empty name provided')\n }\n\n const data = {\n message: `Hello ${name || 'Guest'}!`, // Provide a default value if name is empty\n }\n\n console.log(`Name: ${name}`)\n\n return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })\n } catch (error) {\n console.error(`Error processing request: ${error}`)\n return new Response(JSON.stringify({ error: 'Internal Server Error' }), {\n status: 500,\n headers: { 'Content-Type': 'application/json' },\n })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Logging request headers",
          "content": "When debugging Edge Functions, a common mistake is to try to log headers to the developer console via code like this:\n\n```ts index.ts\nDeno.serve(async (req) => {\n const headers = JSON.stringify(req.headers)\n console.log(`Request headers: ${headers}`)\n // OR\n console.log(`Request headers: ${JSON.stringify(req.headers)}`)\n return new Response('ok', {\n headers: {\n 'Content-Type': 'application/json',\n },\n status: 200,\n })\n})\n```\n\nBoth attempts will give as output the string `\"{}\"`, even though retrieving the value using `request.headers.get(\"Your-Header-Name\")` will indeed give you the correct value. This behavior mirrors that of browsers.\n\nThe reason behind this behavior is that [Headers](https://developer.mozilla.org/en-US/docs/Web/API/Headers) objects don't store headers in JavaScript properties that can be enumerated. As a result, neither the developer console nor the JSON stringifier can properly interpret the names and values of the headers. Essentially, it's not an empty object, but rather an opaque one.\n\nHowever, `Headers` objects are iterable. You can utilize this feature to craft a couple of succinct one-liners for debugging and printing headers.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Convert headers into an object with `Object.fromEntries`:",
          "content": "You can use `Object.fromEntries` which is a call to convert the headers into an object:\n\n```ts index.ts\nDeno.serve(async (req) => {\n let headersObject = Object.fromEntries(req.headers)\n let requestHeaders = JSON.stringify(headersObject, null, 2)\n console.log(`Request headers: ${requestHeaders}`)\n return new Response('ok', {\n headers: {\n 'Content-Type': 'application/json',\n },\n status: 200,\n })\n})\n```\n\nThis results in something like:\n\n```\nRequest headers: {\n \"accept\": \"*/*\",\n \"accept-encoding\": \"gzip\",\n \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InN1cGFuYWNobyIsInJvbGUiOiJhbm9uIiwieW91IjoidmVyeSBzbmVha3ksIGh1aD8iLCJpYXQiOjE2NTQ1NDA5MTYsImV4cCI6MTk3MDExNjkxNn0.cwBbk2tq-fUcKF1S0jVKkOAG2FIQSID7Jjvff5Do99Y\",\n \"cdn-loop\": \"cloudflare; subreqs=1\",\n \"cf-ew-via\": \"15\",\n \"cf-ray\": \"8597a2fcc558a5d7-GRU\",\n \"cf-visitor\": \"{\\\"scheme\\\":\\\"https\\\"}\",\n \"cf-worker\": \"supabase.co\",\n \"content-length\": \"20\",\n \"content-type\": \"application/x-www-form-urlencoded\",\n \"host\": \"edge-runtime.supabase.com\",\n \"my-custom-header\": \"abcd\",\n \"user-agent\": \"curl/8.4.0\",\n \"x-deno-subhost\": \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiIsImtpZCI6InN1cGFiYXNlIn0.eyJkZXBsb3ltZW50X2lkIjoic3VwYW5hY2hvX2M1ZGQxMWFiLTFjYmUtNDA3NS1iNDAxLTY3ZTRlZGYxMjVjNV8wMDciLCJycGNfcm9vdCI6Imh0dHBzOi8vc3VwYWJhc2Utb3JpZ2luLmRlbm8uZGV2L3YwLyIsImV4cCI6MTcwODYxMDA4MiwiaWF0IjoxNzA4NjA5MTgyfQ.-fPid2kEeEM42QHxWeMxxv2lJHZRSkPL-EhSH0r_iV4\",\n \"x-forwarded-host\": \"edge-runtime.supabase.com\",\n \"x-forwarded-port\": \"443\",\n \"x-forwarded-proto\": \"https\"\n}\n```",
          "level": 3
        }
      ],
      "wordCount": 597,
      "characterCount": 5101
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-pricing",
      "identifier": "functions-pricing",
      "name": "Pricing",
      "description": "",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/pricing",
      "dateModified": "2025-06-13T12:45:11.303097",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/pricing.mdx",
      "frontmatter": {
        "id": "functions-pricing",
        "title": "Pricing"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "For a detailed explanation of how charges are calculated, refer to [Manage Edge Function Invocations usage](/docs/guides/platform/manage-your-usage/edge-function-invocations)."
        }
      ],
      "wordCount": 16,
      "characterCount": 175
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-quickstart",
      "identifier": "functions-quickstart",
      "name": "Developing Edge Functions with Supabase",
      "description": "Get started with Edge Functions on the Supabase dashboard.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/quickstart",
      "dateModified": "2025-06-13T12:45:11.303253",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/quickstart.mdx",
      "frontmatter": {
        "id": "functions-quickstart",
        "title": "Developing Edge Functions with Supabase",
        "description": "Get started with Edge Functions on the Supabase dashboard.",
        "subtitle": "Get started with Edge Functions on the Supabase dashboard."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "In this guide we'll cover how to create a basic Edge Function on the Supabase dashboard, and access it using the Supabase CLI."
        },
        {
          "type": "section",
          "title": "Deploy from Dashboard",
          "content": "Go to your project > Edge Functions > Deploy a new function > Via Editor\n\nThis will scaffold a new function for you. You can choose from Templates some of the pre-defined functions for common use cases.\n\nModify the function as needed, name it, and click `Deploy function`.\n\nYour function is now active. Navigate to the function's details page, and click on the test button.\n\nYou can test your function by providing the expected HTTP method, headers, query parameters, and request body. You can also change the authorization token passed (e.g., anon key or a user key).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Access deployed functions via Supabase CLI",
          "content": "Check out the [CLI Docs](/docs/guides/cli) to learn how to install the Supabase CLI on your local machine.\n\nNow that your function is deployed, you can access it from your local development environment.\nHere's how:\n\n1. **Link your project** to your local environment.\n\n You can find your project reference ID in the URL of your Supabase dashboard or in the project settings.\n\n ```bash\n supabase link --project-ref your-project-ref\n ```\n\n2. **List all Functions** in the linked Supabase project.\n\n ```bash\n supabase functions list\n ```\n\n3. **Access the specific function** you want to work on.\n\n ```bash\n supabase functions download function-name\n ```\n\n4. **Make local edits** to the function code as needed.\n\n5. **Run your function locally** before redeploying.\n\n ```bash\n supabase functions serve function-name\n ```\n\n6. **Redeploy** when you're ready with your changes.\n\n ```bash\n supabase functions deploy function-name\n ```\n\n{/* supa-mdx-lint-disable-next-line Rule001HeadingCase */}",
          "level": 2
        },
        {
          "type": "section",
          "title": "Deploy via Assistant",
          "content": "You can also leverage the Supabase Assistant to help you write and deploy edge functions.\n\nGo to your project > Edge Functions > Click on the Assistant icon to Create with Supabase Assistant\n\nThis brings up an assistant window with a pre-filled prompt for generating edge functions.\nWrite up your Edge Function requirement, and let Supabase Assistant do the rest.\n\nClick Deploy and the Assistant will automatically deploy your function.\n\nThis function requires an OpenAI API key. You can add the key in your Edge Functions secrets page, or ask Assistant for help.\n\n1. Navigate to your Edge Functions > Secrets page.\n2. Look for the option to add environment variables.\n3. Add a new environment variable with the key `OPENAI_API_KEY` and set its value to your actual OpenAI API key.\n\nOnce you've set this environment variable, your edge functions will be able to access the OPENAI_API_KEY securely without hardcoding it into the function code. This is a best practice for keeping sensitive information safe.\n\nWith your variable set, you can test by sending a request via the dashboard. Navigate to the function's details page, and click on the test button. Then provide a Request Body your function expects.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Editing functions from the Dashboard",
          "content": "The Dashboard's Edge Function editor currently does not support versioning or rollbacks. We recommend using it only for quick testing and prototypes. When you’re ready to go to production, store Edge Functions code in a source code repository (e.g., git) and deploy it using one of the [CI integrations](https://supabase.com/docs/guides/functions/cicd-workflow).\n\n1. From the functions page, click on the function you want to edit. From the function page, click on the Code tab.\n\n2. This opens up a code editor in the dashboard where you can see your deployed function's code.\n\n3. Modify the code as needed, then click Deploy updates. This will overwrite the existing deployment with the newly edited function code.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Next steps",
          "content": "Check out the [Local development](/docs/guides/functions/local-quickstart) guide for more details on working with Edge Functions.\n\nRead on for some [common development tips](/docs/guides/functions/development-tips).",
          "level": 2
        }
      ],
      "wordCount": 613,
      "characterCount": 3980
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-regional-invocation",
      "identifier": "functions-regional-invocation",
      "name": "Regional Invocations",
      "description": "How to execute an Edge Functions in a particular region.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/regional-invocation",
      "dateModified": "2025-06-13T12:45:11.303375",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/regional-invocation.mdx",
      "frontmatter": {
        "id": "function-regional-invocation",
        "title": "Regional Invocations",
        "description": "How to execute an Edge Functions in a particular region.",
        "subtitle": "How to execute an Edge Function in a particular region."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Functions are executed in the region closest to the user making the request. This helps to reduce network latency and provide faster responses to the user.\n\nHowever, if your Function performs lots of database or storage operations, invoking the Function in the same region as your database may provide better performance. Some situations where this might be helpful include:\n\n- Bulk adding and editing records in your database\n- Uploading files\n\nSupabase provides an option to specify the region when invoking the Function."
        },
        {
          "type": "section",
          "title": "Using the `x-region` header",
          "content": "Use the `x-region` HTTP header when calling an Edge Function to determine where the Function should be executed:\n\n```bash name=cURL",
          "level": 2
        },
        {
          "type": "section",
          "title": "https://supabase.com/docs/guides/functions/deploy#invoking-remote-functions",
          "content": "curl --request POST 'https://.supabase.co/functions/v1/hello-world' \\\n --header 'Authorization: Bearer ANON_KEY' \\\n --header 'Content-Type: application/json' \\\n --header 'x-region: eu-west-3' \\\n --data '{ \"name\":\"Functions\" }'\n```\n\n```js name=JavaScript\n// https://supabase.com/docs/reference/javascript/installing\nimport { createClient } from '@supabase/supabase-js'\n\n// Create a single supabase client for interacting with your database\nconst supabase = createClient('https://xyzcompany.supabase.co', 'public-anon-key')\n\n// https://supabase.com/docs/reference/javascript/functions-invoke\nconst { data, error } = await supabase.functions.invoke('hello-world', {\n body: { name: 'Functions' },\n headers: { 'x-region': 'eu-west-3' },\n})\n```\n\nYou can verify the execution region by looking at the `x-sb-edge-region` HTTP header in the response. You can also find it as metadata in [Edge Function Logs](/docs/guides/functions/logging).",
          "level": 1
        },
        {
          "type": "section",
          "title": "Available regions",
          "content": "These are the currently supported region values you can provide for `x-region` header.\n\n- `ap-northeast-1`\n- `ap-northeast-2`\n- `ap-south-1`\n- `ap-southeast-1`\n- `ap-southeast-2`\n- `ca-central-1`\n- `eu-central-1`\n- `eu-west-1`\n- `eu-west-2`\n- `eu-west-3`\n- `sa-east-1`\n- `us-east-1`\n- `us-west-1`\n- `us-west-2`",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using the client library",
          "content": "You can also specify the region when invoking a Function using the Supabase client library:\n\n```js\nimport { createClient, FunctionRegion } from '@supabase/supabase-js'\nconst supabase = createClient('SUPABASE_URL', 'SUPABASE_ANON_KEY')\n\nconst { data: ret, error } = await supabase.functions.invoke('my-function-name', {\n headers: { 'Content-Type': 'application/json' },\n method: 'GET',\n body: {},\n region: FunctionRegion.UsEast1,\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Handling regional outages",
          "content": "If you explicitly specify the region via `x-region` header, requests **will NOT** be automatically re-routed to another region and you should consider temporarily changing regions during the outage.",
          "level": 2
        }
      ],
      "wordCount": 342,
      "characterCount": 2733
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-routing",
      "identifier": "functions-routing",
      "name": "Handling Routing in Functions",
      "description": "How to handle custom routing within Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/routing",
      "dateModified": "2025-06-13T12:45:11.303668",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/routing.mdx",
      "frontmatter": {
        "id": "function-routing",
        "title": "Handling Routing in Functions",
        "description": "How to handle custom routing within Edge Functions.",
        "subtitle": "How to handle custom routing within Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Usually, an Edge Function is written to perform a single action (e.g. write a record to the database). However, if your app's logic is split into multiple Edge Functions requests to each action may seem slower.\nThis is because each Edge Function needs to be booted before serving a request (known as cold starts). If an action is performed less frequently (e.g. deleting a record), there is a high-chance of that function experiencing a cold-start.\n\nOne way to reduce the cold starts and increase performance of your app is to combine multiple actions into a single Edge Function. This way only one instance of the Edge Function needs to be booted and it can handle multiple requests to different actions.\nFor example, we can use a single Edge Function to create a typical CRUD API (create, read, update, delete records).\n\nTo combine multiple endpoints into a single Edge Function, you can use web application frameworks such as [Express](https://expressjs.com/), [Oak](https://oakserver.github.io/oak/), or [Hono](https://hono.dev).\n\nLet's dive into some examples."
        },
        {
          "type": "section",
          "title": "Routing with frameworks",
          "content": "Here's a simple hello world example using some popular web frameworks.\n\nCreate a new function called `hello-world` using Supabase CLI:\n\n```bash\nsupabase functions new hello-world\n```\n\nCopy and paste the following code:\n\n```ts\nimport express from 'npm:express@4.18.2'\n\nconst app = express()\napp.use(express.json())\n// If you want a payload larger than 100kb, then you can tweak it here:\n// app.use( express.json({ limit : \"300kb\" }));\n\nconst port = 3000\n\napp.get('/hello-world', (req, res) => {\n res.send('Hello World!')\n})\n\napp.post('/hello-world', (req, res) => {\n const { name } = req.body\n res.send(`Hello ${name}!`)\n})\n\napp.listen(port, () => {\n console.log(`Example app listening on port ${port}`)\n})\n```\n\n```ts\nimport { Application } from \"jsr:@oak/oak@15/application\";\nimport { Router } from \"jsr:@oak/oak@15/router\";\n\nconst router = new Router();\n\nrouter.get(\"/hello-world\", (ctx) => {\nctx.response.body = \"Hello world!\";\n});\n\nrouter.post(\"/hello-world\", async (ctx) => {\nconst { name } = await ctx.request.body.json();\nctx.response.body = `Hello ${name}!`;\n});\n\nconst app = new Application();\napp.use(router.routes());\napp.use(router.allowedMethods());\n\napp.listen({ port: 3000 });\n\n````\n\n```ts\nimport { Hono } from 'jsr:@hono/hono';\n\nconst app = new Hono();\n\napp.post('/hello-world', async (c) => {\n const { name } = await c.req.json();\n return new Response(`Hello ${name}!`)\n});\n\napp.get('/hello-world', (c) => {\n return new Response('Hello World!')\n});\n\nDeno.serve(app.fetch);\n````\n\n```ts\nDeno.serve(async (req) => {\n if (req.method === 'GET') {\n return new Response('Hello World!')\n }\n const { name } = await req.json()\n if (name) {\n return new Response(`Hello ${name}!`)\n }\n return new Response('Hello World!')\n});\n```\n\nYou will notice in the above example, we created two routes - `GET` and `POST`. The path for both routes are defined as `/hello-world`.\nIf you run a server outside of Edge Functions, you'd usually set the root path as `/` .\nHowever, within Edge Functions, paths should always be prefixed with the function name (in this case `hello-world`).\n\nYou can deploy the function to Supabase via:\n\n```bash\nsupabase functions deploy hello-world\n```\n\nOnce the function is deployed, you can try to call the two endpoints using cURL (or Postman).\n\n```bash",
          "level": 2
        },
        {
          "type": "section",
          "title": "https://supabase.com/docs/guides/functions/deploy#invoking-remote-functions",
          "content": "curl --request GET 'https://.supabase.co/functions/v1/hello-world' \\\n --header 'Authorization: Bearer ANON_KEY' \\\n```\n\nThis should print the response as `Hello World!`, meaning it was handled by the `GET` route.\n\nSimilarly, we can make a request to the `POST` route.\n\n```bash cURL",
          "level": 1
        },
        {
          "type": "section",
          "title": "https://supabase.com/docs/guides/functions/deploy#invoking-remote-functions",
          "content": "curl --request POST 'https://.supabase.co/functions/v1/hello-world' \\\n --header 'Authorization: Bearer ANON_KEY' \\\n --header 'Content-Type: application/json' \\\n --data '{ \"name\":\"Foo\" }'\n```\n\nWe should see a response printing `Hello Foo!`.",
          "level": 1
        },
        {
          "type": "section",
          "title": "Using route parameters",
          "content": "We can use route parameters to capture values at specific URL segments (e.g. `/tasks/:taskId/notes/:noteId`).\n\nHere's an example Edge Function implemented using the Framework for managing tasks using route parameters.\nKeep in mind paths must be prefixed by function name (i.e. `tasks` in this example). Route parameters can only be used after the function name prefix.\n\n```ts\nimport express from 'npm:express@4.18.2'\n\nconst app = express();\napp.use(express.json());\n\napp.get('/tasks', async (req, res) => {\n// return all tasks\n});\n\napp.post('/tasks', async (req, res) => {\n// create a task\n});\n\napp.get('/tasks/:id', async (req, res) => {\nconst id = req.params.id\nconst task = {} // get task\n\nres.json(task)\n});\n\napp.patch('/tasks/:id', async (req, res) => {\nconst id = req.params.id\n// modify task\n});\n\napp.delete('/tasks/:id', async (req, res) => {\nconst id = req.params.id\n// delete task\n});\n\n````\n\n```ts\nimport { Application } from \"jsr:@oak/oak/application\";\nimport { Router } from \"jsr:@oak/oak/router\";\n\nconst router = new Router();\n\nlet tasks: { [id: string]: any } = {};\n\nrouter\n .get(\"/tasks\", (ctx) => {\n ctx.response.body = Object.values(tasks);\n })\n .post(\"/tasks\", async (ctx) => {\n const body = ctx.request.body();\n const { name } = await body.value;\n const id = Math.random().toString(36).substring(7);\n tasks[id] = { id, name };\n ctx.response.body = tasks[id];\n })\n .get(\"/tasks/:id\", (ctx) => {\n const id = ctx.params.id;\n const task = tasks[id];\n if (task) {\n ctx.response.body = task;\n } else {\n ctx.response.status = 404;\n ctx.response.body = 'Task not found';\n }\n })\n .patch(\"/tasks/:id\", async (ctx) => {\n const id = ctx.params.id;\n const body = ctx.request.body();\n const updates = await body.value;\n const task = tasks[id];\n if (task) {\n tasks[id] = { ...task, ...updates };\n ctx.response.body = tasks[id];\n } else {\n ctx.response.status = 404;\n ctx.response.body = 'Task not found';\n }\n })\n .delete(\"/tasks/:id\", (ctx) => {\n const id = ctx.params.id;\n if (tasks[id]) {\n delete tasks[id];\n ctx.response.body = 'Task deleted successfully';\n } else {\n ctx.response.status = 404;\n ctx.response.body = 'Task not found';\n }\n });\n\nconst app = new Application();\napp.use(router.routes());\napp.use(router.allowedMethods());\n\napp.listen({ port: 3000 });\n````\n\n```ts\nimport { Hono } from 'jsr:@hono/hono'\n\n// You can set the basePath with Hono\nconst functionName = 'tasks'\nconst app = new Hono().basePath(`/${functionName}`)\n\n// /tasks/id\napp.get('/:id', async (c) => {\n const id = c.req.param('id')\n const task = {} // Fetch task by id here\n if (task) {\n return new Response(JSON.stringify(task))\n } else {\n return new Response('Task not found', { status: 404 })\n }\n})\n\napp.patch('/:id', async (c) => {\n const id = c.req.param('id')\n const body = await c.req.body()\n const updates = body.value\n const task = {} // Fetch task by id here\n if (task) {\n Object.assign(task, updates)\n return new Response(JSON.stringify(task))\n } else {\n return new Response('Task not found', { status: 404 })\n }\n})\n\napp.delete('/:id', async (c) => {\n const id = c.req.param('id')\n const task = {} // Fetch task by id here\n if (task) {\n // Delete task\n return new Response('Task deleted successfully')\n } else {\n return new Response('Task not found', { status: 404 })\n }\n})\n\nDeno.serve(app.fetch)\n```\n\n```ts\ninterface Task {\n id: string\n name: string\n}\n\nlet tasks: Task[] = []\n\nconst router = new Map Promise>()\n\nasync function getAllTasks(): Promise {\n return new Response(JSON.stringify(tasks))\n}\n\nasync function getTask(id: string): Promise {\n const task = tasks.find((t) => t.id === id)\n if (task) {\n return new Response(JSON.stringify(task))\n } else {\n return new Response('Task not found', { status: 404 })\n }\n}\n\nasync function createTask(req: Request): Promise {\n const id = Math.random().toString(36).substring(7)\n const task = { id, name: '' }\n tasks.push(task)\n return new Response(JSON.stringify(task), { status: 201 })\n}\n\nasync function updateTask(id: string, req: Request): Promise {\n const index = tasks.findIndex((t) => t.id === id)\n if (index !== -1) {\n tasks[index] = { ...tasks[index] }\n return new Response(JSON.stringify(tasks[index]))\n } else {\n return new Response('Task not found', { status: 404 })\n }\n}\n\nasync function deleteTask(id: string): Promise {\n const index = tasks.findIndex((t) => t.id === id)\n if (index !== -1) {\n tasks.splice(index, 1)\n return new Response('Task deleted successfully')\n } else {\n return new Response('Task not found', { status: 404 })\n }\n}\n\nDeno.serve(async (req) => {\n const url = new URL(req.url)\n const method = req.method\n // Extract the last part of the path as the command\n const command = url.pathname.split('/').pop()\n // Assuming the last part of the path is the task ID\n const id = command\n try {\n switch (method) {\n case 'GET':\n if (id) {\n return getTask(id)\n } else {\n return getAllTasks()\n }\n case 'POST':\n return createTask(req)\n case 'PUT':\n if (id) {\n return updateTask(id, req)\n } else {\n return new Response('Bad Request', { status: 400 })\n }\n case 'DELETE':\n if (id) {\n return deleteTask(id)\n } else {\n return new Response('Bad Request', { status: 400 })\n }\n default:\n return new Response('Method Not Allowed', { status: 405 })\n }\n } catch (error) {\n return new Response(`Internal Server Error: ${error}`, { status: 500 })\n }\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "URL patterns API",
          "content": "If you prefer not to use a web framework, you can directly use [URL Pattern API](https://developer.mozilla.org/en-US/docs/Web/API/URL_Pattern_API) within your Edge Functions to implement routing.\nThis is ideal for small apps with only couple of routes and you want to have a custom matching algorithm.\n\nHere is an example Edge Function using URL Patterns API: https://github.com/supabase/supabase/blob/master/examples/edge-functions/supabase/functions/restful-tasks/index.ts",
          "level": 2
        }
      ],
      "wordCount": 1416,
      "characterCount": 9872
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-schedule-functions",
      "identifier": "functions-schedule-functions",
      "name": "Scheduling Edge Functions",
      "description": "Schedule Edge Functions with pg_cron.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/schedule-functions",
      "dateModified": "2025-06-13T12:45:11.303783",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/schedule-functions.mdx",
      "frontmatter": {
        "id": "schedule-functions",
        "title": "Scheduling Edge Functions",
        "description": "Schedule Edge Functions with pg_cron."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The hosted Supabase Platform supports the [`pg_cron` extension](/docs/guides/database/extensions/pgcron), a recurring job scheduler in Postgres.\n\nIn combination with the [`pg_net` extension](/docs/guides/database/extensions/pgnet), this allows us to invoke Edge Functions periodically on a set schedule.\n\nTo access the auth token securely for your Edge Function call, we recommend storing them in [Supabase Vault](/docs/guides/database/vault)."
        },
        {
          "type": "section",
          "title": "Invoke an Edge Function every minute",
          "content": "Store `project_url` and `anon_key` in Supabase Vault:\n\n```sql\nselect vault.create_secret('https://project-ref.supabase.co', 'project_url');\nselect vault.create_secret('YOUR_SUPABASE_ANON_KEY', 'anon_key');\n```\n\nMake a POST request to a Supabase Edge Function every minute:\n\n```sql\nselect\n cron.schedule(\n 'invoke-function-every-minute',\n '* * * * *', -- every minute\n $$\n select\n net.http_post(\n url:= (select decrypted_secret from vault.decrypted_secrets where name = 'project_url') || '/functions/v1/function-name',\n headers:=jsonb_build_object(\n 'Content-type', 'application/json',\n 'Authorization', 'Bearer ' || (select decrypted_secret from vault.decrypted_secrets where name = 'anon_key')\n ),\n body:=concat('{\"time\": \"', now(), '\"}')::jsonb\n ) as request_id;\n $$\n );\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- [`pg_net` extension](/docs/guides/database/extensions/pgnet)\n- [`pg_cron` extension](/docs/guides/database/extensions/pgcron)",
          "level": 2
        }
      ],
      "wordCount": 145,
      "characterCount": 1419
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-secrets",
      "identifier": "functions-secrets",
      "name": "Managing Secrets (Environment Variables)",
      "description": "Managing secrets and environment variables.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/secrets",
      "dateModified": "2025-06-13T12:45:11.303914",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/secrets.mdx",
      "frontmatter": {
        "id": "functions-secrets",
        "title": "Managing Secrets (Environment Variables)",
        "description": "Managing secrets and environment variables.",
        "subtitle": "Managing secrets and environment variables."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "It's common that you will need to use environment variables or other sensitive information in Edge Functions. You can manage secrets using the CLI or the Dashboard.\n\nYou can access these using Deno's built-in handler\n\n```js\nDeno.env.get('MY_SECRET_NAME')\n```"
        },
        {
          "type": "section",
          "title": "Default secrets",
          "content": "Edge Functions have access to these secrets by default:\n\n- `SUPABASE_URL`: The API gateway for your Supabase project.\n- `SUPABASE_ANON_KEY`: The `anon` key for your Supabase API. This is safe to use in a browser when you have [Row Level Security](/docs/guides/database/postgres/row-level-security) enabled.\n- `SUPABASE_SERVICE_ROLE_KEY`: The `service_role` key for your Supabase API. This is safe to use in Edge Functions, but it should NEVER be used in a browser. This key will bypass [Row Level Security](/docs/guides/database/postgres/row-level-security).\n- `SUPABASE_DB_URL`: The URL for your [Postgres database](/docs/guides/database). You can use this to connect directly to your database.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Local secrets",
          "content": "You can load environment variables in two ways:\n\n1. Through an `.env` file placed at `supabase/functions/.env`, which is automatically loaded on `supabase start`\n2. Through the `--env-file` option for `supabase functions serve`, for example: `supabase functions serve --env-file ./path/to/.env-file`\n\nLet's create a local file for storing our secrets, and inside it we can store a secret `MY_NAME`:\n\n```bash\necho \"MY_NAME=Yoda\" >> ./supabase/.env.local\n```\n\nThis creates a new file `./supabase/.env.local` for storing your local development secrets.\n\nNever check your .env files into Git!\n\nNow let's access this environment variable `MY_NAME` inside our Function. Anywhere in your function, add this line:\n\n```jsx\nconsole.log(Deno.env.get('MY_NAME'))\n```\n\nNow we can invoke our function locally, by serving it with our new `.env.local` file:\n\n```bash\nsupabase functions serve --env-file ./supabase/.env.local\n```\n\nWhen the function starts you should see the name “Yoda” output to the terminal.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Production secrets",
          "content": "You will also need to set secrets for your production Edge Functions. You can do this via the Dashboard or using the CLI.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Using the Dashboard",
          "content": "1. Visit [Edge Function Secrets Management](https://supabase.com/dashboard/project/_/settings/functions) page in your Dashboard.\n2. Add the Key and Value for your secret and press Save.\n3. Note that you can paste multiple secrets at a time.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Using the CLI",
          "content": "Let's create a `.env` to help us deploy our secrets to production. In this case we'll just use the same as our local secrets:\n\n```bash\ncp ./supabase/.env.local ./supabase/.env\n```\n\nThis creates a new file `./supabase/.env` for storing your production secrets.\n\nNever check your `.env` files into Git! You only use the `.env` file to help deploy your secrets to production. Don't commit it to your repository.\n\nLet's push all the secrets from the `.env` file to our remote project using [`supabase secrets set`](/docs/reference/cli/usage#supabase-secrets-set):\n\n```bash\nsupabase secrets set --env-file ./supabase/.env",
          "level": 3
        },
        {
          "type": "section",
          "title": "You can also set secrets individually using:",
          "content": "supabase secrets set MY_NAME=Chewbacca\n```\n\nYou don't need to re-deploy after setting your secrets.\n\nTo see all the secrets which you have set remotely, use [`supabase secrets list`](/docs/reference/cli/usage#supabase-secrets-list):\n\n```bash\nsupabase secrets list\n```",
          "level": 1
        }
      ],
      "wordCount": 472,
      "characterCount": 3354
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-status-codes",
      "identifier": "functions-status-codes",
      "name": "Status codes",
      "description": "Edge Functions can return following status codes.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/status-codes",
      "dateModified": "2025-06-13T12:45:11.303999",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/status-codes.mdx",
      "frontmatter": {
        "id": "functions-status-codes",
        "title": "Status codes",
        "description": "Edge Functions can return following status codes.",
        "subtitle": "Edge Functions can return following status codes."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "{/* supa-mdx-lint-disable Rule001HeadingCase */}"
        },
        {
          "type": "section",
          "title": "2XX Success",
          "content": "A successful Edge Function Response",
          "level": 2
        },
        {
          "type": "section",
          "title": "3XX Redirect",
          "content": "The Edge Function has responded with a `Response.redirect` [API docs](https://developer.mozilla.org/en-US/docs/Web/API/Response/redirect_static)",
          "level": 2
        },
        {
          "type": "section",
          "title": "401 Unauthorized",
          "content": "If the Edge Function has `Verify JWT` option enabled, but the request was made with an invalid JWT.",
          "level": 3
        },
        {
          "type": "section",
          "title": "404 Not Found",
          "content": "Requested Edge Function was not found.",
          "level": 3
        },
        {
          "type": "section",
          "title": "405 Method Not Allowed",
          "content": "Edge Functions only support these HTTP methods: 'POST', 'GET', 'PUT', 'PATCH', 'DELETE', 'OPTIONS'",
          "level": 3
        },
        {
          "type": "section",
          "title": "500 Internal Server Error",
          "content": "Edge Function threw an uncaught exception (`WORKER_ERROR`). Check Edge Function logs to find the cause.",
          "level": 3
        },
        {
          "type": "section",
          "title": "503 Service Unavailable",
          "content": "Edge Function failed to start (`BOOT_ERROR`). Check Edge Function logs to find the cause.",
          "level": 3
        },
        {
          "type": "section",
          "title": "504 Gateway Timeout",
          "content": "Edge Function didn't respond before the [request idle timeout](/docs/guides/functions/limits).",
          "level": 3
        },
        {
          "type": "section",
          "title": "546 Resource Limit (Custom Error Code)",
          "content": "Edge Function execution was stopped due to a resource limit (`WORKER_LIMIT`). Edge Function logs should provide which [resource limit](/docs/guides/functions/limits) was exceeded.",
          "level": 3
        }
      ],
      "wordCount": 161,
      "characterCount": 1220
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-storage-caching",
      "identifier": "functions-storage-caching",
      "name": "Integrating with Supabase Storage",
      "description": "Integrate Edge Functions with Supabase Storage to cache images on the Edge (CDN).",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/storage-caching",
      "dateModified": "2025-06-13T12:45:11.304052",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/storage-caching.mdx",
      "frontmatter": {
        "id": "storage-caching",
        "title": "Integrating with Supabase Storage",
        "description": "Integrate Edge Functions with Supabase Storage to cache images on the Edge (CDN).",
        "video": "https://www.youtube.com/v/wW6L52v9Ldo"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Integrate Edge Functions with Supabase Storage to cache images on the Edge (CDN). [View on GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/og-image-with-storage-cdn)."
        }
      ],
      "wordCount": 16,
      "characterCount": 218
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-troubleshooting",
      "identifier": "functions-troubleshooting",
      "name": "Troubleshooting Common Issues",
      "description": "How to solve common problems and issues related to Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/troubleshooting",
      "dateModified": "2025-06-13T12:45:11.304295",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/troubleshooting.mdx",
      "frontmatter": {
        "id": "functions-debugging",
        "title": "Troubleshooting Common Issues",
        "description": "How to solve common problems and issues related to Edge Functions.",
        "subtitle": "How to solve common problems and issues related to Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "If you encounter any problems or issues with your Edge Functions, here are some tips and steps to help you resolve them."
        },
        {
          "type": "section",
          "title": "Unable to deploy Edge Function",
          "content": "- Make sure you're on the latest version of the [Supabase CLI](/docs/guides/cli#updates).\n- If the output from the commands above does not help you to resolve the issue, open a support ticket via the Supabase Dashboard (by clicking the \"Help\" button at the top right) and include all output from the commands mentioned above.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Unable to call Edge Function",
          "content": "If you’re unable to call your Edge Function or are experiencing any CORS issues:\n\n- Make sure you followed the [CORS guide](/docs/guides/functions/cors). This guide explains how to enable and configure CORS for your Edge Functions, and how to avoid common pitfalls and errors.\n- Check your function logs. Navigate to the [Functions section](https://supabase.com/dashboard/project/_/functions) in your dashboard, select your function from the list, and click `Logs`. Check for any errors or warnings that may indicate the cause of the problem.\n\nThere are two debugging tools available: Invocations and Logs. Invocations shows the Request and Response for each execution, while Logs shows any platform events, including deployments and errors.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Edge Function takes too long to respond",
          "content": "If your Edge Function takes too long to respond or times out:\n\n- Navigate to the [Functions section](https://supabase.com/dashboard/project/_/functions) in your dashboard, select your function from the list, and click `Logs`.\n- In the logs, look for the `booted` event and check if they have consistent boot times.\n - If the boot times are similar, it’s likely an issue with your function’s code, such as a large dependency, a slow API call, or a complex computation. You can try to optimize your code, reduce the size of your dependencies, or use caching techniques to improve the performance of your function.\n - If only some of the `booted` events are slow, find the affected `region` in the metadata and submit a support request via the \"Help\" button at the top.\n\n{/* supa-mdx-lint-disable-next-line Rule001HeadingCase */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Receiving 546 Error Response",
          "content": "The 546 error response might occur because:\n\n- **Memory or CPU Limits**: The function might have exhausted its memory or encountered CPU limits enforced during execution.\n- **Event Loop Completion**: If you observe \"Event loop completed\" in your error logs, it's likely your function is not implemented correctly. You should check your function code for any syntax errors, infinite loops, or unresolved promises that might cause this error. Or you can try running the function locally (using Supabase CLI **`functions serve`**) to see if you can debug the error. The local console should give a full stack trace on the error with line numbers of the source code. You can also refer to [Edge Functions examples](https://github.com/supabase/supabase/tree/master/examples/edge-functions) for guidance.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Issues serving Edge Functions locally with the Supabase CLI",
          "content": "- Make sure you're on the latest version of the [Supabase CLI](/docs/guides/cli#updates).\n- Run the serve command with the `-debug` flag.\n- Support engineers can then try to run the provided sample code locally and see if they can reproduce the issue.\n- Search the [Edge Runtime](https://github.com/supabase/edge-runtime) and [CLI](https://github.com/supabase/cli) repos for the error message, to see if it has been reported before.\n- If the output from the commands above does not help you to resolve the issue, open a support ticket via the Supabase Dashboard (by clicking the \"Help\" button at the top right) and include all output and details about your commands.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Monitoring Edge Function resource usage",
          "content": "To determine how much memory and CPU your Edge Function consumes, follow these steps:\n\n- Navigate to the Supabase Dashboard.\n- Go to **Edge Functions**.\n- Select the specific function by clicking on its name.\n- View the resource usage **Metrics** on the charts provided.\n\n Edge Functions have limited resources (CPU, memory, and execution time) compared to traditional\n servers. Make sure your functions are optimized for performance and don't exceed the allocated\n resources.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Understanding CPU soft and hard limits",
          "content": "An isolate is like a worker that can handle multiple requests for a function. It works until a time limit of 400 seconds is reached. Now, there are two types of limits for the CPU.\n\n1. **Soft Limit**: When the isolate hits the soft limit, it retires. This means it won't take on any new requests, but it will finish processing the ones it's already working on. It keeps going until it either hits the hard limit for CPU time or reaches the 400-second time limit, whichever comes first.\n2. **Hard Limit**: If there are new requests after the soft limit is reached, a new isolate is created to handle them. The original isolate continues until it hits the hard limit or the time limit. This ensures that existing requests are completed, and new ones will be managed by a newly created isolate.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Checking function boot time",
          "content": "Check the logs for the function. In the logs, look for a \"Booted\" event and note the reported boot time. If available, click on the event to access more details, including the regions from where the function was served. Investigate if the boot time is excessively high (longer than 1 second) and note any patterns or regions where it occurs. You can refer to this guide for troubleshooting [regional invocations](/docs/guides/functions/regional-invocation).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Finding bundle size",
          "content": "To find the bundle size of a function, run the following command locally:\n\n`deno info /path/to/function/index.ts`\n\nLook for the \"size\" field in the output which represents the approximate bundle size of the function. You can find the accurate bundle size when you deploy your function via Supabase CLI. If the function is part of a larger application, consider examining the bundle size of the specific function independently.\n\nThe source code of a function is subject to 10MB site limit.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Analyze dependencies",
          "content": "When analyzing dependencies for your Supabase Edge Functions, it's essential to review both Deno and NPM dependencies to ensure optimal performance and resource utilization.\nBy selectively importing only the required submodules, you can effectively reduce the size of your function's dependencies and optimize its performance.\nBefore finalizing your imports, ensure to review both Deno and NPM dependencies, checking for any unnecessary or redundant dependencies that can be removed. Additionally, check for outdated dependencies and update to the latest versions if possible.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Deno dependencies",
          "content": "Run `deno info`, providing the path to your input map if you use one.\nReview the dependencies listed in the output. Pay attention to any significantly large dependencies, as they can contribute to increased bundle size and potential boot time issues.\nExamine if there are any unnecessary or redundant dependencies that can be removed. Check for outdated dependencies and update to the latest versions if possible.\n\n```bash\ndeno info --import-map=/path/to/import_map.json /path/to/function/index.ts\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "NPM dependencies",
          "content": "Additionally, if you utilize NPM modules in your Edge Functions, it's crucial to be mindful of their size and impact on the overall bundle size. While importing NPM modules, consider using the notation `import { submodule } from 'npm:package/submodule'` to selectively import specific submodules rather than importing the entire package. This approach can help minimize unnecessary overhead and streamline the execution of your function.\n\nFor example, if you only need the `Sheets` submodule from the `googleapis` package, you can import it like this:\n\n```typescript\nimport { Sheets } from 'npm:@googleapis/sheets'\n```",
          "level": 4
        }
      ],
      "wordCount": 1215,
      "characterCount": 7865
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-unit-test",
      "identifier": "functions-unit-test",
      "name": "Testing your Edge Functions",
      "description": "Writing Unit Tests for Edge Functions using Deno Test",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/unit-test",
      "dateModified": "2025-06-13T12:45:11.304507",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/unit-test.mdx",
      "frontmatter": {
        "id": "unit-test",
        "title": "Testing your Edge Functions",
        "description": "Writing Unit Tests for Edge Functions using Deno Test",
        "subtitle": "Writing Unit Tests for Edge Functions using Deno Test"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Testing is an essential step in the development process to ensure the correctness and performance of your Edge Functions."
        },
        {
          "type": "section",
          "title": "Testing in Deno",
          "content": "Deno has a built-in test runner that you can use for testing JavaScript or TypeScript code. You can read the [official documentation](https://docs.deno.com/runtime/manual/basics/testing/) for more information and details about the available testing functions.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Folder structure",
          "content": "We recommend creating your testing in a `supabase/functions/tests` directory, using the same name as the Function followed by `-test.ts`:\n\n```bash\n└── supabase\n ├── functions\n │ ├── function-one\n │ │ └── index.ts\n │ └── function-two\n │ │ └── index.ts\n │ └── tests\n │ └── function-one-test.ts # Tests for function-one\n │ └── function-two-test.ts # Tests for function-two\n └── config.toml\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Example script",
          "content": "The following script is a good example to get started with testing your Edge Functions:\n\n```typescript function-one-test.ts\n// Import required libraries and modules\nimport { assert, assertEquals } from 'jsr:@std/assert@1'\nimport { createClient, SupabaseClient } from 'npm:@supabase/supabase-js@2'\n\n// Will load the .env file to Deno.env\nimport 'jsr:@std/dotenv/load'\n\n// Set up the configuration for the Supabase client\nconst supabaseUrl = Deno.env.get('SUPABASE_URL') ?? ''\nconst supabaseKey = Deno.env.get('SUPABASE_ANON_KEY') ?? ''\nconst options = {\n auth: {\n autoRefreshToken: false,\n persistSession: false,\n detectSessionInUrl: false,\n },\n}\n\n// Test the creation and functionality of the Supabase client\nconst testClientCreation = async () => {\n var client: SupabaseClient = createClient(supabaseUrl, supabaseKey, options)\n\n // Verify if the Supabase URL and key are provided\n if (!supabaseUrl) throw new Error('supabaseUrl is required.')\n if (!supabaseKey) throw new Error('supabaseKey is required.')\n\n // Test a simple query to the database\n const { data: table_data, error: table_error } = await client\n .from('my_table')\n .select('*')\n .limit(1)\n if (table_error) {\n throw new Error('Invalid Supabase client: ' + table_error.message)\n }\n assert(table_data, 'Data should be returned from the query.')\n}\n\n// Test the 'hello-world' function\nconst testHelloWorld = async () => {\n var client: SupabaseClient = createClient(supabaseUrl, supabaseKey, options)\n\n // Invoke the 'hello-world' function with a parameter\n const { data: func_data, error: func_error } = await client.functions.invoke('hello-world', {\n body: { name: 'bar' },\n })\n\n // Check for errors from the function invocation\n if (func_error) {\n throw new Error('Invalid response: ' + func_error.message)\n }\n\n // Log the response from the function\n console.log(JSON.stringify(func_data, null, 2))\n\n // Assert that the function returned the expected result\n assertEquals(func_data.message, 'Hello bar!')\n}\n\n// Register and run the tests\nDeno.test('Client Creation Test', testClientCreation)\nDeno.test('Hello-world Function Test', testHelloWorld)\n```\n\nThis test case consists of two parts. The first part tests the client library and verifies that the database can be connected to and returns values from a table (`my_table`). The second part tests the edge function and checks if the received value matches the expected value. Here's a brief overview of the code:\n\n- We import various testing functions from the Deno standard library, including `assert`, `assertExists`, and `assertEquals`.\n- We import the `createClient` and `SupabaseClient` classes from the `@supabase/supabase-js` library to interact with the Supabase client.\n- We define the necessary configuration for the Supabase client, including the Supabase URL, API key, and authentication options.\n- The `testClientCreation` function tests the creation of a Supabase client instance and queries the database for data from a table. It verifies that data is returned from the query.\n- The `testHelloWorld` function tests the \"Hello-world\" Edge Function by invoking it using the Supabase client's `functions.invoke` method. It checks if the response message matches the expected greeting.\n- We run the tests using the `Deno.test` function, providing a descriptive name for each test case and the corresponding test function.\n\nMake sure to replace the placeholders (`supabaseUrl`, `supabaseKey`, `my_table`) with the actual values relevant to your Supabase setup.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Running Edge Functions locally",
          "content": "To locally test and debug Edge Functions, you can utilize the Supabase CLI. Let's explore how to run Edge Functions locally using the Supabase CLI:\n\n1. Ensure that the Supabase server is running by executing the following command:\n\n ```bash\n supabase start\n ```\n\n2. In your terminal, use the following command to serve the Edge Functions locally:\n\n ```bash\n supabase functions serve\n ```\n\n This command starts a local server that runs your Edge Functions, enabling you to test and debug them in a development environment.\n\n3. Create the environment variables file:\n\n ```bash\n # creates the file\n touch .env\n # adds the SUPABASE_URL secret\n echo \"SUPABASE_URL=http://localhost:54321\" >> .env\n # adds the SUPABASE_ANON_KEY secret\n echo \"SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0\" >> .env\n # Alternatively, you can open it in your editor:\n open .env\n ```\n\n4. To run the tests, use the following command in your terminal:\n\n ```bash\n deno test --allow-all supabase/functions/tests/function-one-test.ts\n ```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Full guide on Testing Supabase Edge Functions on [Mansueli's tips](https://blog.mansueli.com/testing-supabase-edge-functions-with-deno-test)",
          "level": 2
        }
      ],
      "wordCount": 759,
      "characterCount": 5648
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-wasm",
      "identifier": "functions-wasm",
      "name": "Using Wasm modules",
      "description": "How to use WebAssembly in Edge Functions.",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/wasm",
      "dateModified": "2025-06-13T12:45:11.304627",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/wasm.mdx",
      "frontmatter": {
        "id": "function-wasm",
        "title": "Using Wasm modules",
        "description": "How to use WebAssembly in Edge Functions.",
        "subtitle": "How to use WebAssembly in Edge Functions."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Functions supports running [WebAssembly (Wasm)](https://developer.mozilla.org/en-US/docs/WebAssembly) modules. WebAssembly is useful if you want to optimize code that's slower to run in JavaScript or require low-level manipulation.\n\nIt also gives you the option to port existing libraries written in other languages to be used with JavaScript. For example, [magick-wasm](https://supabase.com/docs/guides/functions/examples/image-manipulation), which does image manipulation and transforms, is a port of an existing C library to WebAssembly."
        },
        {
          "type": "section",
          "title": "Writing a Wasm module",
          "content": "You can use different languages and SDKs to write Wasm modules. For this tutorial, we will write a simple Wasm module in Rust that adds two numbers.\n\nFollow this [guide on writing Wasm modules in Rust](https://developer.mozilla.org/en-US/docs/WebAssembly/Rust_to_Wasm) to setup your dev environment.\n\nCreate a new Edge Function called `wasm-add`.\n\n```bash\nsupabase functions new wasm-add\n```\n\nCreate a new Cargo project for the Wasm module inside the function's directory:\n\n```bash\ncd supabase/functions/wasm-add\ncargo new --lib add-wasm\n```\n\nAdd the following code to `add-wasm/src/lib.rs`.\n\nUpdate the `add-wasm/Cargo.toml` to include the `wasm-bindgen` dependency.\n\nAfter that we can build the package, by running:\n\n```bash\nwasm-pack build --target deno\n```\n\nThis will produce a Wasm binary file inside `add-wasm/pkg` directory.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Calling the Wasm module from the Edge Function",
          "content": "Now let's update the Edge Function to call `add` from the Wasm module.\n\n Supabase Edge Functions currently use Deno 1.46. From [Deno 2.1, importing Wasm\n modules](https://deno.com/blog/v2.1) will require even less boilerplate code.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Bundle and deploy the Edge Function",
          "content": "Before deploying the Edge Function, we need to ensure it bundles the Wasm module with it. We can do this by defining it in the `static_files` for the function in `superbase/config.toml`.\n\n You will need update Supabase CLI to 2.7.0 or higher for the `static_files` support.\n\n```toml\n[functions.wasm-add]\nstatic_files = [ \"./functions/wasm-add/add-wasm/pkg/*\"]\n```\n\nDeploy the function by running:\n\n```bash\nsupabase functions deploy wasm-add\n```",
          "level": 3
        }
      ],
      "wordCount": 294,
      "characterCount": 2177
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:functions-websockets",
      "identifier": "functions-websockets",
      "name": "Handling WebSockets",
      "description": "How to handle WebSocket connections in Edge Functions",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/functions/websockets",
      "dateModified": "2025-06-13T12:45:11.304826",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/functions/websockets.mdx",
      "frontmatter": {
        "id": "function-WebSockets",
        "title": "Handling WebSockets",
        "description": "How to handle WebSocket connections in Edge Functions",
        "subtitle": "How to handle WebSocket connections in Edge Functions"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Edge Functions supports hosting WebSocket servers that can facilitate bi-directional communications with browser clients.\n\nYou can also establish outgoing WebSocket client connections to another server from Edge Functions (e.g., [OpenAI Realtime API](https://platform.openai.com/docs/guides/realtime/overview)). You can find an example OpenAI Realtime Relay Server implementation on the [supabase-community GitHub account](https://github.com/supabase-community/openai-realtime-console?tab=readme-ov-file#using-supabase-edge-functions-as-a-relay-server)."
        },
        {
          "type": "section",
          "title": "Writing a WebSocket server",
          "content": "Here are some basic examples of setting up WebSocket servers using Deno and Node.js APIs.\n\n```ts\nDeno.serve((req) => {\n const upgrade = req.headers.get('upgrade') || ''\n\n if (upgrade.toLowerCase() != 'WebSocket') {\n return new Response(\"request isn't trying to upgrade to WebSocket.\", { status: 400 })\n }\n\n const { socket, response } = Deno.upgradeWebSocket(req)\n\n socket.onopen = () => console.log('socket opened')\n socket.onmessage = (e) => {\n console.log('socket message:', e.data)\n socket.send(new Date().toString())\n }\n\n socket.onerror = (e) => console.log('socket errored:', e.message)\n socket.onclose = () => console.log('socket closed')\n\n return response\n})\n```\n\n```ts\nimport { createServer } from \"node:http\";\nimport { WebSocketServer } from \"npm:ws\";\n\nconst server = createServer();\n// Since we manually created the HTTP server,\n// turn on the noServer mode.\nconst wss = new WebSocketServer({ noServer: true });\n\nwss.on(\"connection\", ws => {\nconsole.log(\"socket opened\");\nws.on(\"message\", (data /** Buffer \\*/, isBinary /** bool \\*/) => {\nif (isBinary) {\nconsole.log(\"socket message:\", data);\n} else {\nconsole.log(\"socket message:\", data.toString());\n}\n\n ws.send(new Date().toString());\n });\n\n ws.on(\"error\", err => {\n console.log(\"socket errored:\", err.message);\n });\n\n ws.on(\"close\", () => console.log(\"socket closed\"));\n\n});\n\nserver.on(\"upgrade\", (req, socket, head) => {\nwss.handleUpgrade(req, socket, head, ws => {\nwss.emit(\"connection\", ws, req);\n});\n});\n\nserver.listen(8080);\n\n````",
          "level": 3
        },
        {
          "type": "section",
          "title": "Outbound WebSockets",
          "content": "You can also establish an outbound WebSocket connection to another server from an Edge Function.\n\nCombining it with incoming WebSocket servers, it's possible to use Edge Functions as a WebSocket proxy, for example as a [relay server](https://github.com/supabase-community/openai-realtime-console?tab=readme-ov-file#using-supabase-edge-functions-as-a-relay-server) for the [OpenAI Realtime API](https://platform.openai.com/docs/guides/realtime/overview).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Authentication",
          "content": "WebSocket browser clients don't have the option to send custom headers. Because of this, Edge Functions won't be able to perform the usual authorization header check to verify the JWT.\n\nYou can skip the default authorization header checks by explicitly providing `--no-verify-jwt` when serving and deploying functions.\n\nTo authenticate the user making WebSocket requests, you can pass the JWT in URL query params or via a custom protocol.\n\n```ts\n import { createClient } from \"npm:@supabase/supabase-js@2\";\n\nconst supabase = createClient(\nDeno.env.get(\"SUPABASE_URL\"),\nDeno.env.get(\"SUPABASE_SERVICE_ROLE_KEY\"),\n);\nDeno.serve(req => {\nconst upgrade = req.headers.get(\"upgrade\") || \"\";\n\n if (upgrade.toLowerCase() != \"WebSocket\") {\n return new Response(\"request isn't trying to upgrade to WebSocket.\", { status: 400 });\n }\n\n// Please be aware query params may be logged in some logging systems.\nconst url = new URL(req.url);\nconst jwt = url.searchParams.get(\"jwt\");\nif (!jwt) {\nconsole.error(\"Auth token not provided\");\nreturn new Response(\"Auth token not provided\", { status: 403 });\n}\nconst { error, data } = await supabase.auth.getUser(jwt);\nif (error) {\nconsole.error(error);\nreturn new Response(\"Invalid token provided\", { status: 403 });\n}\nif (!data.user) {\nconsole.error(\"user is not authenticated\");\nreturn new Response(\"User is not authenticated\", { status: 403 });\n}\n\n const { socket, response } = Deno.upgradeWebSocket(req);\n\n socket.onopen = () => console.log(\"socket opened\");\n socket.onmessage = (e) => {\n console.log(\"socket message:\", e.data);\n socket.send(new Date().toString());\n };\n\n socket.onerror = e => console.log(\"socket errored:\", e.message);\n socket.onclose = () => console.log(\"socket closed\");\n\n return response;\n\n});\n\n````\n\n```ts\n import { createClient } from \"npm:@supabase/supabase-js@2\";\n\nconst supabase = createClient(\nDeno.env.get(\"SUPABASE_URL\"),\nDeno.env.get(\"SUPABASE_SERVICE_ROLE_KEY\"),\n);\nDeno.serve(req => {\nconst upgrade = req.headers.get(\"upgrade\") || \"\";\n\n if (upgrade.toLowerCase() != \"WebSocket\") {\n return new Response(\"request isn't trying to upgrade to WebSocket.\", { status: 400 });\n }\n\n// Sec-WebScoket-Protocol may return multiple protocol values `jwt-TOKEN, value1, value 2`\nconst customProtocols = (req.headers.get(\"Sec-WebSocket-Protocol\") ?? '').split(',').map(p => p.trim())\nconst jwt = customProtocols.find(p => p.startsWith('jwt')).replace('jwt-', '')\nif (!jwt) {\nconsole.error(\"Auth token not provided\");\nreturn new Response(\"Auth token not provided\", { status: 403 });\n}\nconst { error, data } = await supabase.auth.getUser(jwt);\nif (error) {\nconsole.error(error);\nreturn new Response(\"Invalid token provided\", { status: 403 });\n}\nif (!data.user) {\nconsole.error(\"user is not authenticated\");\nreturn new Response(\"User is not authenticated\", { status: 403 });\n}\n\n const { socket, response } = Deno.upgradeWebSocket(req);\n\n socket.onopen = () => console.log(\"socket opened\");\n socket.onmessage = (e) => {\n console.log(\"socket message:\", e.data);\n socket.send(new Date().toString());\n };\n\n socket.onerror = e => console.log(\"socket errored:\", e.message);\n socket.onclose = () => console.log(\"socket closed\");\n\n return response;\n\n});\n\n````",
          "level": 3
        },
        {
          "type": "section",
          "title": "Limits",
          "content": "The maximum duration is capped based on the wall-clock, CPU, and memory limits. The Function will shutdown when it reaches one of these [limits](/docs/guides/functions/limits).",
          "level": 3
        },
        {
          "type": "section",
          "title": "Testing WebSockets locally",
          "content": "When testing Edge Functions locally with Supabase CLI, the instances are terminated automatically after a request is completed. This will prevent keeping WebSocket connections open.\n\nTo prevent that, you can update the `supabase/config.toml` with the following settings:\n\n```toml\n[edge_runtime]\npolicy = \"per_worker\"\n````\n\nWhen running with `per_worker` policy, Function won't auto-reload on edits. You will need to manually restart it by running `supabase functions serve`.",
          "level": 3
        }
      ],
      "wordCount": 782,
      "characterCount": 6479
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:queues-api",
      "identifier": "queues-api",
      "name": "API",
      "description": "",
      "category": "queues",
      "url": "https://supabase.com/docs/guides/queues/api",
      "dateModified": "2025-06-13T12:45:11.304939",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/queues/api.mdx",
      "frontmatter": {
        "title": "API"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "{/* */}\nWhen you create a Queue in Supabase, you can choose to create helper database functions in the `pgmq_public` schema. This schema exposes operations to manage Queue Messages to consumers client-side, but does not expose functions for creating or dropping Queues.\n\nDatabase functions in `pgmq_public` can be exposed via Supabase Data API so consumers client-side can call them. Visit the [Quickstart](/docs/guides/queues/quickstart) for an example."
        },
        {
          "type": "section",
          "title": "`pgmq_public.pop(queue_name)`",
          "content": "Retrieves the next available message and deletes it from the specified Queue.\n\n- `queue_name` (`text`): Queue name\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "`pgmq_public.send(queue_name, message, sleep_seconds)`",
          "content": "Adds a Message to the specified Queue, optionally delaying its visibility to all consumers by a number of seconds.\n\n- `queue_name` (`text`): Queue name\n- `message` (`jsonb`): Message payload to send\n- `sleep_seconds` (`integer`, optional): Delay message visibility by specified seconds. Defaults to 0\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "`pgmq_public.send_batch(queue_name, messages, sleep_seconds)`",
          "content": "Adds a batch of Messages to the specified Queue, optionally delaying their availability to all consumers by a number of seconds.\n\n- `queue_name` (`text`): Queue name\n- `messages` (`jsonb[]`): Array of message payloads to send\n- `sleep_seconds` (`integer`, optional): Delay messages visibility by specified seconds. Defaults to 0\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "`pgmq_public.archive(queue_name, message_id)`",
          "content": "Archives a Message by moving it from the Queue table to the Queue's archive table.\n\n- `queue_name` (`text`): Queue name\n- `message_id` (`bigint`): ID of the Message to archive\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "`pgmq_public.delete(queue_name, message_id)`",
          "content": "Permanently deletes a Message from the specified Queue.\n\n- `queue_name` (`text`): Queue name\n- `message_id` (`bigint`): ID of the Message to delete\n\n---",
          "level": 3
        },
        {
          "type": "section",
          "title": "`pgmq_public.read(queue_name, sleep_seconds, n)`",
          "content": "Reads up to \"n\" Messages from the specified Queue with an optional \"sleep_seconds\" (visibility timeout).\n\n- `queue_name` (`text`): Queue name\n- `sleep_seconds` (`integer`): Visibility timeout in seconds\n- `n` (`integer`): Maximum number of Messages to read",
          "level": 3
        }
      ],
      "wordCount": 286,
      "characterCount": 2128
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:queues-pgmq",
      "identifier": "queues-pgmq",
      "name": "PGMQ Extension",
      "description": "",
      "category": "queues",
      "url": "https://supabase.com/docs/guides/queues/pgmq",
      "dateModified": "2025-06-13T12:45:11.305497",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/queues/pgmq.mdx",
      "frontmatter": {
        "title": "PGMQ Extension"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "pgmq is a lightweight message queue built on Postgres."
        },
        {
          "type": "section",
          "title": "Features",
          "content": "- Lightweight - No background worker or external dependencies, just Postgres functions packaged in an extension\n- \"exactly once\" delivery of messages to a consumer within a visibility timeout\n- API parity with AWS SQS and RSMQ\n- Messages stay in the queue until explicitly removed\n- Messages can be archived, instead of deleted, for long-term retention and replayability",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable the extension",
          "content": "```sql\ncreate extension pgmq;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "`create`",
          "content": "Create a new queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.create(queue_name text)\nreturns void\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect from pgmq.create('my_queue');\n create\n--------\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "`create_unlogged`",
          "content": "Creates an unlogged table. This is useful when write throughput is more important than durability.\nSee Postgres documentation for [unlogged tables](https://www.postgresql.org/docs/current/sql-createtable.html#SQL-CREATETABLE-UNLOGGED) for more information.\n\n{/* prettier-ignore */}\n```sql\npgmq.create_unlogged(queue_name text)\nreturns void\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect pgmq.create_unlogged('my_unlogged');\n create_unlogged\n-----------------\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`detach_archive`",
          "content": "Drop the queue's archive table as a member of the PGMQ extension. Useful for preventing the queue's archive table from being drop when `drop extension pgmq` is executed.\nThis does not prevent the further archives() from appending to the archive table.\n\n{/* prettier-ignore */}\n```sql\npgmq.detach_archive(queue_name text)\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.detach_archive('my_queue');\n detach_archive\n----------------\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`drop_queue`",
          "content": "Deletes a queue and its archive table.\n\n{/* prettier-ignore */}\n```sql\npgmq.drop_queue(queue_name text)\nreturns boolean\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.drop_queue('my_unlogged');\n drop_queue\n------------\n t\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "`send`",
          "content": "Send a single message to a queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.send(\n queue_name text,\n msg jsonb,\n delay integer default 0\n)\nreturns setof bigint\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :-------- | :----------------------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `msg` | `jsonb` | The message to send to the queue |\n| `delay` | `integer` | Time in seconds before the message becomes visible. Defaults to 0. |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.send('my_queue', '{\"hello\": \"world\"}');\n send\n------\n 4\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`send_batch`",
          "content": "Send 1 or more messages to a queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.send_batch(\n queue_name text,\n msgs jsonb[],\n delay integer default 0\n)\nreturns setof bigint\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :-------- | :------------------------------------------------------------------ |\n| `queue_name` | `text` | The name of the queue |\n| `msgs` | `jsonb[]` | Array of messages to send to the queue |\n| `delay` | `integer` | Time in seconds before the messages becomes visible. Defaults to 0. |\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.send_batch(\n 'my_queue',\n array[\n '{\"hello\": \"world_0\"}'::jsonb,\n '{\"hello\": \"world_1\"}'::jsonb\n ]\n);\n send_batch\n------------\n 1\n 2\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`read`",
          "content": "Read 1 or more messages from a queue. The VT specifies the delay in seconds between reading and the message becoming invisible to other consumers.\n\n{/* prettier-ignore */}\n```sql\npgmq.read(\n queue_name text,\n vt integer,\n qty integer\n)\n\nreturns setof pgmq.message_record\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :-------- | :-------------------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `vt` | `integer` | Time in seconds that the message become invisible after reading |\n| `qty` | `integer` | The number of messages to read from the queue. Defaults to 1 |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.read('my_queue', 10, 2);\n msg_id | read_ct | enqueued_at | vt | message\n--------+---------+-------------------------------+-------------------------------+----------------------\n 1 | 1 | 2023-10-28 19:14:47.356595-05 | 2023-10-28 19:17:08.608922-05 | {\"hello\": \"world_0\"}\n 2 | 1 | 2023-10-28 19:14:47.356595-05 | 2023-10-28 19:17:08.608974-05 | {\"hello\": \"world_1\"}\n(2 rows)\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`read_with_poll`",
          "content": "Same as read(). Also provides convenient long-poll functionality.\nWhen there are no messages in the queue, the function call will wait for `max_poll_seconds` in duration before returning.\nIf messages reach the queue during that duration, they will be read and returned immediately.\n\n{/* prettier-ignore */}\n```sql\n pgmq.read_with_poll(\n queue_name text,\n vt integer,\n qty integer,\n max_poll_seconds integer default 5,\n poll_interval_ms integer default 100\n)\nreturns setof pgmq.message_record\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------------- | :-------- | :-------------------------------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `vt` | `integer` | Time in seconds that the message become invisible after reading. |\n| `qty` | `integer` | The number of messages to read from the queue. Defaults to 1. |\n| `max_poll_seconds` | `integer` | Time in seconds to wait for new messages to reach the queue. Defaults to 5. |\n| `poll_interval_ms` | `integer` | Milliseconds between the internal poll operations. Defaults to 100. |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.read_with_poll('my_queue', 1, 1, 5, 100);\n msg_id | read_ct | enqueued_at | vt | message\n--------+---------+-------------------------------+-------------------------------+--------------------\n 1 | 1 | 2023-10-28 19:09:09.177756-05 | 2023-10-28 19:27:00.337929-05 | {\"hello\": \"world\"}\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`pop`",
          "content": "Reads a single message from a queue and deletes it upon read.\n\nNote: utilization of pop() results in at-most-once delivery semantics if the consuming application does not guarantee processing of the message.\n\n{/* prettier-ignore */}\n```sql\npgmq.pop(queue_name text)\nreturns setof pgmq.message_record\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\npgmq=# select * from pgmq.pop('my_queue');\n msg_id | read_ct | enqueued_at | vt | message\n--------+---------+-------------------------------+-------------------------------+--------------------\n 1 | 2 | 2023-10-28 19:09:09.177756-05 | 2023-10-28 19:27:00.337929-05 | {\"hello\": \"world\"}\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`delete` (single)",
          "content": "Deletes a single message from a queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.delete (queue_name text, msg_id: bigint)\nreturns boolean\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :------- | :---------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `msg_id` | `bigint` | Message ID of the message to delete |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect pgmq.delete('my_queue', 5);\n delete\n--------\n t\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`delete` (batch)",
          "content": "Delete one or many messages from a queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.delete (queue_name text, msg_ids: bigint[])\nreturns setof bigint\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :--------- | :----------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `msg_ids` | `bigint[]` | Array of message IDs to delete |\n\nExamples:\n\nDelete two messages that exist.\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.delete('my_queue', array[2, 3]);\n delete\n--------\n 2\n 3\n```\n\nDelete two messages, one that exists and one that does not. Message `999` does not exist.\n\n```sql\nselect * from pgmq.delete('my_queue', array[6, 999]);\n delete\n--------\n 6\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`purge_queue`",
          "content": "Permanently deletes all messages in a queue. Returns the number of messages that were deleted.\n\n```text\npurge_queue(queue_name text)\nreturns bigint\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\nExample:\n\nPurge the queue when it contains 8 messages;\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.purge_queue('my_queue');\n purge_queue\n-------------\n 8\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`archive` (single)",
          "content": "Removes a single requested message from the specified queue and inserts it into the queue's archive.\n\n{/* prettier-ignore */}\n```sql\npgmq.archive(queue_name text, msg_id bigint)\nreturns boolean\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :------- | :----------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `msg_id` | `bigint` | Message ID of the message to archive |\n\nReturns\nBoolean value indicating success or failure of the operation.\n\nExample; remove message with ID 1 from queue `my_queue` and archive it:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.archive('my_queue', 1);\n archive\n---------\n t\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`archive` (batch)",
          "content": "Deletes a batch of requested messages from the specified queue and inserts them into the queue's archive.\nReturns an array of message ids that were successfully archived.\n\n```text\npgmq.archive(queue_name text, msg_ids bigint[])\nRETURNS SETOF bigint\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :--------- | :------------------------------ |\n| `queue_name` | `text` | The name of the queue |\n| `msg_ids` | `bigint[]` | Array of message IDs to archive |\n\nExamples:\n\nDelete messages with ID 1 and 2 from queue `my_queue` and move to the archive.\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.archive('my_queue', array[1, 2]);\n archive\n---------\n 1\n 2\n```\n\nDelete messages 4, which exists and 999, which does not exist.\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.archive('my_queue', array[4, 999]);\n archive\n---------\n 4\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`set_vt`",
          "content": "Sets the visibility timeout of a message to a specified time duration in the future. Returns the record of the message that was updated.\n\n{/* prettier-ignore */}\n```sql\npgmq.set_vt(\n queue_name text,\n msg_id bigint,\n vt_offset integer\n)\nreturns pgmq.message_record\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :----------- | :-------- | :-------------------------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `msg_id` | `bigint` | ID of the message to set visibility time |\n| `vt_offset` | `integer` | Duration from now, in seconds, that the message's VT should be set to |\n\nExample:\n\nSet the visibility timeout of message 1 to 30 seconds from now.\n\n```sql\nselect * from pgmq.set_vt('my_queue', 11, 30);\n msg_id | read_ct | enqueued_at | vt | message\n--------+---------+-------------------------------+-------------------------------+----------------------\n 1 | 0 | 2023-10-28 19:42:21.778741-05 | 2023-10-28 19:59:34.286462-05 | {\"hello\": \"world_0\"}\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`list_queues`",
          "content": "List all the queues that currently exist.\n\n{/* prettier-ignore */}\n```sql\nlist_queues()\nRETURNS TABLE(\n queue_name text,\n created_at timestamp with time zone,\n is_partitioned boolean,\n is_unlogged boolean\n)\n```\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.list_queues();\n queue_name | created_at | is_partitioned | is_unlogged\n----------------------+-------------------------------+----------------+-------------\n my_queue | 2023-10-28 14:13:17.092576-05 | f | f\n my_partitioned_queue | 2023-10-28 19:47:37.098692-05 | t | f\n my_unlogged | 2023-10-28 20:02:30.976109-05 | f | t\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`metrics`",
          "content": "Get metrics for a specific queue.\n\n{/* prettier-ignore */}\n```sql\npgmq.metrics(queue_name: text)\nreturns table(\n queue_name text,\n queue_length bigint,\n newest_msg_age_sec integer,\n oldest_msg_age_sec integer,\n total_messages bigint,\n scrape_time timestamp with time zone\n)\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n| :--------- | :--- | :-------------------- |\n| queue_name | text | The name of the queue |\n\n**Returns:**\n\n| Attribute | Type | Description |\n| :------------------- | :------------------------- | :------------------------------------------------------------------------ | -------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `queue_length` | `bigint` | Number of messages currently in the queue |\n| `newest_msg_age_sec` | `integer | null` | Age of the newest message in the queue, in seconds |\n| `oldest_msg_age_sec` | `integer | null` | Age of the oldest message in the queue, in seconds |\n| `total_messages` | `bigint` | Total number of messages that have passed through the queue over all time |\n| `scrape_time` | `timestamp with time zone` | The current timestamp |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.metrics('my_queue');\n queue_name | queue_length | newest_msg_age_sec | oldest_msg_age_sec | total_messages | scrape_time\n------------+--------------+--------------------+--------------------+----------------+-------------------------------\n my_queue | 16 | 2445 | 2447 | 35 | 2023-10-28 20:23:08.406259-05\n```\n\n---",
          "level": 4
        },
        {
          "type": "section",
          "title": "`metrics_all`",
          "content": "Get metrics for all existing queues.\n\n```text\npgmq.metrics_all()\nRETURNS TABLE(\n queue_name text,\n queue_length bigint,\n newest_msg_age_sec integer,\n oldest_msg_age_sec integer,\n total_messages bigint,\n scrape_time timestamp with time zone\n)\n```\n\n**Returns:**\n\n| Attribute | Type | Description |\n| :------------------- | :------------------------- | :------------------------------------------------------------------------ | -------------------------------------------------- |\n| `queue_name` | `text` | The name of the queue |\n| `queue_length` | `bigint` | Number of messages currently in the queue |\n| `newest_msg_age_sec` | `integer | null` | Age of the newest message in the queue, in seconds |\n| `oldest_msg_age_sec` | `integer | null` | Age of the oldest message in the queue, in seconds |\n| `total_messages` | `bigint` | Total number of messages that have passed through the queue over all time |\n| `scrape_time` | `timestamp with time zone` | The current timestamp |\n\n{/* prettier-ignore */}\n```sql\nselect * from pgmq.metrics_all();\n queue_name | queue_length | newest_msg_age_sec | oldest_msg_age_sec | total_messages | scrape_time\n----------------------+--------------+--------------------+--------------------+----------------+-------------------------------\n my_queue | 16 | 2563 | 2565 | 35 | 2023-10-28 20:25:07.016413-05\n my_partitioned_queue | 1 | 11 | 11 | 1 | 2023-10-28 20:25:07.016413-05\n my_unlogged | 1 | 3 | 3 | 1 | 2023-10-28 20:25:07.016413-05\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "`message_record`",
          "content": "The complete representation of a message in a queue.\n\n| Attribute Name | Type | Description |\n| :------------- | :------------------------- | :--------------------------------------------------------------------- |\n| `msg_id` | `bigint` | Unique ID of the message |\n| `read_ct` | `bigint` | Number of times the message has been read. Increments on read(). |\n| `enqueued_at` | `timestamp with time zone` | time that the message was inserted into the queue |\n| `vt` | `timestamp with time zone` | Timestamp when the message will become available for consumers to read |\n| `message` | `jsonb` | The message payload |\n\nExample:\n\n{/* prettier-ignore */}\n```sql\n msg_id | read_ct | enqueued_at | vt | message\n--------+---------+-------------------------------+-------------------------------+--------------------\n 1 | 1 | 2023-10-28 19:06:19.941509-05 | 2023-10-28 19:06:27.419392-05 | {\"hello\": \"world\"}\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "- Official Docs: [pgmq/api](https://pgmq.github.io/pgmq/#creating-a-queue)",
          "level": 2
        }
      ],
      "wordCount": 2273,
      "characterCount": 16508
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:queues-quickstart",
      "identifier": "queues-quickstart",
      "name": "Quickstart",
      "description": "",
      "category": "queues",
      "url": "https://supabase.com/docs/guides/queues/quickstart",
      "dateModified": "2025-06-13T12:45:11.305743",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/queues/quickstart.mdx",
      "frontmatter": {
        "title": "Quickstart",
        "subtitle": "Learn how to use Supabase Queues to add and read messages"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "{/* */}\nThis guide is an introduction to interacting with Supabase Queues via the Dashboard and official client library. Check out [Queues API Reference](/docs/guides/queues/api) for more details on our API."
        },
        {
          "type": "section",
          "title": "Concepts",
          "content": "Supabase Queues is a pull-based Message Queue consisting of three main components: Queues, Messages, and Queue Types.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Pull-Based Queue",
          "content": "A pull-based Queue is a Message storage and delivery system where consumers actively fetch Messages when they're ready to process them - similar to constantly refreshing a webpage to display the latest updates. Our pull-based Queues process Messages in a First-In-First-Out (FIFO) manner without priority levels.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Message",
          "content": "A Message in a Queue is a JSON object that is stored until a consumer explicitly processes and removes it, like a task waiting in a to-do list until someone checks and completes it.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Queue types",
          "content": "Supabase Queues offers three types of Queues:\n\n- **Basic Queue**: A durable Queue that stores Messages in a logged table.\n- **Unlogged Queue**: A transient Queue that stores Messages in an unlogged table for better performance but may result in loss of Queue Messages.\n\n- **Partitioned Queue** (_Coming Soon_): A durable and scalable Queue that stores Messages in multiple table partitions for better performance.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create Queues",
          "content": "To get started, navigate to the [Supabase Queues](/dashboard/project/_/integrations/queues/overview) Postgres Module under Integrations in the Dashboard and enable the `pgmq` extension.\n\n`pgmq` extension is available in Postgres version 15.6.1.143 or later.\n\nOn the [Queues page](/dashboard/project/_/integrations/queues/queues):\n\n- Click **Add a new queue** button\n\nIf you've already created a Queue click the **Create a queue** button instead.\n\n- Name your queue\n\nQueue names can only be lowercase and hyphens and underscores are permitted.\n\n- Select your [Queue Type](#queue-types)",
          "level": 2
        },
        {
          "type": "section",
          "title": "What happens when you create a queue?",
          "content": "Every new Queue creates two tables in the `pgmq` schema. These tables are `pgmq.q_` to store and process active messages and `pgmq.a_` to store any archived messages.\n\nA \"Basic Queue\" will create `pgmq.q_` and `pgmq.a_` tables as logged tables.\n\nHowever, an \"Unlogged Queue\" will create `pgmq.q_` as an unlogged table for better performance while sacrificing durability. The `pgmq.a_` table will still be created as a logged table so your archived messages remain safe and secure.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Expose Queues to client-side consumers",
          "content": "Queues, by default, are not exposed over Supabase Data API and are only accessible via Postgres clients.\n\nHowever, you may grant client-side consumers access to your Queues by enabling the Supabase Data API and granting permissions to the Queues API, which is a collection of database functions in the `pgmq_public` schema that wraps the database functions in the `pgmq` schema.\n\nThis is to prevent direct access to the `pgmq` schema and its tables (RLS is not enabled by default on any tables) and database functions.\n\nTo get started, navigate to the Queues [Settings page](/dashboard/project/_/integrations/queues/settings) and toggle on “Expose Queues via PostgREST”. Once enabled, Supabase creates and exposes a `pgmq_public` schema containing database function wrappers to a subset of `pgmq`'s database functions.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Enable RLS on your tables in `pgmq` schema",
          "content": "For security purposes, you must enable Row Level Security (RLS) on all Queue tables (all tables in `pgmq` schema that begin with `q_`) if the Data API is enabled.\n\nYou’ll want to create RLS policies for any Queues you want your client-side consumers to interact with.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Grant permissions to `pgmq_public` database functions",
          "content": "On top of enabling RLS and writing RLS policies on the underlying Queue tables, you must grant the correct permissions to the `pgmq_public` database functions for each Data API role.\n\nThe permissions required for each Queue API database function:\n\n| **Operations** | **Permissions Required** |\n| ------------------- | ------------------------ |\n| `send` `send_batch` | `Select` `Insert` |\n| `read` `pop` | `Select` `Update` |\n| `archive` `delete` | `Select` `Delete` |\n\nTo manage your queue permissions, click on the Queue Settings button.\n\nThen enable the required roles permissions.\n\n`postgres` and `service_role` roles should never be exposed client-side.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Enqueueing and dequeueing messages",
          "content": "Once your Queue has been created, you can begin enqueueing and dequeueing Messages.\n\nHere's a TypeScript example using the official Supabase client library:\n\n```tsx\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabaseUrl = 'supabaseURL'\nconst supabaseKey = 'supabaseKey'\n\nconst supabase = createClient(supabaseUrl, supabaseKey)\n\nconst QueuesTest: React.FC = () => {\n //Add a Message\n const sendToQueue = async () => {\n const result = await supabase.schema('pgmq_public').rpc('send', {\n queue_name: 'foo',\n message: { hello: 'world' },\n sleep_seconds: 30,\n })\n console.log(result)\n }\n\n //Dequeue Message\n const popFromQueue = async () => {\n const result = await supabase.schema('pgmq_public').rpc('pop', { queue_name: 'foo' })\n console.log(result)\n }\n\n return (\n \n Queue Test Component\n \n Add Message\n\n Pop Message\n\n )\n}\n\nexport default QueuesTest\n```",
          "level": 3
        }
      ],
      "wordCount": 766,
      "characterCount": 5239
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:resources-glossary",
      "identifier": "resources-glossary",
      "name": "Glossary",
      "description": "Definitions for terminology and acronyms used in the Supabase documentation.",
      "category": "resources",
      "url": "https://supabase.com/docs/guides/resources/glossary",
      "dateModified": "2025-06-13T12:45:11.306075",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/resources/glossary.mdx",
      "frontmatter": {
        "id": "glossary",
        "title": "Glossary",
        "description": "Definitions for terminology and acronyms used in the Supabase documentation."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Definitions for terminology and acronyms used in the Supabase documentation."
        },
        {
          "type": "section",
          "title": "Access token",
          "content": "An access token is a short-lived (usually no more than 1 hour) token that authorizes a client to access resources on a server. It comes in the form of a [JSON Web Token (JWT)](#json-web-token-jwt).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Authentication",
          "content": "Authentication (often abbreviated `authn.`) is the process of verifying the identity of a user. Verification of the identity of a user can happen in multiple ways:\n\n1. Asking users for something they know. For example: password, passphrase.\n2. Checking that users have access to something they own. For example: an email address, a phone number, a hardware key, recovery codes.\n3. Confirming that users have some biological features. For example: a fingerprint, a certain facial structure, an iris print.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Authenticator app",
          "content": "An authenticator app generates time-based one-time passwords (TOTPs). These passwords are generated based off a long and difficult to guess secret string. The secret is initially passed to the application by scanning a QR code.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Authorization",
          "content": "Authorization (often abbreviated `authz.`) is the process of verifying if a certain identity is allowed to access resources. Authorization often occurs by verifying an access token.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Identity provider",
          "content": "An identity provider is software or service that allows third-party applications to identify users without the exchange of passwords. Social login and enterprise single-sign on won't be possible without identity providers.\n\nSocial login platforms typically use the OAuth protocol, while enterprise single-sign on is based on the OIDC or SAML protocols.",
          "level": 2
        },
        {
          "type": "section",
          "title": "JSON Web Token (JWT)",
          "content": "A [JSON Web Token](https://jwt.io/introduction) is a type of data structure, represented as a string, that usually contains identity and authorization information about a user. It encodes information about its lifetime and is signed with cryptographic key making it tamper resistant.\n\nAccess tokens are JWTs and by inspecting the information they contain you can allow or deny access to resources. Row level security policies are based on the information present in JWTs.",
          "level": 2
        },
        {
          "type": "section",
          "title": "JWT signing secret",
          "content": "JWTs issued by Supabase are signed using the HMAC-SHA256 algorithm. The secret key used in the signing is called the JWT signing secret. You should not share this secret with someone or some thing you don't trust, nor should you post it publicly. Anyone with access to the secret can create arbitrary JWTs.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Multi-factor authentication (MFA or 2FA)",
          "content": "Multi-factor authentication is the process of authenticating a user's identity by using a combination of factors: something users know, something users have or something they are.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Nonce",
          "content": "Nonce means number used once. In reality though, it is a unique and difficult to guess string used to either initialize a protocol or algorithm securely, or detect abuse in various forms of replay attacks.",
          "level": 2
        },
        {
          "type": "section",
          "title": "OAuth",
          "content": "OAuth is a protocol allowing third-party applications to request and receive authorization from their users. It is typically used to implement social login, and serves as a base for enterprise single-sign on in the OIDC protocol. Applications can request different levels of access, including basic user identification information such as name, email address, and user ID.",
          "level": 2
        },
        {
          "type": "section",
          "title": "OIDC",
          "content": "OIDC stands for OpenID Connect and is a protocol that enables single-sign on for enterprises. OIDC is based on modern web technologies such as OAuth and JSON Web Tokens. It is commonly used instead of the older SAML protocol.",
          "level": 2
        },
        {
          "type": "section",
          "title": "One-time password (OTP)",
          "content": "A one-time password is a short, randomly generated and difficult to guess password or code that is sent to a device (like a phone number) or generated by a device or application.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Password hashing function",
          "content": "Password hashing functions are specially-designed algorithms that allow web servers to verify a password without storing it as-is. Unlike other difficult to guess strings generated from secure random number generators, passwords are picked by users and often are easy to guess by attackers. These algorithms slow down and make it very costly for attackers to guess passwords.\n\nThere are three generally accepted password hashing functions: Argon2, bcrypt and scrypt.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Password strength",
          "content": "Password strength is a measurement of how difficult a password is to guess. Simple measurement includes calculating the number of possibilities given the types of characters used in the password. For example a password of only letters has fewer variations than ones with letters and digits. Better measurements include strategies such as looking for similarity to words, phrases or already known passwords.",
          "level": 2
        },
        {
          "type": "section",
          "title": "PKCE",
          "content": "Proof Key for Code Exchange is an extension to the OAuth protocol that enables secure exchange of refresh and access tokens between an application (web app, single-page app or mobile app) and the authorization server. It is used in places where the exchange of the refresh and access token may be intercepted by third parties such as other applications running in the operating system. This is a common problem on mobile devices where the operating system may hand out URLs to other applications. This can sometimes be also exploited in single-page apps too.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Provider refresh token",
          "content": "A provider refresh token is a refresh token issued by a third-party identity provider which can be used to refresh the provider token returned.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Provider tokens",
          "content": "A provider token is a long-lived token issued by a third-party identity provider. These are issued by social login services (e.g., Google, Twitter, Apple, Microsoft) and uniquely identify a user on those platforms.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Refresh token",
          "content": "A refresh token is a long-lived (in most cases with an indefinite lifetime) token that is meant to be stored and exchanged for a new refresh and access tokens only once. Once a refresh token is exchanged it becomes invalid, and can't be exchanged again. In practice, though, a refresh token can be exchanged multiple times but in a short time window.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Refresh token flow",
          "content": "The refresh token flow is a mechanism that issues a new refresh and access token on the basis of a valid refresh token. It is used to extend authorization access for an application. An application that is being constantly used will invoke the refresh token flow just before the access token expires.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Replay attack",
          "content": "A replay attack is when sensitive information is stolen or intercepted by attackers who then attempt to use it again (thus replay) in an effort to compromise a system. Commonly replay attacks can be mitigated with the proper use of nonces.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Row level security policies (RLS)",
          "content": "Row level security policies are special objects within the Postgres database that limit the available operations or data returned to clients. RLS policies use information contained in a JWT to identify users and the actions and data they are allowed to perform or view.",
          "level": 2
        },
        {
          "type": "section",
          "title": "SAML",
          "content": "SAML stands for Security Assertion Markup Language and is a protocol that enables single-sign on for enterprises. SAML was invented in the early 2000s and is based on XML technology. It is the de facto standard for enabling single-sign on for enterprises, although the more recent OIDC (OpenID Connect) protocol is gaining popularity.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Session",
          "content": "A session or authentication session is the concept that binds a verified user identity to a web browser. A session usually is long-lived, and can be terminated by the user logging out. An access and refresh token pair represent a session in the browser, and they are stored in local storage or as cookies.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Single-sign on (SSO)",
          "content": "Single-sign on allows enterprises to centrally manage accounts and access to applications. They use identity provider software or services to organize employee information in directories and connect those accounts with applications via OIDC or SAML protocols.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Time-based one-time password (TOTP)",
          "content": "A time-based one-time password is a one-time password generated at regular time intervals from a secret, usually from an application in a mobile device (e.g., Google Authenticator, 1Password).",
          "level": 2
        }
      ],
      "wordCount": 1281,
      "characterCount": 8096
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:security-hipaa-compliance",
      "identifier": "security-hipaa-compliance",
      "name": "HIPAA Compliance and Supabase",
      "description": "Supabase provides a HIPAA compliant environment and helps you meet your compliance controls.",
      "category": "security",
      "url": "https://supabase.com/docs/guides/security/hipaa-compliance",
      "dateModified": "2025-06-13T12:45:11.306409",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/security/hipaa-compliance.mdx",
      "frontmatter": {
        "id": "hipaa-compliance",
        "title": "HIPAA Compliance and Supabase",
        "description": "Supabase provides a HIPAA compliant environment and helps you meet your compliance controls."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The [Health Insurance Portability and Accountability Act (HIPAA)](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html) is a comprehensive law that protects individuals' health information while ensuring the continuity of health insurance coverage. It sets standards for privacy and security that must be followed by all entities that handle Protected Health Information (PHI), also known as electronic PHI (ePHI). HIPAA is specific to the United States, however many countries have similar or laws already in place or under legislation.\n\nUnder HIPAA, both covered entities and business associates have distinct responsibilities to ensure the protection of PHI. Supabase acts as a business associate for customers (the covered entity) who wish to provide healthcare related services. As a business associate, Supabase has a number of obligations and has undergone auditing of the security and privacy controls that are in place to meet these. Supabase has signed a Business Associate Agreement (BAA) with all of our vendors who would have access to ePHI, such as AWS, and ensure that we follow their terms listed in the agreements. Similarly when a customer signs a BAA with us, they have some responsibilities they agree to when using Supabase to store PHI."
        },
        {
          "type": "section",
          "title": "Customer responsibilities",
          "content": "Covered entities (the customer) are organizations that directly handle PHI, such as health plans, healthcare clearinghouses, and healthcare providers that conduct certain electronic transactions.\n\n1. **Compliance with HIPAA Rules**: Covered entities must comply with the [HIPAA Privacy Rule](https://www.hhs.gov/hipaa/for-professionals/privacy/index.html), [Security Rule](https://www.hhs.gov/hipaa/for-professionals/security/index.html), and [Breach Notification Rule](https://www.hhs.gov/hipaa/for-professionals/breach-notification/index.html) to protect the privacy and security of ePHI.\n2. **Business Associate Agreements (BAAs)**: Customers must sign a BAA with Supabase. When the covered entity engages a business associate to help carry out its healthcare activities, it must have a written BAA. This agreement outlines the business associate's responsibilities and requires them to comply with HIPAA Rules.\n3. **Internal Compliance Programs**: Customers must [configure their HIPAA projects](/docs/guides/platform/hipaa-projects) and follow the guidance given by the security advisor. Covered entities are responsible for implementing internal processes and compliance programs to ensure they meet HIPAA requirements.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Supabase responsibilities",
          "content": "Supabase as the business associate, and the vendors used by Supabase, are the entities that perform functions or activities on behalf of the customer.\n\n1. **Direct Liability**: Supabase is directly liable for compliance with certain provisions of the HIPAA Rules. This means Supabase has to implement safeguards to protect ePHI and report breaches to the customer.\n2. **Compliance with BAAs**: Supabase must comply with the terms of the BAA, which includes implementing appropriate administrative, physical, and technical safeguards to protect ePHI.\n3. **Vendor Management**: Supabase must also ensure that our vendors, who may have access to ePHI, comply with HIPAA Rules. This is done through a BAA with each vendor.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Staying compliant and secure",
          "content": "Compliance is a continuous process and should not be treated as a point-in-time audit of controls. Supabase applies all the necessary privacy and security controls to ensure HIPAA compliance at audit time, but also has additional checks and monitoring in place to ensure those controls are not disabled or altered in between audit periods. Customers commit to doing the same in their HIPAA environments. Supabase provides a growing set of checks that warn customers of changes to their projects that disable or weaken HIPAA required controls. Customers will receive warnings and guidance via the Security Advisor, however the responsibility of applying the recommended controls falls directly to the customer.\n\nOur [shared responsibility model](/docs/guides/deployment/shared-responsibility-model#managing-healthcare-data) document discusses both HIPAA and general data management best practices, how this responsibility is shared between customers and Supabase, and how to stay compliant.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Frequently asked questions",
          "content": "**What is the difference between SOC 2 and HIPAA?**\n\nBoth are frameworks for protecting sensitive data, however they serve two different purposes. They share many security and privacy controls and meeting the controls of one normally means being close to complying with the other.\n\nThe main differentiator comes down to purpose and scope.\n\n- SOC 2 is not industry-specific and can be applied to any service organization that handles customer data.\n- HIPAA is a federal regulation in the United States. HIPAA sets standards for the privacy and security of PHI/ePHI, ensuring that patient data is handled confidentially and securely.\n\n**Are Supabase HIPAA environments also SOC 2 compliant?**\n\nYes. Supabase applies the same SOC 2 controls to all environments, with additional controls being applied to HIPAA environments.\n\n**How often is Supabase audited?**\n\nSupabase undergoes annual audits. The HIPAA controls are audited during the same audit period as the SOC 2 controls.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "1. [Health Insurance Portability and Accountability Act (HIPAA)](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)\n2. [HIPAA Privacy Rule](https://www.hhs.gov/hipaa/for-professionals/privacy/index.html)\n3. [Security Rule](https://www.hhs.gov/hipaa/for-professionals/security/index.html)\n4. [Breach Notification Rule](https://www.hhs.gov/hipaa/for-professionals/breach-notification/index.html)\n5. [Configuring HIPAA projects](/docs/guides/platform/hipaa-projects) on Supabase\n6. [Shared Responsibility Model](/docs/guides/deployment/shared-responsibility-model)\n7. [HIPAA shared responsibility](/docs/guides/deployment/shared-responsibility-model#managing-healthcare-data)",
          "level": 2
        }
      ],
      "wordCount": 772,
      "characterCount": 6043
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:security-product-security",
      "identifier": "security-product-security",
      "name": "Secure configuration of Supabase products",
      "description": "Supabase provides a secure yet flexible platform. Here is how to adjust various security settings across the platform.",
      "category": "security",
      "url": "https://supabase.com/docs/guides/security/product-security",
      "dateModified": "2025-06-13T12:45:11.306576",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/security/product-security.mdx",
      "frontmatter": {
        "id": "product-security",
        "title": "Secure configuration of Supabase products",
        "description": "Supabase provides a secure yet flexible platform. Here is how to adjust various security settings across the platform."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The Supabase [production checklist](/docs/guides/deployment/going-into-prod) provides detailed advice on preparing an app for production. While our [SOC 2](/docs/guides/security/soc-2-compliance) and [HIPAA](/docs/guides/security/hipaa-compliance) compliance documents outline the roles and responsibilities for building a secure and compliant app.\n\nVarious products at Supabase have their own hardening and configuration guides, below is a definitive list of these to help guide your way."
        },
        {
          "type": "section",
          "title": "Auth",
          "content": "- [Password security](/docs/guides/auth/password-security)\n- [Rate limits](/docs/guides/auth/rate-limits)\n- [Bot detection / Prevention](/docs/guides/auth/auth-captcha)\n- [JWTs](/docs/guides/auth/jwts)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Database",
          "content": "- [Row Level Security](/docs/guides/database/postgres/row-level-security)\n- [Column Level Security](/docs/guides/database/postgres/column-level-security)\n- [Hardening the Data API](/docs/guides/database/hardening-data-api)\n- [Additional security controls for the Data API](/docs/guides/api/securing-your-api)\n- [Custom claims and role based access control](/docs/guides/database/postgres/custom-claims-and-role-based-access-control-rbac)\n- [Managing Postgres roles](/docs/guides/database/postgres/roles)\n- [Managing secrets with Vault](/docs/guides/database/vault)\n- [Superuser access and unsupported operations](docs/guides/database/postgres/roles-superuser)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Storage",
          "content": "- [Object ownership](/docs/guides/storage/security/ownership)\n- [Access control](/docs/guides/storage/security/access-control)\n - The Storage API docs contain hints about required [RLS policy permissions](/docs/reference/javascript/storage-createbucket)\n- [Custom roles with the storage schema](/docs/guides/storage/schema/custom-roles)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Realtime",
          "content": "- [Authorization](docs/guides/realtime/authorization)",
          "level": 2
        }
      ],
      "wordCount": 148,
      "characterCount": 1793
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:security-security-testing",
      "identifier": "security-security-testing",
      "name": "Security testing of your Supabase projects",
      "description": "Supabase provides a secure yet flexible platform. You may wish to validate the security of your own project implementation, we ask that you follow these guidelines.",
      "category": "security",
      "url": "https://supabase.com/docs/guides/security/security-testing",
      "dateModified": "2025-06-13T12:45:11.307068",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/security/security-testing.mdx",
      "frontmatter": {
        "id": "security-testing",
        "title": "Security testing of your Supabase projects",
        "description": "Supabase provides a secure yet flexible platform. You may wish to validate the security of your own project implementation, we ask that you follow these guidelines."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase customer support policy for penetration testing\n\nCustomers of Supabase are permitted to carry out security assessments or penetration tests of their hosted Supabase project components. This testing may be carried out without prior approval for the customer services listed under [permitted services](#permitted-services). Supabase does not permit hosting security tooling that may be perceived as malicious or part of a campaign against Supabase customers or external services. This section is covered by the [Supabase Acceptable Use Policy](https://supabase.com/aup) (AUP).\n\nIt is the customer’s responsibility to ensure that testing activities are aligned with this policy. Any testing performed outside of the policy will be seen as testing directly against Supabase and may be flagged as abuse behaviour. If Supabase receives an abuse report for activities related to your security testing, we will forward these to you. If you discover a security issue within any of the Supabase products, contact [Supabase Security](mailto:security@supabase.io) immediately.\n\nFurthermore, Supabase runs a [Vulnerability Disclosure Program](https://hackerone.com/ca63b563-9661-4ac3-8d23-7581582ef451/embedded_submissions/new) (VDP) with HackerOne, and external security researchers may report any bugs found within the scope of the aforementioned program. Customer penetration testing does not form part of this VDP."
        },
        {
          "type": "section",
          "title": "Permitted services",
          "content": "- Authentication\n- Database\n- Edge Functions\n- Storage\n- Realtime\n- `https://.supabase.co/*`\n- `https://db..supabase.co/*`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Prohibited testing and activities",
          "content": "- Any activity contrary to what is listed in the AUP.\n- Denial of Service (DoS) and Distributed Denial of Service (DDoS) testing.\n- Cross-tenant attacks, testing that directly targets other Supabase customers' accounts, organizations, and projects not under the customer’s control.\n- Request flooding.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Terms and conditions",
          "content": "The customer agrees to the following,\n\nSecurity testing:\n\n- Will be limited to the services within the customer’s project.\n- Is subject to the general [Terms of Service](https://supabase.com/terms).\n- Is within the [Acceptable Usage Policy](https://supabase.com/aup).\n- Will be stopped if contacted by Supabase due to a breach of the above or a negative impact on Supabase and Supabase customers.\n- Any vulnerabilities discovered directly in a Supabase product will be reported to Supabase Security within 24 hours of completion of testing.",
          "level": 2
        }
      ],
      "wordCount": 339,
      "characterCount": 2471
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:security-soc-2-compliance",
      "identifier": "security-soc-2-compliance",
      "name": "SOC 2 Compliance and Supabase",
      "description": "Supabase is SOC 2 compliant and helps you meet your compliance controls.",
      "category": "security",
      "url": "https://supabase.com/docs/guides/security/soc-2-compliance",
      "dateModified": "2025-06-13T12:45:11.307429",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/security/soc-2-compliance.mdx",
      "frontmatter": {
        "id": "soc-2-compliance",
        "title": "SOC 2 Compliance and Supabase",
        "description": "Supabase is SOC 2 compliant and helps you meet your compliance controls."
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase is Systems and Organization Controls 2 (SOC 2) Type 2 compliant and is assessed annually to ensure continued adherence to the SOC 2 security framework. SOC 2 assesses Supabase’s adherence to, and implementation of, controls governing the security, availability, processing integrity, confidentiality, and privacy on the Supabase platform. These controls define requirements for the management and storage of customer data on the platform. These controls applied to Supabase, as a service provider, serve two customer data environments.\n\nThe first environment is the customer relationship with Supabase, this refers to the data Supabase has on a customer of the platform. All billing, contact, usage and contract information is managed and stored according to SOC 2 requirements.\n\nThe second environment is the backend as a service (the product) that Supabase provides to customers. Supabase implements the controls from the SOC 2 framework to ensure the security of the platform, which hosts the backend as a service (the product), including the Postgres Database, Storage, Authentication, Realtime, Edge Functions and Data API features. Supabase can assert that the environment hosting customer data, stored within the product, adheres to SOC 2 requirements. And the management and storage of data within this environment (the product) is strictly controlled and kept secure.\n\nSupabase’s SOC 2 compliance does not transfer to environments outside of the Supabase product or Supabase’s control. This is known as the security or compliance boundary and forms part of the Shared Responsibility Model that Supabase and their customers enter into.\n\nSOC 2 does not cover, nor is it a substitute for, compliance with the Health Insurance Portability and Accountability Act (HIPAA).\nOrganizations must have a signed Business Associate Agreement (BAA) with Supabase and have the HIPAA add-on enabled when dealing with Protected Health Information (PHI).\n\nOur [HIPAA documentation](/docs/guides/security/hipaa-compliance) provides more information about the responsibilities and requirements for HIPAA on Supabase."
        },
        {
          "type": "section",
          "title": "Meeting compliance requirements",
          "content": "SOC 2 compliance is a critical aspect of data security for Supabase and our customers. Being fully SOC 2 compliant is a shared responsibility and here’s a breakdown of the responsibilities for both parties:",
          "level": 1
        },
        {
          "type": "section",
          "title": "Supabase responsibilities",
          "content": "1. **Security Measures**: Supabase implements robust security controls to protect customer data. These includes measures to prevent data breaches and ensure the confidentiality and integrity of the information managed and stored by the platform. Supabase is obliged to be vigilant about security risks and must demonstrate that our security measures meet industry standards through regular audits.\n2. **Compliance Audits**: Supabase undergoes SOC 2 audits yearly to verify that our data management practices comply with the Trust Services Criteria (TSC), which include security, availability, processing integrity, confidentiality, and privacy. These audits are conducted by an independent third party.\n3. **Incident Response**: Supabase has an incident response plan in place to handle data breaches efficiently. This plan outlines how the organization detects issues, responds to incidents, and manages system vulnerabilities.\n4. **Reporting**: Upon a successful audit, Supabase receive a SOC 2 report that details our compliance status. This report is available to customers as a SOC 2 Type 2 report, and allows customers and stakeholders to assure that Supabase has implemented adequate and the requisite safeguards to protect sensitive information.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Customer responsibilities",
          "content": "1. **Compliance Requirements**: Understand your own compliance requirements. While SOC 2 compliance is not a legal requirement, many enterprise customers require their providers to have a SOC 2 report. This is because it provides assurance that the provider has implemented robust controls to protect customer data.\n2. **Due Diligence**: Customers must perform due diligence when selecting Supabase as a provider. This includes reviewing the SOC 2 Type 2 report to ensure that Supabase meets the expected security standards. Customers should also understand the division of responsibilities between themselves and Supabase to avoid duplication of effort.\n3. **Monitoring and Review**: Customers should regularly monitor and review Supabase’s compliance status.\n4. **Control Compliance**: If a customer needs to be SOC 2 compliant, they should themselves implement the requisite controls and undergo a SOC 2 audit.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Shared responsibilities",
          "content": "1. **Data Security**: Both customers and Supabase share the responsibility of ensuring data security. While the Supabase, as the provider, implements the security controls, the customer must ensure that their use of the Supabase platform does not compromise these controls.\n2. **Control Compliance**: Supabase asserts through our SOC 2 that all requisite security controls are met. Customers wishing to also be SOC 2 compliant need to go through their own SOC 2 audit, verifying that security controls are met on the customer's side.\n\nIn summary, SOC 2 compliance involves a shared responsibility between Supabase and our customers to ensure the security and integrity of data. Supabase, as a provider, must implement and maintain robust security measures, customers must perform due diligence and monitor Supabase's compliance status, while also implement their own compliance controls to protect their sensitive information.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Frequently asked questions",
          "content": "**How often is Supabase SOC 2 audited?**\n\nSupabase has obtained SOC 2 Type 2 certification, which means Supabase's controls are fully audited annually. The auditor's reports on these examinations are issued as soon as they are ready after the audit. Supabase makes the SOC 2 Type 2 report available to [Enterprise and Team Plan](https://supabase.com/pricing) customers. The audit report covers a rolling 12-month window, known as the audit period, and runs from 1 March to 28 February of the next calendar year.\n\n**How to obtain Supabase's SOC 2 Type 2 report?**\n\nTo access the SOC 2 Type 2 report, you must be a Enterprise or Team Plan Supabase customer. The report is downloadable from the [Legal Documents](https://supabase.com/dashboard/org/_/documents) section in the organization dashboard.\n\n**Why does it matter that Supabase is SOC 2 Compliant?**\n\nSOC 2 is used to assert that controls are in place to ensure the proper management and storage of data. SOC 2 provides a framework for measuring how secure a service provider is and re-evaluates the provider on an annual basis. This provides the confidence and assurance that data stored within the Supabase platform is correctly secured and managed.\n\n**If Supabase’s SOC 2 does not transfer to the customer, why does it matter that Supabase has SOC 2?**\n\nEven though Supabase’s SOC 2 compliance does not transfer outside of the product, it does provide the assurance that all data within the product is correctly managed and stored. Supabase can assert that only authorized persons have access to the data, and security controls are in place to prevent, detect and respond to data intrusions. This forms part of a customer’s own adherence to the SOC 2 framework and relieves part of the burden of data management and storage on the customer. In many organizations' security and risk departments require all vendors or sub-processors to be SOC 2 compliant.\n\n**What is the security or compliance boundary?**\n\nThis defines the boundary or border between Supabase and customer responsibility for data security within the Shared Responsibility Model. Customer data stored within the Supabase product, on the Supabase side of the security boundary, is managed and secured by Supabase. Supabase ensures the safe handling and storage of data within this environment. This includes controls for preventing unauthorized access, monitoring data access, alerting, data backups and redundancy. Data on the customer side of the boundary, the data that enters and leaves the Supabase product, is the responsibility of the customer. Management and possible storage of such data outside of Supabase should be performed by the customer, and any security and compliance controls are the responsibility of the customer.\n\n**We have strong data residency requirements. Does Supabase SOC 2 cover data residency?**\n\nWhile SOC 2 itself does not mandate specific data residency requirements, organizations may still need to comply with other regulatory frameworks, such as GDPR, that do have such requirements. Ensuring projects are deployed in the correct region is a customer responsibility as each Supabase project is deployed into the region the customer specifies at creation time. All data will remain within the chosen region.\n[Read replicas](/docs/guides/platform/read-replicas) can be created for multi-region availability, it remains the customer's responsibility to ensure regions chosen for read replicas are within the geographic area required by any additional regulatory frameworks.\n\n**Does SOC 2 cover health related data (HIPAA)?**\n\nSOC 2 is non-industry specific and provides a framework for the security and privacy of data. This is however not sufficient in most cases when dealing with Protected Healthcare Information (PHI), which requires additional privacy and legal controls.\nWhen dealing with PHI in the United States or for United States customers, HIPAA is mandatory.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resources",
          "content": "1. [System and Organization Controls: SOC Suite of Services](https://www.aicpa-cima.com/resources/landing/system-and-organization-controls-soc-suite-of-services)\n2. [Shared Responsibility Model](/docs/guides/deployment/shared-responsibility-model)",
          "level": 2
        }
      ],
      "wordCount": 1425,
      "characterCount": 9768
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-buckets-creating-buckets",
      "identifier": "storage-buckets-creating-buckets",
      "name": "Creating Buckets",
      "description": "Learn how to create Supabase Storage buckets.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/buckets/creating-buckets",
      "dateModified": "2025-06-13T12:45:11.307570",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/buckets/creating-buckets.mdx",
      "frontmatter": {
        "id": "storage-creating-buckets",
        "title": "Creating Buckets",
        "description": "Learn how to create Supabase Storage buckets.",
        "sidebar_label": "Buckets"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You can create a bucket using the Supabase Dashboard. Since storage is interoperable with your Postgres database, you can also use SQL or our client libraries.\nHere we create a bucket called \"avatars\":\n\n```js\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_KEY!)\n\n// ---cut---\n// Use the JS library to create a bucket.\n\nconst { data, error } = await supabase.storage.createBucket('avatars', {\n public: true, // default: false\n})\n```\n\n[Reference.](/docs/reference/javascript/storage-createbucket)\n\n1. Go to the [Storage](https://supabase.com/dashboard/project/_/storage/buckets) page in the Dashboard.\n2. Click **New Bucket** and enter a name for the bucket.\n3. Click **Create Bucket**.\n\n```sql\n-- Use Postgres to create a bucket.\n\ninsert into storage.buckets\n (id, name, public)\nvalues\n ('avatars', 'avatars', true);\n```\n\n```dart\nvoid main() async {\n final supabase = SupabaseClient('supabaseUrl', 'supabaseKey');\n\n final storageResponse = await supabase\n .storage\n .createBucket('avatars');\n}\n```\n\n[Reference.](https://pub.dev/documentation/storage_client/latest/storage_client/SupabaseStorageClient/createBucket.html)\n\n```swift\ntry await supabase.storage.createBucket(\n \"avatars\",\n options: BucketOptions(public: true)\n)\n```\n\n[Reference.](/docs/reference/swift/storage-createbucket)\n\n```python\nsupabase.storage.create_bucket(\n 'avatars',\n options={\"public\": True}\n)\n```\n\n[Reference.](/docs/reference/python/storage-createbucket)"
        },
        {
          "type": "section",
          "title": "Restricting uploads",
          "content": "When creating a bucket you can add additional configurations to restrict the type or size of files you want this bucket to contain.\nFor example, imagine you want to allow your users to upload only images to the `avatars` bucket and the size must not be greater than 1MB.\n\nYou can achieve the following by providing: `allowedMimeTypes` and `maxFileSize`\n\n```js\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_KEY!)\n\n// ---cut---\n// Use the JS library to create a bucket.\n\nconst { data, error } = await supabase.storage.createBucket('avatars', {\n public: true,\n allowedMimeTypes: ['image/*'],\n fileSizeLimit: '1MB',\n})\n```\n\nIf an upload request doesn't meet the above restrictions it will be rejected.\n\nFor more information check [File Limits](/docs/guides/storage/uploads/file-limits) Section.",
          "level": 2
        }
      ],
      "wordCount": 277,
      "characterCount": 2414
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-buckets-fundamentals",
      "identifier": "storage-buckets-fundamentals",
      "name": "Storage Buckets",
      "description": "Learn how Supabase Storage Buckets works.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/buckets/fundamentals",
      "dateModified": "2025-06-13T12:45:11.307665",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/buckets/fundamentals.mdx",
      "frontmatter": {
        "id": "storage-bucket-public-and-private",
        "title": "Storage Buckets",
        "description": "Learn how Supabase Storage Buckets works.",
        "sidebar_label": "Buckets"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Buckets allow you to keep your files organized and determines the [Access Model](#access-model) for your assets. [Upload restrictions](/docs/guides/storage/buckets/creating-buckets#restricting-uploads) like max file size and allowed content types are also defined at the bucket level."
        },
        {
          "type": "section",
          "title": "Access model",
          "content": "There are 2 access models for buckets, **public** and **private** buckets.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Private buckets",
          "content": "When a bucket is set to **Private** all operations are subject to access control via [RLS policies](/docs/guides/storage/security/access-control). This also applies when downloading assets. Buckets are private by default.\n\nThe only ways to download assets within a private bucket is to:\n\n- Use the [download method](/docs/reference/javascript/storage-from-download) by providing a authorization header containing your user's JWT. The RLS policy you create on the `storage.objects` table will use this user to determine if they have access.\n- Create a signed URL with the [`createSignedUrl` method](/docs/reference/javascript/storage-from-createsignedurl) that can be accessed for a limited time.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Example use cases:",
          "content": "- Uploading users' sensitive documents\n- Securing private assets by using RLS to set up fine-grain access controls",
          "level": 4
        },
        {
          "type": "section",
          "title": "Public buckets",
          "content": "When a bucket is designated as 'Public,' it effectively bypasses access controls for both retrieving and serving files within the bucket. This means that anyone who possesses the asset URL can readily access the file.\n\nAccess control is still enforced for other types of operations including uploading, deleting, moving, and copying.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Example use cases:",
          "content": "- User profile pictures\n- User public media\n- Blog post content\n\nPublic buckets are more performant than private buckets since they are [cached differently](/docs/guides/storage/cdn/fundamentals#public-vs-private-buckets).",
          "level": 4
        }
      ],
      "wordCount": 245,
      "characterCount": 1840
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-cdn-fundamentals",
      "identifier": "storage-cdn-fundamentals",
      "name": "Storage CDN",
      "description": "Learn how Supabase Storage caches objects with a CDN.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/cdn/fundamentals",
      "dateModified": "2025-06-13T12:45:11.307779",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/cdn/fundamentals.mdx",
      "frontmatter": {
        "id": "storage-cdn",
        "title": "Storage CDN",
        "description": "Learn how Supabase Storage caches objects with a CDN.",
        "sidebar_label": "CDN"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "All assets uploaded to Supabase Storage are cached on a Content Delivery Network (CDN) to improve the latency for users all around the world. CDNs are a geographically distributed set of servers or **nodes** which cache content from an **origin server**. For Supabase Storage, the origin is the storage server running in the [same region as your project](https://supabase.com/dashboard/project/_/settings/general). Aside from performance, CDNs also help with security and availability by mitigating Distributed Denial of Service (DDoS) and other application attacks."
        },
        {
          "type": "section",
          "title": "Example",
          "content": "Let's walk through an example of how a CDN helps with performance.\n\nA new bucket is created for a Supabase project launched in Singapore. All requests to the Supabase Storage API are routed to the CDN first.\n\nA user from the United States requests an object and is routed to the U.S. CDN. At this point, that CDN node does not have the object in its cache and pings the origin server in Singapore.\n![CDN cache miss](/docs/img/cdn-cache-miss.png)\n\nAnother user, also in the United States, requests the same object and is served directly from the CDN cache in the United States instead of routing the request back to Singapore.\n![CDN cache hit](/docs/img/cdn-cache-hit.png)\n\nNote that CDNs might still evict your object from their cache if it has not been requested for a while from a specific region. For example, if no user from United States requests your object, it will be removed from the CDN cache even if we set a very long cache control duration.\n\nThe cache status of a particular request is sent in the `cf-cache-status` header. A cache status of `MISS` indicates that the CDN node did not have the object in its cache and had to ping the origin to get it. A cache status of `HIT` indicates that the object was sent directly from the CDN.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Public vs private buckets",
          "content": "Objects in public buckets do not require any authorization to access objects. This leads to a better cache hit rate compared to private buckets.\n\nFor private buckets, permissions for accessing each object is checked on a per user level. For example, if two different users access the same object in a private bucket from the same region, it results in a cache miss for both the users since they might have different security policies attached to them.\nOn the other hand, if two different users access the same object in a public bucket from the same region, it results in a cache hit for the second user.",
          "level": 3
        }
      ],
      "wordCount": 412,
      "characterCount": 2464
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-cdn-metrics",
      "identifier": "storage-cdn-metrics",
      "name": "Cache Metrics",
      "description": "Learn how Supabase Storage caches objects with a CDN.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/cdn/metrics",
      "dateModified": "2025-06-13T12:45:11.307882",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/cdn/metrics.mdx",
      "frontmatter": {
        "id": "storage-cdn",
        "title": "Cache Metrics",
        "description": "Learn how Supabase Storage caches objects with a CDN.",
        "sidebar_label": "CDN"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "Cache hits can be determined via the `metadata.response.headers.cf_cache_status` key in our [Logs Explorer](/docs/guides/platform/logs#logs-explorer). Any value that corresponds to either `HIT`, `STALE`, `REVALIDATED`, or `UPDATING` is categorized as a cache hit.\nThe following example query will show the top cache misses from the `edge_logs`:\n\n```sql\nselect\n r.path as path,\n r.search as search,\n count(id) as count\nfrom\n edge_logs as f\n cross join unnest(f.metadata) as m\n cross join unnest(m.request) as r\n cross join unnest(m.response) as res\n cross join unnest(res.headers) as h\nwhere\n starts_with(r.path, '/storage/v1/object')\n and r.method = 'GET'\n and h.cf_cache_status in ('MISS', 'NONE/UNKNOWN', 'EXPIRED', 'BYPASS', 'DYNAMIC')\ngroup by path, search\norder by count desc\nlimit 50;\n```\n\nTry out [this query](https://supabase.com/dashboard/project/_/logs/explorer?q=%0Aselect%0A++r.path+as+path%2C%0A++r.search+as+search%2C%0A++count%28id%29+as+count%0Afrom%0A++edge_logs+as+f%0A++cross+join+unnest%28f.metadata%29+as+m%0A++cross+join+unnest%28m.request%29+as+r%0A++cross+join+unnest%28m.response%29+as+res%0A++cross+join+unnest%28res.headers%29+as+h%0Awhere%0A++starts_with%28r.path%2C+%27%2Fstorage%2Fv1%2Fobject%27%29%0A++and+r.method+%3D+%27GET%27%0A++and+h.cf_cache_status+in+%28%27MISS%27%2C+%27NONE%2FUNKNOWN%27%2C+%27EXPIRED%27%2C+%27BYPASS%27%2C+%27DYNAMIC%27%29%0Agroup+by+path%2C+search%0Aorder+by+count+desc%0Alimit+50%3B) in the Logs Explorer.\n\nYour cache hit ratio over time can then be determined using the following query:\n\n```sql\nselect\n timestamp_trunc(timestamp, hour) as timestamp,\n countif(h.cf_cache_status in ('HIT', 'STALE', 'REVALIDATED', 'UPDATING')) / count(f.id) as ratio\nfrom\n edge_logs as f\n cross join unnest(f.metadata) as m\n cross join unnest(m.request) as r\n cross join unnest(m.response) as res\n cross join unnest(res.headers) as h\nwhere starts_with(r.path, '/storage/v1/object') and r.method = 'GET'\ngroup by timestamp\norder by timestamp desc;\n```\n\nTry out [this query](https://supabase.com/dashboard/project/_/logs/explorer?q=%0Aselect%0A++timestamp_trunc%28timestamp%2C+hour%29+as+timestamp%2C%0A++countif%28h.cf_cache_status+in+%28%27HIT%27%2C+%27STALE%27%2C+%27REVALIDATED%27%2C+%27UPDATING%27%29%29+%2F+count%28f.id%29+as+ratio%0Afrom%0A++edge_logs+as+f%0A++cross+join+unnest%28f.metadata%29+as+m%0A++cross+join+unnest%28m.request%29+as+r%0A++cross+join+unnest%28m.response%29+as+res%0A++cross+join+unnest%28res.headers%29+as+h%0Awhere+starts_with%28r.path%2C+%27%2Fstorage%2Fv1%2Fobject%27%29+and+r.method+%3D+%27GET%27%0Agroup+by+timestamp%0Aorder+by+timestamp+desc%3B) in the Logs Explorer."
        }
      ],
      "wordCount": 189,
      "characterCount": 2642
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-cdn-smart-cdn",
      "identifier": "storage-cdn-smart-cdn",
      "name": "Smart CDN",
      "description": "Learn how Supabase Storage caches objects with a CDN.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/cdn/smart-cdn",
      "dateModified": "2025-06-13T12:45:11.308109",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/cdn/smart-cdn.mdx",
      "frontmatter": {
        "id": "storage-cdn",
        "title": "Smart CDN",
        "description": "Learn how Supabase Storage caches objects with a CDN.",
        "sidebar_label": "CDN"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "With Smart CDN caching enabled, the asset metadata in your database is synchronized to the edge. This automatically revalidates the cache when the asset is changed or deleted.\n\nMoreover, the Smart CDN achieves a greater cache hit rate by shielding the origin server from asset requests that remain unchanged, even when different query strings are used in the URL.\n\nSmart CDN caching is automatically enabled for [Pro Plan and above](https://supabase.com/pricing)."
        },
        {
          "type": "section",
          "title": "Cache duration",
          "content": "When Smart CDN is enabled, the asset is cached on the CDN for as long as possible. You can still control how long assets are stored in the browser using the [`cacheControl`](/docs/reference/javascript/storage-from-upload) option when uploading a file. Smart CDN caching works with all types of storage operations including signed URLs.\n\nWhen a file is updated or deleted, the CDN cache is automatically invalidated to reflect the change (including transformed images). It can take **up to 60 seconds** for the CDN cache to be invalidated as the asset metadata has to propagate across all the data-centers around the globe.\n\nWhen an asset is invalidated at the CDN level, browsers may not update its cache. This is where cache eviction comes into play.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Cache eviction",
          "content": "Even when an asset is marked as invalidated at the CDN level, browsers may not refresh their cache for that asset.\n\nIf you have assets that undergo frequent updates, it is advisable to upload the new asset to a different path. This approach ensures that you always have the most up-to-date asset accessible.\n\nIf you anticipate that your asset might be deleted, it's advisable to set a shorter browser Time-to-Live (TTL) value using the `cacheControl` option. The default TTL is typically set to 1 hour, which is generally a reasonable default value.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Bypassing cache",
          "content": "If you need to ensure assets refresh directly from the origin server and bypass the cache, you can achieve this by adding a unique query string to the URL.\n\nFor instance, you can use a URL like `/storage/v1/object/sign/profile-pictures/cat.jpg?version=1` with a long browser cache (e.g., 1 year). To update the picture, increment the version query parameter in the URL, like `/storage/v1/object/sign/profile-pictures/cat.jpg?version=2`. The CDN will recognize it as a new object and fetch the updated version from the origin.",
          "level": 2
        }
      ],
      "wordCount": 369,
      "characterCount": 2352
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-debugging-error-codes",
      "identifier": "storage-debugging-error-codes",
      "name": "Error Codes",
      "description": "Supabase Error Codes",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/debugging/error-codes",
      "dateModified": "2025-06-13T12:45:11.308502",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/debugging/error-codes.mdx",
      "frontmatter": {
        "id": "storage-errors-codes",
        "title": "Error Codes",
        "description": "Supabase Error Codes",
        "subtitle": "Learn about the Storage error codes and how to resolve them",
        "sidebar_label": "Debugging"
      },
      "sections": [
        {
          "type": "section",
          "title": "Storage error codes",
          "content": "We are transitioning to a new error code system. For backwards compatibility you'll still be able\n to see the old error codes\n\nError codes in Storage are returned as part of the response body. They are useful for debugging and understanding what went wrong with your request.\nThe error codes are returned in the following format:\n\n```json\n{\n \"code\": \"error_code\",\n \"message\": \"error_message\"\n}\n```\n\nHere is the full list of error codes and their descriptions:\n\n| `ErrorCode` | Description | `StatusCode` | Resolution |\n| --------------------------- | --------------------------------------------------------------- | ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `NoSuchBucket` | The specified bucket does not exist. | 404 | Verify the bucket name and ensure it exists in the system, if it exists you don't have permissions to access it. |\n| `NoSuchKey` | The specified key does not exist. | 404 | Check the key name and ensure it exists in the specified bucket, if it exists you don't have permissions to access it. |\n| `NoSuchUpload` | The specified upload does not exist. | 404 | The upload ID provided might not exists or the Upload was previously aborted |\n| `InvalidJWT` | The provided JWT (JSON Web Token) is invalid. | 401 | The JWT provided might be expired or malformed, provide a valid JWT |\n| `InvalidRequest` | The request is not properly formed. | 400 | Review the request parameters and structure, ensure they meet the API's requirements, the error message will provide more details |\n| `TenantNotFound` | The specified tenant does not exist. | 404 | The Storage service had issues while provisioning, [Contact Support](https://supabase.com/dashboard/support/new) |\n| `EntityTooLarge` | The entity being uploaded is too large. | 413 | Verify the max-file-limit is equal or higher to the resource you are trying to upload, you can change this value on the [Project Setting](https://supabase.com/dashboard/project/_/settings/storage) |\n| `InternalError` | An internal server error occurred. | 500 | Investigate server logs to identify the cause of the internal error. If you think it's a Storage error [Contact Support](https://supabase.com/dashboard/support/new) |\n| `ResourceAlreadyExists` | The specified resource already exists. | 409 | Use a different name or identifier for the resource to avoid conflicts. Use `x-upsert:true` header to overwrite the resource. |\n| `InvalidBucketName` | The specified bucket name is invalid. | 400 | Ensure the bucket name follows the naming conventions and does not contain invalid characters. |\n| `InvalidKey` | The specified key is invalid. | 400 | Verify the key name and ensure it follows the naming conventions. |\n| `InvalidRange` | The specified range is not valid. | 416 | Make sure that range provided is within the file size boundary and follow the [HTTP Range spec](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Range) |\n| `InvalidMimeType` | The specified MIME type is not valid. | 400 | Provide a valid MIME type, ensure using the standard MIME type format |\n| `InvalidUploadId` | The specified upload ID is invalid. | 400 | The upload ID provided is invalid or missing. Make sure to provide a active upload ID |\n| `KeyAlreadyExists` | The specified key already exists. | 409 | Use a different key name to avoid conflicts with existing keys. Use `x-upsert:true` header to overwrite the resource. |\n| `BucketAlreadyExists` | The specified bucket already exists. | 409 | Choose a unique name for the bucket that does not conflict with existing buckets. |\n| `DatabaseTimeout` | Timeout occurred while accessing the database. | 504 | Investigate database performance and increase the default pool size. If this error still occurs, upgrade your instance |\n| `InvalidSignature` | The signature provided does not match the calculated signature. | 403 | Check that you are providing the correct signature format, for more information refer to [SignatureV4](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) |\n| `SignatureDoesNotMatch` | The request signature does not match the calculated signature. | 403 | Check your credentials, access key id / access secret key / region that are all correct, refer to [S3 Authentication](/docs/guides/storage/s3/authentication). |\n| `AccessDenied` | Access to the specified resource is denied. | 403 | Check that you have the correct RLS policy to allow access to this resource |\n| `ResourceLocked` | The specified resource is locked. | 423 | This resource cannot be altered while there is a lock. Wait and try the request again |\n| `DatabaseError` | An error occurred while accessing the database. | 500 | Investigate database logs and system configuration to identify and address the database error. |\n| `MissingContentLength` | The Content-Length header is missing. | 411 | Ensure the Content-Length header is included in the request with the correct value. |\n| `MissingParameter` | A required parameter is missing in the request. | 400 | Provide all required parameters in the request to fulfill the API's requirements. The message field will contain more details |\n| `InvalidUploadSignature` | The provided upload signature is invalid. | 403 | The `MultiPartUpload` record was altered while the upload was ongoing, the signature do not match. Do not alter the upload record |\n| `LockTimeout` | Timeout occurred while waiting for a lock. | 423 | The lock couldn't be acquired within the specified timeout. Wait and try the request again |\n| `S3Error` | An error occurred related to Amazon S3. | - | Refer to Amazon S3 documentation or [Contact Support](https://supabase.com/dashboard/support/new) for assistance with resolving the S3 error. |\n| `S3InvalidAccessKeyId` | The provided AWS access key ID is invalid. | 403 | Verify the AWS access key ID provided and ensure it is correct and active. |\n| `S3MaximumCredentialsLimit` | The maximum number of credentials has been reached. | 400 | The maximum limit of credentials is reached. |\n| `InvalidChecksum` | The checksum of the entity does not match. | 400 | Recalculate the checksum of the entity and ensure it matches the one provided in the request. |\n| `MissingPart` | A part of the entity is missing. | 400 | Ensure all parts of the entity are included in the request before completing the operation. |\n| `SlowDown` | The request rate is too high and has been throttled. | 503 | Reduce the request rate or implement exponential backoff and retry mechanisms to handle throttling. |",
          "level": 2
        },
        {
          "type": "section",
          "title": "Legacy error codes",
          "content": "As we are transitioning to a new error code system, you might still see the following error format:\n\n```json\n{\n \"httpStatusCode\": 400,\n \"code\": \"error_code\",\n \"message\": \"error_message\"\n}\n```\n\nHere's a list of the most common error codes and their potential resolutions:",
          "level": 2
        },
        {
          "type": "section",
          "title": "404 `not_found`",
          "content": "Indicates that the resource is not found or you don't have the correct permission to access it\n**Resolution:**\n\n- Add a RLS policy to grant permission to the resource. See our [Access Control docs](/docs/guides/storage/uploads/access-control) for more information.\n- Ensure you include the user `Authorization` header\n- Verify the object exists",
          "level": 3
        },
        {
          "type": "section",
          "title": "409 `already_exists`",
          "content": "Indicates that the resource already exists.\n**Resolution:**\n\n- Use the `upsert` functionality in order to overwrite the file. Find out more [here](/docs/guides/storage/uploads/standard-uploads#overwriting-files).",
          "level": 3
        },
        {
          "type": "section",
          "title": "403 `unauthorized`",
          "content": "You don't have permission to action this request\n**Resolution:**\n\n- Add RLS policy to grant permission. See our [Access Control docs](/docs/guides/storage/security/access-control) for more information.\n- Ensure you include the user `Authorization` header",
          "level": 3
        },
        {
          "type": "section",
          "title": "429 `too many requests`",
          "content": "This problem typically arises when a large number of clients are concurrently interacting with the Storage service, and the pooler has reached its `max_clients` limit.\n\n**Resolution:**\n\n- Increase the max_clients limits of the pooler.\n- Upgrade to a bigger project compute instance [here](https://supabase.com/dashboard/project/_/settings/addons).",
          "level": 3
        },
        {
          "type": "section",
          "title": "544 `database_timeout`",
          "content": "This problem arises when a high number of clients are concurrently using the Storage service, and Postgres doesn't have enough available connections to efficiently handle requests to Storage.\n\n**Resolution:**\n\n- Increase the pool_size limits of the pooler.\n- Upgrade to a bigger project compute instance [here](https://supabase.com/dashboard/project/_/settings/addons).",
          "level": 3
        },
        {
          "type": "section",
          "title": "500 `internal_server_error`",
          "content": "This issue occurs where there is a unhandled error.\n**Resolution:**\n\n- File a support ticket to Storage team [here](https://supabase.com/dashboard/support/new)",
          "level": 3
        }
      ],
      "wordCount": 1315,
      "characterCount": 8830
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-debugging-logs",
      "identifier": "storage-debugging-logs",
      "name": "Logs",
      "description": "Learn how to check Storage Logs",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/debugging/logs",
      "dateModified": "2025-06-13T12:45:11.308609",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/debugging/logs.mdx",
      "frontmatter": {
        "id": "storage-logs",
        "title": "Logs",
        "description": "Learn how to check Storage Logs",
        "sidebar_label": "Debugging"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Accessing the [Storage Logs](/dashboard/project/__/logs/explorer?q=select+id%2C+storage_logs.timestamp%2C+event_message+from+storage_logs%0A++%0A++order+by+timestamp+desc%0A++limit+100%0A++) allows you to examine all incoming request logs to your Storage service. You can also filter logs and delve into specific aspects of your requests."
        },
        {
          "type": "section",
          "title": "Filter by status 5XX error",
          "content": "```sql\nselect\n id,\n storage_logs.timestamp,\n event_message,\n r.statusCode,\n e.message as errorMessage,\n e.raw as rawError\nfrom\n storage_logs\n cross join unnest(metadata) as m\n cross join unnest(m.res) as r\n cross join unnest(m.error) as e\nwhere r.statusCode >= 500\norder by timestamp desc\nlimit 100;\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Filter by status 4XX error",
          "content": "```sql\nselect\n id,\n storage_logs.timestamp,\n event_message,\n r.statusCode,\n e.message as errorMessage,\n e.raw as rawError\nfrom\n storage_logs\n cross join unnest(metadata) as m\n cross join unnest(m.res) as r\n cross join unnest(m.error) as e\nwhere r.statusCode >= 400 and r.statusCode < 500\norder by timestamp desc\nlimit 100;\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Filter by method",
          "content": "```sql\nselect id, storage_logs.timestamp, event_message, r.method\nfrom\n storage_logs\n cross join unnest(metadata) as m\n cross join unnest(m.req) as r\nwhere r.method in (\"POST\")\norder by timestamp desc\nlimit 100;\n```",
          "level": 4
        },
        {
          "type": "section",
          "title": "Filter by IP address",
          "content": "```sql\nselect id, storage_logs.timestamp, event_message, r.remoteAddress\nfrom\n storage_logs\n cross join unnest(metadata) as m\n cross join unnest(m.req) as r\nwhere r.remoteAddress in (\"IP_ADDRESS\")\norder by timestamp desc\nlimit 100;\n```",
          "level": 4
        }
      ],
      "wordCount": 196,
      "characterCount": 1565
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-management-copy-move-objects",
      "identifier": "storage-management-copy-move-objects",
      "name": "Copy Objects",
      "description": "Learn how to copy and move objects",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/management/copy-move-objects",
      "dateModified": "2025-06-13T12:45:11.308720",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/management/copy-move-objects.mdx",
      "frontmatter": {
        "id": "storage-management",
        "title": "Copy Objects",
        "description": "Learn how to copy and move objects",
        "subtitle": "Learn how to copy and move objects",
        "sidebar_label": "Copy / Move Objects"
      },
      "sections": [
        {
          "type": "section",
          "title": "Copy objects",
          "content": "You can copy objects between buckets or within the same bucket. Currently only objects up to 5 GB can be copied using the API.\n\nWhen making a copy of an object, the owner of the new object will be the user who initiated the copy operation.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Copying objects within the same bucket",
          "content": "To copy an object within the same bucket, use the `copy` method.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nawait supabase.storage.from('avatars').copy('public/avatar1.png', 'private/avatar2.png')\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Copying objects across buckets",
          "content": "To copy an object across buckets, use the `copy` method and specify the destination bucket.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nawait supabase.storage.from('avatars').copy('public/avatar1.png', 'private/avatar2.png', {\n destinationBucket: 'avatars2',\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Move objects",
          "content": "You can move objects between buckets or within the same bucket. Currently only objects up to 5GB can be moved using the API.\n\nWhen moving an object, the owner of the new object will be the user who initiated the move operation. Once the object is moved, the original object will no longer exist.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Moving objects within the same bucket",
          "content": "To move an object within the same bucket, you can use the `move` method.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nconst { data, error } = await supabase.storage\n .from('avatars')\n .move('public/avatar1.png', 'private/avatar2.png')\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Moving objects across buckets",
          "content": "To move an object across buckets, use the `move` method and specify the destination bucket.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nawait supabase.storage.from('avatars').move('public/avatar1.png', 'private/avatar2.png', {\n destinationBucket: 'avatars2',\n})\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Permissions",
          "content": "For a user to move and copy objects, they need `select` permission on the source object and `insert` permission on the destination object. For example:\n\n```sql\ncreate policy \"User can select their own objects (in any buckets)\"\non storage.objects\nfor select\nto authenticated\nusing (\n owner_id = (select auth.uid())\n);\n\ncreate policy \"User can upload in their own folders (in any buckets)\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n (storage.folder(name))[1] = (select auth.uid())\n);\n```",
          "level": 2
        }
      ],
      "wordCount": 353,
      "characterCount": 2677
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-management-delete-objects",
      "identifier": "storage-management-delete-objects",
      "name": "Delete Objects",
      "description": "Learn about deleting objects",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/management/delete-objects",
      "dateModified": "2025-06-13T12:45:11.308798",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/management/delete-objects.mdx",
      "frontmatter": {
        "id": "storage-management",
        "title": "Delete Objects",
        "description": "Learn about deleting objects",
        "subtitle": "Learn about deleting objects",
        "sidebar_label": "Delete Objects"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "When you delete one or more objects from a bucket, the files are permanently removed and not recoverable. You can delete a single object or multiple objects at once.\n\nDeleting objects should always be done via the **Storage API** and NOT via a **SQL query**. Deleting objects via a SQL query will not remove the object from the bucket and will result in the object being orphaned."
        },
        {
          "type": "section",
          "title": "Delete objects",
          "content": "To delete one or more objects, use the `remove` method.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nawait supabase.storage.from('bucket').remove(['object-path-2', 'folder/avatar2.png'])\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "RLS",
          "content": "To delete an object, the user must have the `delete` permission on the object. For example:\n\n```sql\ncreate policy \"User can delete their own objects\"\non storage.objects\nfor delete\nTO authenticated\nUSING (\n owner = (select auth.uid()::text)\n);\n```",
          "level": 2
        }
      ],
      "wordCount": 139,
      "characterCount": 959
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-management-pricing",
      "identifier": "storage-management-pricing",
      "name": "Pricing",
      "description": "",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/management/pricing",
      "dateModified": "2025-06-13T12:45:11.308856",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/management/pricing.mdx",
      "frontmatter": {
        "id": "storage-management-pricing",
        "title": "Pricing"
      },
      "sections": [
        {
          "type": "content",
          "title": "Content",
          "content": "You are charged for the total size of all assets in your buckets.\n\nFor a detailed explanation of how charges are calculated, refer to [Manage Storage size usage](/docs/guides/platform/manage-your-usage/storage-size).\n\nIf you use [Storage Image Transformations](/docs/guides/storage/serving/image-transformations), additional charges apply."
        }
      ],
      "wordCount": 37,
      "characterCount": 339
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-production-scaling",
      "identifier": "storage-production-scaling",
      "name": "Storage Optimizations",
      "description": "Scaling Storage",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/production/scaling",
      "dateModified": "2025-06-13T12:45:11.308969",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/production/scaling.mdx",
      "frontmatter": {
        "id": "storage-schema-optimizations",
        "title": "Storage Optimizations",
        "description": "Scaling Storage",
        "subtitle": "Scaling Storage",
        "sidebar_label": "Schema"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Here are some optimizations that you can consider to improve performance and reduce costs as you start scaling Storage."
        },
        {
          "type": "section",
          "title": "Egress",
          "content": "If your project has high egress, these optimizations can help reducing it.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Resize images",
          "content": "Images typically make up most of your egress. By keeping them as small as possible, you can cut down on egress and boost your application's performance. You can take advantage of our [Image Transformation](/docs/guides/storage/serving/image-transformations) service to optimize any image on the fly.",
          "level": 4
        },
        {
          "type": "section",
          "title": "Set a high cache-control value",
          "content": "Using the browser cache can effectively lower your egress since the asset remains stored in the user's browser after the initial download. Setting a high `cache-control` value ensures the asset stays in the user's browser for an extended period, decreasing the need to download it from the server repeatedly. Read more [here](/docs/guides/storage/cdn/smart-cdn#cache-duration)",
          "level": 4
        },
        {
          "type": "section",
          "title": "Limit the upload size",
          "content": "You have the option to set a maximum upload size for your bucket. Doing this can prevent users from uploading and then downloading excessively large files. You can control the maximum file size by configuring this option at the [bucket level](/docs/guides/storage/buckets/creating-buckets).",
          "level": 4
        },
        {
          "type": "section",
          "title": "Optimize listing objects",
          "content": "Once you have a substantial number of objects, you might observe that the `supabase.storage.list()` method starts to slow down. This occurs because the endpoint is quite generic and attempts to retrieve both folders and objects in a single query. While this approach is very useful for building features like the Storage viewer on the Supabase dashboard, it can impact performance with a large number of objects.\n\nIf your application doesn't need the entire hierarchy computed you can speed up drastically the query execution for listing your objects by creating a Postgres function as following:\n\n```sql\ncreate or replace function list_objects(\n bucketid text,\n prefix text,\n limits int default 100,\n offsets int default 0\n) returns table (\n name text,\n id uuid,\n updated_at timestamptz,\n created_at timestamptz,\n last_accessed_at timestamptz,\n metadata jsonb\n) as $$\nbegin\n return query SELECT\n objects.name,\n objects.id,\n objects.updated_at,\n objects.created_at,\n objects.last_accessed_at,\n objects.metadata\n FROM storage.objects\n WHERE objects.name like prefix || '%'\n AND bucket_id = bucketid\n ORDER BY name ASC\n LIMIT limits\n OFFSET offsets;\nend;\n$$ language plpgsql stable;\n```\n\nYou can then use the your Postgres function as following:\n\nUsing SQL:\n\n```sql\nselect * from list_objects('bucket_id', '', 100, 0);\n```\n\nUsing the SDK:\n\n```js\nconst { data, error } = await supabase.rpc('list_objects', {\n bucketid: 'yourbucket',\n prefix: '',\n limit: 100,\n offset: 0,\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Optimizing RLS",
          "content": "When creating RLS policies against the storage tables you can add indexes to the interested columns to speed up the lookup",
          "level": 2
        }
      ],
      "wordCount": 421,
      "characterCount": 2910
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-quickstart",
      "identifier": "storage-quickstart",
      "name": "Storage Quickstart",
      "description": "Learn how to use Supabase to store and serve files.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/quickstart",
      "dateModified": "2025-06-13T12:45:11.309153",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/quickstart.mdx",
      "frontmatter": {
        "id": "storage-quickstart",
        "title": "Storage Quickstart",
        "description": "Learn how to use Supabase to store and serve files.",
        "subtitle": "Learn how to use Supabase to store and serve files.",
        "sidebar_label": "Quickstart",
        "tocVideo": "J9mTPY8rIXE"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "This guide shows the basic functionality of Supabase Storage. Find a full [example application on GitHub](https://github.com/supabase/supabase/tree/master/examples/user-management/nextjs-user-management)."
        },
        {
          "type": "section",
          "title": "Concepts",
          "content": "Supabase Storage consists of Files, Folders, and Buckets.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Files",
          "content": "Files can be any sort of media file. This includes images, GIFs, and videos. It is best practice to store files outside of your database because of their sizes. For security, HTML files are returned as plain text.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Folders",
          "content": "Folders are a way to organize your files (just like on your computer). There is no right or wrong way to organize your files. You can store them in whichever folder structure suits your project.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Buckets",
          "content": "Buckets are distinct containers for files and folders. You can think of them like \"super folders\". Generally you would create distinct buckets for different Security and Access Rules. For example, you might keep all video files in a \"video\" bucket, and profile pictures in an \"avatar\" bucket.\n\nFile, Folder, and Bucket names **must follow** [AWS object key naming guidelines](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html) and avoid use of any other characters.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Create a bucket",
          "content": "You can create a bucket using the Supabase Dashboard. Since the storage is interoperable with your Postgres database, you can also use SQL or our client libraries. Here we create a bucket called \"avatars\":\n\n1. Go to the [Storage](https://supabase.com/dashboard/project/_/storage/buckets) page in the Dashboard.\n2. Click **New Bucket** and enter a name for the bucket.\n3. Click **Create Bucket**.\n\n```sql\n-- Use Postgres to create a bucket.\n\ninsert into storage.buckets\n (id, name)\nvalues\n ('avatars', 'avatars');\n```\n\n```js\n// Use the JS library to create a bucket.\n\nconst { data, error } = await supabase.storage.createBucket('avatars')\n```\n\n[Reference.](/docs/reference/javascript/storage-createbucket)\n\n```dart\nvoid main() async {\n final supabase = SupabaseClient('supabaseUrl', 'supabaseKey');\n\n final storageResponse = await supabase\n .storage\n .createBucket('avatars');\n}\n```\n\n[Reference.](https://pub.dev/documentation/storage_client/latest/storage_client/SupabaseStorageClient/createBucket.html)\n\n```swift\ntry await supabase.storage.createBucket(\"avatars\")\n```\n\n[Reference.](/docs/reference/swift/storage-createbucket)\n\n```python\nresponse = supabase.storage.create_bucket('avatars')\n```\n\n[Reference.](/docs/reference/python/storage-createbucket)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Upload a file",
          "content": "You can upload a file from the Dashboard, or within a browser using our JS libraries.\n\n1. Go to the [Storage](https://supabase.com/dashboard/project/_/storage/buckets) page in the Dashboard.\n2. Select the bucket you want to upload the file to.\n3. Click **Upload File**.\n4. Select the file you want to upload.\n\n```js\nconst avatarFile = event.target.files[0]\nconst { data, error } = await supabase.storage\n .from('avatars')\n .upload('public/avatar1.png', avatarFile)\n```\n\n[Reference.](/docs/reference/javascript/storage-from-upload)\n\n```dart\nvoid main() async {\n final supabase = SupabaseClient('supabaseUrl', 'supabaseKey');\n\n // Create file `example.txt` and upload it in `public` bucket\n final file = File('example.txt');\n file.writeAsStringSync('File content');\n final storageResponse = await supabase\n .storage\n .from('public')\n .upload('example.txt', file);\n}\n```\n\n[Reference.](https://pub.dev/documentation/storage_client/latest/storage_client/SupabaseStorageClient/createBucket.html)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Download a file",
          "content": "You can download a file from the Dashboard, or within a browser using our JS libraries.\n\n1. Go to the [Storage](https://supabase.com/dashboard/project/_/storage/buckets) page in the Dashboard.\n2. Select the bucket that contains the file.\n3. Select the file that you want to download.\n4. Click **Download**.\n\n```js\n// Use the JS library to download a file.\n\nconst { data, error } = await supabase.storage.from('avatars').download('public/avatar1.png')\n```\n\n[Reference.](/docs/reference/javascript/storage-from-download)\n\n```dart\nvoid main() async {\n final supabase = SupabaseClient('supabaseUrl', 'supabaseKey');\n\n final storageResponse = await supabase\n .storage\n .from('public')\n .download('example.txt');\n}\n```\n\n[Reference.](/docs/reference/dart/storage-from-download)\n\n```swift\nlet response = try await supabase.storage.from(\"avatars\").download(path: \"public/avatar1.png\")\n```\n\n[Reference.](/docs/reference/python/storage-from-download)\n\n```python\nresponse = supabase.storage.from_('avatars').download('public/avatar1.png')\n```\n\n[Reference.](/docs/reference/python/storage-from-download)",
          "level": 2
        },
        {
          "type": "section",
          "title": "Add security rules",
          "content": "To restrict access to your files you can use either the Dashboard or SQL.\n\n1. Go to the [Storage](https://supabase.com/dashboard/project/_/storage/buckets) page in the Dashboard.\n2. Click **Policies** in the sidebar.\n3. Click **Add Policies** in the `OBJECTS` table to add policies for Files. You can also create policies for Buckets.\n4. Choose whether you want the policy to apply to downloads (SELECT), uploads (INSERT), updates (UPDATE), or deletes (DELETE).\n5. Give your policy a unique name.\n6. Write the policy using SQL.\n\n```sql\n-- Use SQL to create a policy.\n\ncreate policy \"Public Access\"\n on storage.objects for select\n using ( bucket_id = 'public' );\n```\n\n---\n\n{/* Finish with a video. This also appears in the Sidebar via the \"tocVideo\" metadata */}",
          "level": 2
        }
      ],
      "wordCount": 641,
      "characterCount": 5394
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-s3-authentication",
      "identifier": "storage-s3-authentication",
      "name": "S3 Authentication",
      "description": "Authentication",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/s3/authentication",
      "dateModified": "2025-06-13T12:45:11.309272",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/s3/authentication.mdx",
      "frontmatter": {
        "id": "storage-s3-authentication",
        "title": "S3 Authentication",
        "description": "Authentication",
        "subtitle": "Learn about authenticating with Supabase Storage S3.",
        "sidebar_label": "S3"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You have two options to authenticate with Supabase Storage S3:\n\n- Using the generated S3 access keys from your [project settings](/dashboard/project/_/settings/storage) (Intended exclusively for server-side use)\n- Using a Session Token, which will allow you to authenticate with a user JWT token and provide limited access via Row Level Security (RLS)."
        },
        {
          "type": "section",
          "title": "S3 access keys",
          "content": "S3 access keys provide full access to all S3 operations across all buckets and bypass RLS policies. These are meant to be used only on the server.\n\nTo authenticate with S3, generate a pair of credentials (Access Key ID and Secret Access Key), copy the endpoint and region from the [project settings page](/dashboard/project/_/settings/storage).\n\nThis is all the information you need to connect to Supabase Storage using any S3-compatible service.\n\n[Image]\n\n ```js\n import { S3Client } from '@aws-sdk/client-s3';\n\n const client = new S3Client({\n forcePathStyle: true,\n region: 'project_region',\n endpoint: 'https://project_ref.supabase.co/storage/v1/s3',\n credentials: {\n accessKeyId: 'your_access_key_id',\n secretAccessKey: 'your_secret_access_key',\n }\n })\n ```\n\n ```bash\n # ~/.aws/credentials\n\n [supabase]\n aws_access_key_id = your_access_key_id\n aws_secret_access_key = your_secret_access_key\n endpoint_url = https://project_ref.supabase.co/storage/v1/s3\n region = project_region\n ```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Session token",
          "content": "You can authenticate to Supabase S3 with a user JWT token to provide limited access via RLS to all S3 operations. This is useful when you want initialize the S3 client on the server scoped to a specific user, or use the S3 client directly from the client side.\n\nAll S3 operations performed with the Session Token are scoped to the authenticated user. RLS policies on the Storage Schema are respected.\n\nTo authenticate with S3 using a Session Token, use the following credentials:\n\n- access_key_id: `project_ref`\n- secret_access_key: `anonKey`\n- session_token: `valid jwt token`\n\nFor example, using the `aws-sdk` library:\n\n```javascript\nimport { S3Client } from '@aws-sdk/client-s3'\n\nconst {\n data: { session },\n} = await supabase.auth.getSession()\n\nconst client = new S3Client({\n forcePathStyle: true,\n region: 'project_region',\n endpoint: 'https://project_ref.supabase.co/storage/v1/s3',\n credentials: {\n accessKeyId: 'project_ref',\n secretAccessKey: 'anonKey',\n sessionToken: session.access_token,\n },\n})\n```",
          "level": 2
        }
      ],
      "wordCount": 311,
      "characterCount": 2389
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-s3-compatibility",
      "identifier": "storage-s3-compatibility",
      "name": "S3 Compatibility",
      "description": "Compatibility spec",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/s3/compatibility",
      "dateModified": "2025-06-13T12:45:11.309895",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/s3/compatibility.mdx",
      "frontmatter": {
        "id": "storage-s3-compatibility",
        "title": "S3 Compatibility",
        "description": "Compatibility spec",
        "subtitle": "Learn about the compatibility of Supabase Storage with S3.",
        "sidebar_label": "S3"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Storage is compatible with the S3 protocol. You can use any S3 client to interact with your Storage objects.\n\nStorage supports [standard](/docs/guides/storage/uploads/standard-uploads), [resumable](/docs/guides/storage/uploads/resumable-uploads) and [S3 uploads](/docs/guides/storage/uploads/s3-uploads) and all these protocols are interoperable. You can upload a file with the S3 protocol and list it with the REST API or upload with Resumable uploads and list with S3.\n\nStorage supports presigning a URL using query parameters. Specifically, Supabase Storage expects requests to be made using [AWS Signature Version 4](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html). To enable this feature, enable the S3 connection via S3 protocol in the Settings page for Supabase Storage.\n\nThe S3 protocol is currently in Public Alpha. If you encounter any issues or have feature requests, [contact us](/dashboard/support/new)."
        },
        {
          "type": "section",
          "title": "Implemented endpoints",
          "content": "The most commonly used endpoints are implemented, and more will be added. Implemented S3 endpoints are marked with ✅ in the following tables.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Bucket operations",
          "content": "{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| API Name | Feature |\n| ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ✅ [ListBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html) | |\n| ✅ [HeadBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadBucket.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [CreateBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateBucket.html) | ❌ ACL: ❌ x-amz-acl ❌ x-amz-grant-full-control ❌ x-amz-grant-read ❌ x-amz-grant-read-acp ❌ x-amz-grant-write ❌ x-amz-grant-write-acp❌ Object Locking: ❌ x-amz-bucket-object-lock-enabled❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [DeleteBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteBucket.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [GetBucketLocation](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketLocation.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [DeleteBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteBucketCors.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [GetBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketEncryption.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [GetBucketLifecycleConfiguration](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketLifecycleConfiguration.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [GetBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketCors.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [PutBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketCors.html) | ❌ Checksums: ❌ x-amz-sdk-checksum-algorithm ❌ x-amz-checksum-algorithm❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ❌ [PutBucketLifecycleConfiguration](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketLifecycleConfiguration.html) | ❌ Checksums: ❌ x-amz-sdk-checksum-algorithm ❌ x-amz-checksum-algorithm❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        },
        {
          "type": "section",
          "title": "Object operations",
          "content": "{/* supa-mdx-lint-disable Rule003Spelling */}\n\n| API Name | Feature |\n| -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ✅ [HeadObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html) | ✅ Conditional Operations: ✅ If-Match ✅ If-Modified-Since ✅ If-None-Match ✅ If-Unmodified-Since✅ Range: ✅ Range (has no effect in HeadObject) ✅ partNumber❌ SSE-C: ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [ListObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html) | Query Parameters: ✅ delimiter ✅ encoding-type ✅ marker ✅ max-keys ✅ prefix❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [ListObjectsV2](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html) | Query Parameters: ✅ list-type ✅ continuation-token ✅ delimiter ✅ encoding-type ✅ fetch-owner ✅ max-keys ✅ prefix ✅ start-after❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html) | ✅ Conditional Operations: ✅ If-Match ✅ If-Modified-Since ✅ If-None-Match ✅ If-Unmodified-Since✅ Range: ✅ Range ✅ PartNumber❌ SSE-C: ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html) | System Metadata: ✅ Content-Type ✅ Cache-Control ✅ Content-Disposition ✅ Content-Encoding ✅ Content-Language ✅ Expires ❌ Content-MD5❌ Object Lifecycle❌ Website: ❌ x-amz-website-redirect-location❌ SSE-C: ❌ x-amz-server-side-encryption ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5 ❌ x-amz-server-side-encryption-aws-kms-key-id ❌ x-amz-server-side-encryption-context ❌ x-amz-server-side-encryption-bucket-key-enabled❌ Request Payer: ❌ x-amz-request-payer❌ Tagging: ❌ x-amz-tagging❌ Object Locking: ❌ x-amz-object-lock-mode ❌ x-amz-object-lock-retain-until-date ❌ x-amz-object-lock-legal-hold❌ ACL: ❌ x-amz-acl ❌ x-amz-grant-full-control ❌ x-amz-grant-read ❌ x-amz-grant-read-acp ❌ x-amz-grant-write-acp❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [DeleteObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html) | ❌ Multi-factor authentication: ❌ x-amz-mfa❌ Object Locking: ❌ x-amz-bypass-governance-retention❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [DeleteObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObjects.html) | ❌ Multi-factor authentication: ❌ x-amz-mfa❌ Object Locking: ❌ x-amz-bypass-governance-retention❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [ListMultipartUploads](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListMultipartUploads.html) | ✅ Query Parameters: ✅ delimiter ✅ encoding-type ✅ key-marker ✅️ max-uploads ✅ prefix ✅ upload-id-marker |\n| ✅ [CreateMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html) | ✅ System Metadata: ✅ Content-Type ✅ Cache-Control ✅ Content-Disposition ✅ Content-Encoding ✅ Content-Language ✅ Expires ❌ Content-MD5❌ Website: ❌ x-amz-website-redirect-location❌ SSE-C: ❌ x-amz-server-side-encryption ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5 ❌ x-amz-server-side-encryption-aws-kms-key-id ❌ x-amz-server-side-encryption-context ❌ x-amz-server-side-encryption-bucket-key-enabled❌ Request Payer: ❌ x-amz-request-payer❌ Tagging: ❌ x-amz-tagging❌ Object Locking: ❌ x-amz-object-lock-mode ❌ x-amz-object-lock-retain-until-date ❌ x-amz-object-lock-legal-hold❌ ACL: ❌ x-amz-acl ❌ x-amz-grant-full-control ❌ x-amz-grant-read ❌ x-amz-grant-read-acp ❌ x-amz-grant-write-acp❌ Storage class: ❌ x-amz-storage-class❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [CompleteMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html) | ❌ Bucket Owner: ❌ x-amz-expected-bucket-owner❌ Request Payer: ❌ x-amz-request-payer |\n| ✅ [AbortMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_AbortMultipartUpload.html) | ❌ Request Payer: ❌ x-amz-request-payer |\n| ✅ [CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html) | ✅ Operation Metadata: ⚠️ x-amz-metadata-directive✅ System Metadata: ✅ Content-Type ✅ Cache-Control ✅ Content-Disposition ✅ Content-Encoding ✅ Content-Language ✅ Expires✅ Conditional Operations: ✅ x-amz-copy-source ✅ x-amz-copy-source-if-match ✅ x-amz-copy-source-if-modified-since ✅ x-amz-copy-source-if-none-match ✅ x-amz-copy-source-if-unmodified-since❌ ACL: ❌ x-amz-acl ❌ x-amz-grant-full-control ❌ x-amz-grant-read ❌ x-amz-grant-read-acp ❌ x-amz-grant-write-acp❌ Website: ❌ x-amz-website-redirect-location❌ SSE-C: ❌ x-amz-server-side-encryption ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5 ❌ x-amz-server-side-encryption-aws-kms-key-id ❌ x-amz-server-side-encryption-context ❌ x-amz-server-side-encryption-bucket-key-enabled ❌ x-amz-copy-source-server-side-encryption-customer-algorithm ❌ x-amz-copy-source-server-side-encryption-customer-key ❌ x-amz-copy-source-server-side-encryption-customer-key-MD5❌ Request Payer: ❌ x-amz-request-payer❌ Tagging: ❌ x-amz-tagging ❌ x-amz-tagging-directive❌ Object Locking: ❌ x-amz-object-lock-mode ❌ x-amz-object-lock-retain-until-date ❌ x-amz-object-lock-legal-hold❌ Bucket Owner: ❌ x-amz-expected-bucket-owner ❌ x-amz-source-expected-bucket-owner❌ Checksums: ❌ x-amz-checksum-algorithm |\n| ✅ [UploadPart](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html) | ✅ System Metadata:❌ Content-MD5❌ SSE-C: ❌ x-amz-server-side-encryption ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n| ✅ [UploadPartCopy](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPartCopy.html) | ❌ Conditional Operations: ❌ x-amz-copy-source ❌ x-amz-copy-source-if-match ❌ x-amz-copy-source-if-modified-since ❌ x-amz-copy-source-if-none-match ❌ x-amz-copy-source-if-unmodified-since✅ Range: ✅ x-amz-copy-source-range❌ SSE-C: ❌ x-amz-server-side-encryption-customer-algorithm ❌ x-amz-server-side-encryption-customer-key ❌ x-amz-server-side-encryption-customer-key-MD5 ❌ x-amz-copy-source-server-side-encryption-customer-algorithm ❌ x-amz-copy-source-server-side-encryption-customer-key ❌ x-amz-copy-source-server-side-encryption-customer-key-MD5❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner ❌ x-amz-source-expected-bucket-owner |\n| ✅ [ListParts](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListParts.html) | Query Parameters: ✅ max-parts ✅ part-number-marker❌ Request Payer: ❌ x-amz-request-payer❌ Bucket Owner: ❌ x-amz-expected-bucket-owner |\n\n{/* supa-mdx-lint-enable Rule003Spelling */}",
          "level": 3
        }
      ],
      "wordCount": 870,
      "characterCount": 12759
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-schema-custom-roles",
      "identifier": "storage-schema-custom-roles",
      "name": "Custom Roles",
      "description": "Learn about the storage schema",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/schema/custom-roles",
      "dateModified": "2025-06-13T12:45:11.310023",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/schema/custom-roles.mdx",
      "frontmatter": {
        "id": "storage-schema-design",
        "title": "Custom Roles",
        "description": "Learn about the storage schema",
        "subtitle": "Learn about using custom roles with storage schema",
        "sidebar_label": "Schema"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "In this guide, you will learn how to create and use custom roles with Storage to manage role-based access to objects and buckets. The same approach can be used to use custom roles with any other Supabase service.\n\nSupabase Storage uses the same role-based access control system as any other Supabase service using RLS (Row Level Security)."
        },
        {
          "type": "section",
          "title": "Create a custom role",
          "content": "Let's create a custom role `manager` to provide full read access to a specific bucket. For a more advanced setup, see the [RBAC Guide](/docs/guides/auth/custom-claims-and-role-based-access-control-rbac#create-auth-hook-to-apply-user-role).\n\n```sql\ncreate role 'manager';\n\n-- Important to grant the role to the authenticator and anon role\ngrant manager to authenticator;\ngrant anon to manager;\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Create a policy",
          "content": "Let's create a policy that gives full read permissions to all objects in the bucket `teams` for the `manager` role.\n\n```sql\ncreate policy \"Manager can view all files in the bucket 'teams'\"\non storage.objects\nfor select\nto manager\nusing (\n bucket_id = 'teams'\n);\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Test the policy",
          "content": "To impersonate the `manager` role, you will need a valid JWT token with the `manager` role.\nYou can quickly create one using the `jsonwebtoken` library in Node.js.\n\nSigning a new JWT requires your `JWT_SECRET`. You must store this secret securely. Never expose it in frontend code, and do not check it into version control.\n\n```js\nconst jwt = require('jsonwebtoken')\n\nconst JWT_SECRET = 'your-jwt-secret' // You can find this in your Supabase project settings under API. Store this securely.\nconst USER_ID = '' // the user id that we want to give the manager role\n\nconst token = jwt.sign({ role: 'manager', sub: USER_ID }, JWT_SECRET, {\n expiresIn: '1h',\n})\n```\n\nNow you can use this token to access the Storage API.\n\n```js\nconst { StorageClient } = require('@supabase/storage-js')\n\nconst PROJECT_URL = 'https://your-project-id.supabase.co/storage/v1'\n\nconst storage = new StorageClient(PROJECT_URL, {\n authorization: `Bearer ${token}`,\n})\n\nawait storage.from('teams').list()\n```",
          "level": 2
        }
      ],
      "wordCount": 308,
      "characterCount": 2050
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-schema-design",
      "identifier": "storage-schema-design",
      "name": "The Storage Schema",
      "description": "Learn about the storage schema",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/schema/design",
      "dateModified": "2025-06-13T12:45:11.310117",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/schema/design.mdx",
      "frontmatter": {
        "id": "storage-schema-design",
        "title": "The Storage Schema",
        "description": "Learn about the storage schema",
        "subtitle": "Learn about the storage schema",
        "sidebar_label": "Schema"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Storage uses Postgres to store metadata regarding your buckets and objects. Users can use RLS (Row-Level Security) policies for access control. This data is stored in a dedicated schema within your project called `storage`.\n\nWhen working with SQL, it's crucial to consider all records in Storage tables as read-only. All operations, including uploading, copying, moving, and deleting, should **exclusively go through the API**.\n\nThis is important because the storage schema only stores the metadata and the actual objects are stored in a provider like S3. Deleting the metadata doesn't remove the object in the underlying storage provider. This results in your object being inaccessible, but you'll still be billed for it.\n\nHere is the schema that represents the Storage service:\n\n[Image]\n\nYou have the option to query this table directly to retrieve information about your files in Storage without the need to go through our API."
        },
        {
          "type": "section",
          "title": "Modifying the schema",
          "content": "We strongly recommend refraining from making any alterations to the `storage` schema and treating it as read-only. This approach is important because any modifications to the schema on your end could potentially clash with our future updates, leading to downtime.\n\nHowever, we encourage you to add custom indexes as they can significantly improve the performance of the RLS policies you create for enforcing access control.",
          "level": 2
        }
      ],
      "wordCount": 215,
      "characterCount": 1380
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-schema-helper-functions",
      "identifier": "storage-schema-helper-functions",
      "name": "Storage Helper Functions",
      "description": "Learn the storage schema",
      "category": "functions",
      "url": "https://supabase.com/docs/guides/storage/schema/helper-functions",
      "dateModified": "2025-06-13T12:45:11.310201",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/schema/helper-functions.mdx",
      "frontmatter": {
        "id": "storage-schema",
        "title": "Storage Helper Functions",
        "description": "Learn the storage schema",
        "subtitle": "Learn the storage schema",
        "sidebar_label": "Schema"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Storage provides SQL helper functions which you can use to write RLS policies."
        },
        {
          "type": "section",
          "title": "`storage.filename()`",
          "content": "Returns the name of a file. For example, if your file is stored in `public/subfolder/avatar.png` it would return: `'avatar.png'`\n\n**Usage**\n\nThis example demonstrates how you would allow any user to download a file called `favicon.ico`:\n\n```sql\ncreate policy \"Allow public downloads\"\non storage.objects\nfor select\nto public\nusing (\n storage.filename(name) = 'favicon.ico'\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`storage.foldername()`",
          "content": "Returns an array path, with all of the subfolders that a file belongs to. For example, if your file is stored in `public/subfolder/avatar.png` it would return: `[ 'public', 'subfolder' ]`\n\n**Usage**\n\nThis example demonstrates how you would allow authenticated users to upload files to a folder called `private`:\n\n```sql\ncreate policy \"Allow authenticated uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n (storage.foldername(name))[1] = 'private'\n);\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "`storage.extension()`",
          "content": "Returns the extension of a file. For example, if your file is stored in `public/subfolder/avatar.png` it would return: `'png'`\n\n**Usage**\n\nThis example demonstrates how you would allow restrict uploads to only PNG files inside a bucket called `cats`:\n\n```sql\ncreate policy \"Only allow PNG uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n bucket_id = 'cats' and storage.extension(name) = 'png'\n);\n```",
          "level": 3
        }
      ],
      "wordCount": 205,
      "characterCount": 1445
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-security-access-control",
      "identifier": "storage-security-access-control",
      "name": "Storage Access Control",
      "description": "Learn how to restrict Supabase file uploads.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/security/access-control",
      "dateModified": "2025-06-13T12:45:11.310327",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/security/access-control.mdx",
      "frontmatter": {
        "id": "storage-access-control",
        "title": "Storage Access Control",
        "description": "Learn how to restrict Supabase file uploads.",
        "sidebar_label": "Uploads",
        "tocVideo": "4ERX__Y908k"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Storage is designed to work perfectly with Postgres [Row Level Security](https://supabase.com/docs/guides/database/postgres/row-level-security) (RLS).\n\nYou can use RLS to create [Security Access Policies](https://www.postgresql.org/docs/current/sql-createpolicy.html) that are incredibly powerful and flexible, allowing you to restrict access based on your business needs."
        },
        {
          "type": "section",
          "title": "Access policies",
          "content": "By default Storage does not allow any uploads to buckets without RLS policies. You selectively allow certain operations by creating RLS policies on the `storage.objects` table.\n\nYou can find the documentation for the storage schema [here](/docs/guides/storage/schema/design) , and to simplify the process of crafting your policies, you can utilize these [helper functions](/docs/guides/storage/schema/helper-functions) .\n\nThe RLS policies required for different operations are documented [here](/docs/reference/javascript/storage-createbucket)\n\nFor example, the only RLS policy required for [uploading](/docs/reference/javascript/storage-from-upload) objects is to grant the `INSERT` permission to the `storage.objects` table.\n\nTo allow overwriting files using the `upsert` functionality you will need to additionally grant `SELECT` and `UPDATE` permissions.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Policy examples",
          "content": "An easy way to get started would be to create RLS policies for `SELECT`, `INSERT`, `UPDATE`, `DELETE` operations and restrict the policies to meet your security requirements. For example, one can start with the following `INSERT` policy:\n\n```sql\ncreate policy \"policy_name\"\nON storage.objects\nfor insert with check (\n true\n);\n```\n\nand modify it to only allow authenticated users to upload assets to a specific bucket by changing it to:\n\n```sql\ncreate policy \"policy_name\"\non storage.objects for insert to authenticated with check (\n -- restrict bucket\n bucket_id = 'my_bucket_id'\n);\n```\n\nThis example demonstrates how you would allow authenticated users to upload files to a folder called `private` inside `my_bucket_id`:\n\n```sql\ncreate policy \"Allow authenticated uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n bucket_id = 'my_bucket_id' and\n (storage.foldername(name))[1] = 'private'\n);\n```\n\nThis example demonstrates how you would allow authenticated users to upload files to a folder called with their `users.id` inside `my_bucket_id`:\n\n```sql\ncreate policy \"Allow authenticated uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n bucket_id = 'my_bucket_id' and\n (storage.foldername(name))[1] = (select auth.uid()::text)\n);\n```\n\nAllow a user to access a file that was previously uploaded by the same user:\n\n```sql\ncreate policy \"Individual user Access\"\non storage.objects for select\nto authenticated\nusing ( (select auth.uid()) = owner_id::uuid );\n```\n\n---\n\n{/* Finish with a video. This also appears in the Sidebar via the \"tocVideo\" metadata */}",
          "level": 2
        },
        {
          "type": "section",
          "title": "Bypassing access controls",
          "content": "If you exclusively use Storage from trusted clients, such as your own servers, and need to bypass the RLS policies, you can use the `service key` in the `Authorization` header. Service keys entirely bypass RLS policies, granting you unrestricted access to all Storage APIs.\n\nRemember you should not share the service key publicly.",
          "level": 2
        }
      ],
      "wordCount": 434,
      "characterCount": 3237
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-security-ownership",
      "identifier": "storage-security-ownership",
      "name": "Ownership",
      "description": "Learn how ownership works in Supabase Storage and how to control access",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/security/ownership",
      "dateModified": "2025-06-13T12:45:11.310418",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/security/ownership.mdx",
      "frontmatter": {
        "id": "storage-access-control",
        "title": "Ownership",
        "description": "Learn how ownership works in Supabase Storage and how to control access",
        "sidebar_label": "Security"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "When creating new buckets or objects in Supabase Storage, an owner is automatically assigned to the bucket or object. The owner is the user who created the resource and the value is derived from the `sub` claim in the JWT.\nWe store the `owner` in the `owner_id` column.\n\nWhen using the `service_key` to create a resource, the owner will not be set and the resource will be owned by anyone. This is also the case when you are creating Storage resources via the Dashboard.\n\nThe Storage schema has 2 fields to represent ownership: `owner` and `owner_id`. `owner` is deprecated and will be removed. Use `owner_id` instead."
        },
        {
          "type": "section",
          "title": "Access control",
          "content": "By itself, the ownership of a resource does not provide any access control. However, you can enforce the ownership by implementing access control against storage resources scoped to their owner.\n\nFor example, you can implement a policy where only the owner of an object can delete it. To do this, check the `owner_id` field of the object and compare it with the `sub` claim of the JWT:\n\n```sql\ncreate policy \"User can delete their own objects\"\non storage.objects\nfor delete\nto authenticated\nusing (\n owner_id = (select auth.uid())\n);\n```\n\nThe use of RLS policies is just one way to enforce access control. You can also implement access control in your server code by following the same pattern.",
          "level": 2
        }
      ],
      "wordCount": 227,
      "characterCount": 1333
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-serving-bandwidth",
      "identifier": "storage-serving-bandwidth",
      "name": "Bandwidth & Storage Egress",
      "description": "How to calculate the Bandwidth and Storage Egress",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/serving/bandwidth",
      "dateModified": "2025-06-13T12:45:11.310521",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/serving/bandwidth.mdx",
      "frontmatter": {
        "id": "storage-bandwidth",
        "title": "Bandwidth & Storage Egress",
        "description": "How to calculate the Bandwidth and Storage Egress",
        "subtitle": "Bandwidth & Storage Egress",
        "sidebar_label": "Bandwidth & Storage Egress"
      },
      "sections": [
        {
          "type": "section",
          "title": "Bandwidth & Storage egress",
          "content": "Free Plan Organizations in Supabase have a limit of 5 GB of bandwidth. This limit is calculated by the sum of all the data transferred from the Supabase servers to the client. This includes all the data transferred from the database, storage, and functions.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Checking Storage egress requests in Logs Explorer:",
          "content": "We have a template query that you can use to get the number of requests for each object in [Logs Explorer](/dashboard/project/_/logs/explorer/templates).\n\n```sql\nselect\n r.method as http_verb,\n r.path as filepath,\n count(*) as num_requests\nfrom\n edge_logs\n cross join unnest(metadata) as m\n cross join unnest(m.request) as r\n cross join unnest(r.headers) as h\nwhere (path like '%storage/v1/object/%' or path like '%storage/v1/render/%') and r.method = 'GET'\ngroup by r.path, r.method\norder by num_requests desc\nlimit 100;\n```\n\nExample of the output:\n\n```\n[\n {\"filepath\":\"/storage/v1/object/sign/large%20bucket/20230902_200037.gif\",\n \"http_verb\":\"GET\",\n \"num_requests\":100\n },\n {\"filepath\":\"/storage/v1/object/public/demob/Sports/volleyball.png\",\n \"http_verb\":\"GET\",\n \"num_requests\":168\n }\n]\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Calculating egress:",
          "content": "If you already know the size of those files, you can calculate the egress by multiplying the number of requests by the size of the file.\nYou can also get the size of the file with the following cURL:\n\n```bash\ncurl -s -w \"%{size_download}\\n\" -o /dev/null \"https://my_project.supabase.co/storage/v1/object/large%20bucket/20230902_200037.gif\"\n```\n\nThis will return the size of the file in bytes.\nFor this example, let's say that `20230902_200037.gif` has a file size of 3 megabytes and `volleyball.png` has a file size of 570 kilobytes.\n\nNow, we have to sum all the egress for all the files to get the total egress:\n\n```\n100 * 3MB = 300MB\n168 * 570KB = 95.76MB\nTotal Egress = 395.76MB\n```\n\nYou can see that these values can get quite large, so it's important to keep track of the egress and optimize the files.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Optimizing egress:",
          "content": "If you are on the Pro Plan, you can use the [Supabase Image Transformations](/docs/guides/storage/image-transformations) to optimize the images and reduce the egress.",
          "level": 3
        }
      ],
      "wordCount": 310,
      "characterCount": 2166
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-serving-downloads",
      "identifier": "storage-serving-downloads",
      "name": "Serving assets from Storage",
      "description": "Serving assets from Storage",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/serving/downloads",
      "dateModified": "2025-06-13T12:45:11.310617",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/serving/downloads.mdx",
      "frontmatter": {
        "id": "storage-image-downloads",
        "title": "Serving assets from Storage",
        "description": "Serving assets from Storage",
        "subtitle": "Serving assets from Storage",
        "sidebar_label": "Serving assets",
        "tocVideo": "dLqSmxX3r7I"
      },
      "sections": [
        {
          "type": "section",
          "title": "Public buckets",
          "content": "As mentioned in the [Buckets Fundamentals](/docs/guides/storage/buckets/fundamentals) all files uploaded in a public bucket are publicly accessible and benefit a high CDN cache HIT ratio.\n\nYou can access them by using this conventional URL:\n\n```\nhttps://[project_id].supabase.co/storage/v1/object/public/[bucket]/[asset-name]\n```\n\nYou can also use the Supabase SDK `getPublicUrl` to generate this URL for you\n\n```js\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nconst { data } = supabase.storage.from('bucket').getPublicUrl('filePath.jpg')\n\nconsole.log(data.publicUrl)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Downloading",
          "content": "If you want the browser to start an automatic download of the asset instead of trying serving it, you can add the `?download` query string parameter.\n\nBy default it will use the asset name to save the file on disk. You can optionally pass a custom name to the `download` parameter as following: `?download=customname.jpg`",
          "level": 3
        },
        {
          "type": "section",
          "title": "Private buckets",
          "content": "Assets stored in a non-public bucket are considered private and are not accessible via a public URL like the public buckets.\n\nYou can access them only by:\n\n- Signing a time limited URL on the Server, for example with Edge Functions.\n- with a GET request the URL `https://[project_id].supabase.co/storage/v1/object/authenticated/[bucket]/[asset-name]` and the user Authorization header",
          "level": 2
        },
        {
          "type": "section",
          "title": "Signing URLs",
          "content": "You can sign a time-limited URL that you can share to your users by invoking the `createSignedUrl` method on the SDK.\n\n```js\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nconst { data, error } = await supabase.storage\n .from('bucket')\n .createSignedUrl('private-document.pdf', 3600)\n\nif (data) {\n console.log(data.signedUrl)\n}\n```",
          "level": 3
        }
      ],
      "wordCount": 243,
      "characterCount": 1879
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-serving-image-transformations",
      "identifier": "storage-serving-image-transformations",
      "name": "Storage Image Transformations",
      "description": "Transform images with Storage",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/serving/image-transformations",
      "dateModified": "2025-06-13T12:45:11.311135",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/serving/image-transformations.mdx",
      "frontmatter": {
        "id": "storage-image-transformations",
        "title": "Storage Image Transformations",
        "description": "Transform images with Storage",
        "subtitle": "Transform images with Storage",
        "sidebar_label": "Image Transformations",
        "tocVideo": "dLqSmxX3r7I"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "Supabase Storage offers the functionality to optimize and resize images on the fly. Any image stored in your buckets can be transformed and optimized for fast delivery.\n\nImage Resizing is currently enabled for [Pro Plan and above](https://supabase.com/pricing)."
        },
        {
          "type": "section",
          "title": "Get a public URL for a transformed image",
          "content": "Our client libraries methods like `getPublicUrl` and `createSignedUrl` support the `transform` option. This returns the URL that serves the transformed image.\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nsupabase.storage.from('bucket').getPublicUrl('image.jpg', {\n transform: {\n width: 500,\n height: 600,\n },\n})\n```\n\n```dart\nfinal url = supabase.storage.from('bucket').getPublicUrl(\n 'image.jpg',\n transform: const TransformOptions(\n width: 500,\n height: 600,\n ),\n );\n```\n\n```swift\nlet url = try await supabase.storage.from(\"bucket\")\n .getPublicURL(\n path: \"image.jpg\"\n options: TransformOptions(with: 500, height: 600)\n )\n```\n\n```kotlin\nval url = supabase.storage.from(\"bucket\").publicRenderUrl(\"image.jpg\") {\n size(width = 500, height = 600)\n}\n```\n\n```python\nurl = supabase.storage.from_('avatars').get_public_url(\n 'image.jpg',\n {\n 'transform': {\n 'width': 500,\n 'height': 500,\n },\n }\n)\n```\n\nAn example URL could look like this:\n\n```\nhttps://project_id.supabase.co/storage/v1/render/image/public/bucket/image.jpg?width=500&height=600`\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Signing URLs with transformation options",
          "content": "To share a transformed image in a private bucket for a fixed amount of time, provide the transform option when you create the signed URL:\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nsupabase.storage.from('bucket').createSignedUrl('image.jpg', 60000, {\n transform: {\n width: 200,\n height: 200,\n },\n})\n```\n\n```dart\nfinal url = await supabase.storage.from('bucket').createSignedUrl(\n 'image.jpg',\n 60000,\n transform: const TransformOptions(\n width: 200,\n height: 200,\n ),\n );\n```\n\n```swift\nlet url = try await supabase.storage.from(\"bucket\")\n .createSignedURL(\n path: \"image.jpg\",\n expiresIn: 60,\n transform: TransformOptions(\n width: 200,\n height: 200\n )\n )\n```\n\n```kotlin\nval url = supabase.storage.from(\"bucket\").createSignedUrl(\"image.jpg\", 60.seconds) {\n size(200, 200)\n}\n```\n\nThe transformation options are embedded into the token attached to the URL — they cannot be changed once signed.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Downloading images",
          "content": "To download a transformed image, pass the `transform` option to the `download` function.\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nsupabase.storage.from('bucket').download('image.jpg', {\n transform: {\n width: 800,\n height: 300,\n },\n})\n```\n\n```dart\nfinal data = await supabase.storage.from('bucket').download(\n 'image.jpg',\n transform: const TransformOptions(\n width: 800,\n height: 300,\n ),\n );\n```\n\n```swift\nlet data = try await supabase.storage.from(\"bucket\")\n .download(\n path: \"image.jpg\",\n options: TransformOptions(\n width: 800,\n height: 300\n )\n )\n```\n\n```kotlin\nval data = supabase.storage.from(\"bucket\").downloadAuthenticated(\"image.jpg\") {\n transform {\n size(800, 300)\n }\n}\n\n//Or on JVM stream directly to a file\nval file = File(\"image.jpg\")\nsupabase.storage.from(\"bucket\").downloadAuthenticatedTo(\"image.jpg\", file) {\n transform {\n size(800, 300)\n }\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket').download(\n 'image.jpg',\n {\n 'width': 800,\n 'height': 300,\n },\n)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Automatic image optimization (WebP)",
          "content": "When using the image transformation API, Storage will automatically find the best format supported by the client and return that to the client, without any code change. For instance, if you use Chrome when viewing a JPEG image and using transformation options, you'll see that images are automatically optimized as `webp` images.\n\nAs a result, this will lower the bandwidth that you send to your users and your application will load much faster.\n\nWe currently only support WebP. AVIF support will come in the near future.\n\n**Disabling automatic optimization:**\n\nIn case you'd like to return the original format of the image and **opt-out** from the automatic image optimization detection, you can pass the `format=origin` parameter when requesting a transformed image, this is also supported in the JavaScript SDK starting from v2.2.0\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nawait supabase.storage.from('bucket').download('image.jpeg', {\n transform: {\n width: 200,\n height: 200,\n format: 'origin',\n },\n})\n```\n\n```dart\nfinal data = await supabase.storage.from('bucket').download(\n 'image.jpeg',\n transform: const TransformOptions(\n width: 200,\n height: 200,\n format: RequestImageFormat.origin,\n ),\n );\n```\n\n```swift\nlet data = try await supabase.storage.from(\"bucket\")\n .download(\n path: \"image.jpg\",\n options: TransformOptions(\n width: 200,\n height: 200,\n format: \"origin\"\n )\n )\n```\n\n```kotlin\nval data = supabase.storage.from(\"bucket\").downloadAuthenticated(\"image.jpg\") {\n transform {\n size(200, 200)\n format = ImageTransformation.Format.ORIGIN\n }\n}\n\n//Or on JVM stream directly to a file\nval file = File(\"image.jpg\")\nsupabase.storage.from(\"bucket\").downloadAuthenticatedTo(\"image.jpg\", file) {\n transform {\n size(200, 200)\n format = ImageTransformation.Format.ORIGIN\n }\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket').download(\n 'image.jpeg',\n {\n 'transform': {\n 'width': 200,\n 'height': 200,\n 'format': 'origin',\n },\n }\n)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Next.js loader",
          "content": "You can use Supabase Image Transformation to optimize your Next.js images using a custom [Loader](https://nextjs.org/docs/api-reference/next/image#loader-configuration).\n\nTo get started, create a `supabase-image-loader.js` file in your Next.js project which exports a default function:\n\n```ts\nconst projectId = '' // your supabase project id\n\nexport default function supabaseLoader({ src, width, quality }) {\n return `https://${projectId}.supabase.co/storage/v1/render/image/public/${src}?width=${width}&quality=${quality || 75}`\n}\n```\n\nIn your `nextjs.config.js` file add the following configuration to instruct Next.js to use our custom loader\n\n```js\nmodule.exports = {\n images: {\n loader: 'custom',\n loaderFile: './supabase-image-loader.js',\n },\n}\n```\n\nAt this point you are ready to use the `Image` component provided by Next.js\n\n```tsx\nimport Image from 'next/image'\n\nconst MyImage = (props) => {\n return \n}\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Transformation options",
          "content": "We currently support a few transformation options focusing on optimizing, resizing, and cropping images.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Optimizing",
          "content": "You can set the quality of the returned image by passing a value from 20 to 100 (with 100 being the highest quality) to the `quality` parameter. This parameter defaults to 80.\n\nExample:\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nsupabase.storage.from('bucket').download('image.jpg', {\n transform: {\n quality: 50,\n },\n})\n```\n\n```dart\nfinal data = await supabase.storage.from('bucket').download(\n 'image.jpg',\n transform: const TransformOptions(\n quality: 50,\n ),\n );\n```\n\n```swift\nlet data = try await supabase.storage.from(\"bucket\")\n .download(\n path: \"image.jpg\",\n options: TransformOptions(\n quality: 50\n )\n )\n```\n\n```kotlin\nval data = supabase.storage[\"bucket\"].downloadAuthenticated(\"image.jpg\") {\n transform {\n quality = 50\n }\n}\n\n//Or on JVM stream directly to a file\nval file = File(\"image.jpg\")\nsupabase.storage[\"bucket\"].downloadAuthenticatedTo(\"image.jpg\", file) {\n transform {\n quality = 50\n }\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket').download(\n 'image.jpg',\n {\n 'transform': {\n 'quality': 50,\n },\n }\n)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Resizing",
          "content": "You can use `width` and `height` parameters to resize an image to a specific dimension. If only one parameter is specified, the image will be resized and cropped, maintaining the aspect ratio.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Modes",
          "content": "You can use different resizing modes to fit your needs, each of them uses a different approach to resize the image:\n\nUse the `resize` parameter with one of the following values:\n\n- `cover` : resizes the image while keeping the aspect ratio to fill a given size and crops projecting parts. (default)\n\n- `contain` : resizes the image while keeping the aspect ratio to fit a given size.\n\n- `fill` : resizes the image without keeping the aspect ratio.\n\nExample:\n\n```ts\nimport { createClient } from '@supabase/supabase-js'\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// ---cut---\nsupabase.storage.from('bucket').download('image.jpg', {\n transform: {\n width: 800,\n height: 300,\n resize: 'contain', // 'cover' | 'fill'\n },\n})\n```\n\n```dart\nfinal data = supabase.storage.from('bucket').download(\n 'image.jpg',\n transform: const TransformOptions(\n width: 800,\n height: 300,\n resize: ResizeMode.contain, // 'cover' | 'fill'\n ),\n );\n```\n\n```swift\nlet data = try await supabase.storage.from(\"bucket\")\n .download(\n path: \"image.jpg\",\n options: TransformOptions(\n width: 800,\n height: 300,\n resize: \"contain\" // \"cover\" | \"fill\"\n )\n )\n```\n\n```kotlin\nval data = supabase.storage.from(\"bucket\").downloadAuthenticated(\"image.jpg\") {\n transform {\n size(800, 300)\n resize = ImageTransformation.Resize.CONTAIN\n }\n}\n\n//Or on JVM stream directly to a file\nval file = File(\"image.jpg\")\nsupabase.storage.from(\"bucket\").downloadAuthenticatedTo(\"image.jpg\", file) {\n transform {\n size(800, 300)\n resize = ImageTransformation.Resize.CONTAIN\n }\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket').download(\n 'image.jpg',\n {\n 'transform': {\n 'width': 800,\n 'height': 300,\n 'resize': 'contain', # 'cover' | 'fill'\n }\n }\n)\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Limits",
          "content": "- Width and height must be an integer value between 1-2500.\n- The image size cannot exceed 25MB.\n- The image resolution cannot exceed 50MP.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Supported image formats",
          "content": "| Format | Extension | Source | Result |\n| ------ | --------- | ------ | ------ |\n| PNG | `png` | ☑️ | ☑️ |\n| JPEG | `jpg` | ☑️ | ☑️ |\n| WebP | `webp` | ☑️ | ☑️ |\n| AVIF | `avif` | ☑️ | ☑️ |\n| GIF | `gif` | ☑️ | ☑️ |\n| ICO | `ico` | ☑️ | ☑️ |\n| SVG | `svg` | ☑️ | ☑️ |\n| HEIC | `heic` | ☑️ | ❌ |\n| BMP | `bmp` | ☑️ | ☑️ |\n| TIFF | `tiff` | ☑️ | ☑️ |",
          "level": 3
        },
        {
          "type": "section",
          "title": "Pricing",
          "content": "For a detailed breakdown of how charges are calculated, refer to [Manage Storage Image Transformations usage](/docs/guides/platform/manage-your-usage/storage-image-transformations).",
          "level": 2
        },
        {
          "type": "section",
          "title": "Self hosting",
          "content": "Our solution to image resizing and optimization can be self-hosted as with any other Supabase product. Under the hood we use [imgproxy](https://imgproxy.net/)",
          "level": 2
        },
        {
          "type": "section",
          "title": "imgproxy configuration:",
          "content": "Deploy an imgproxy container with the following configuration:\n\n```yaml\nimgproxy:\n image: darthsim/imgproxy\n environment:\n - IMGPROXY_ENABLE_WEBP_DETECTION=true\n - IMGPROXY_JPEG_PROGRESSIVE=true\n```\n\nNote: make sure that this service can only be reachable within an internal network and not exposed to the public internet",
          "level": 4
        },
        {
          "type": "section",
          "title": "Storage API configuration:",
          "content": "Once [imgproxy](https://imgproxy.net/) is deployed we need to configure a couple of environment variables in your self-hosted [`storage-api`](https://github.com/supabase/storage-api) service as follows:\n\n```shell\nENABLE_IMAGE_TRANSFORMATION=true\nIMGPROXY_URL=yourinternalimgproxyurl.internal.com\n```\n\n{/* Finish with a video. This also appears in the Sidebar via the \"tocVideo\" metadata */}",
          "level": 4
        }
      ],
      "wordCount": 1474,
      "characterCount": 11577
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-uploads-file-limits",
      "identifier": "storage-uploads-file-limits",
      "name": "Limits",
      "description": "Learn how to increase Supabase file limits.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/uploads/file-limits",
      "dateModified": "2025-06-13T12:45:11.311281",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/uploads/file-limits.mdx",
      "frontmatter": {
        "id": "storage-file-limits",
        "title": "Limits",
        "subtitle": "Learn how to increase Supabase file limits.",
        "description": "Learn how to increase Supabase file limits.",
        "sidebar_label": "Limits"
      },
      "sections": [
        {
          "type": "section",
          "title": "Global file size",
          "content": "You can set the max file size across all your buckets by setting this global value in the dashboard [here](https://supabase.com/dashboard/project/_/settings/storage). For Free projects, the limit can't exceed 50 MB. On the Pro Plan and up, you can set this value to up to 50 GB. If you need more than 50 GB, [contact us](https://supabase.com/dashboard/support/new).\n\n| Plan | Max File Size Limit |\n| ---------- | ------------------- |\n| Free | 50 MB |\n| Pro | 50 GB |\n| Team | 50 GB |\n| Enterprise | Custom |\n\nThis option is a global limit, which applies to all your buckets.\n\nAdditionally, you can specify the max file size on a per [bucket level](/docs/guides/storage/buckets/creating-buckets#restricting-uploads) but it can't be higher than this global limit. As a good practice, the global limit should be set to the highest possible file size that your application accepts, and apply per bucket limits.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Per bucket restrictions",
          "content": "You can have different restrictions on a per bucket level such as restricting the file types (e.g. `pdf`, `images`, `videos`) or the max file size, which should be lower than the global limit. To apply these limit on a bucket level see [Creating Buckets](/docs/guides/storage/buckets/creating-buckets#restricting-uploads).",
          "level": 2
        }
      ],
      "wordCount": 201,
      "characterCount": 1280
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-uploads-resumable-uploads",
      "identifier": "storage-uploads-resumable-uploads",
      "name": "Resumable Uploads",
      "description": "Learn how to upload files to Supabase Storage.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/uploads/resumable-uploads",
      "dateModified": "2025-06-13T12:45:11.311581",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/uploads/resumable-uploads.mdx",
      "frontmatter": {
        "id": "resumable-uploads",
        "title": "Resumable Uploads",
        "description": "Learn how to upload files to Supabase Storage.",
        "subtitle": "Learn how to upload files to Supabase Storage.",
        "sidebar_label": "Uploads"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "The resumable upload method is recommended when:\n\n- Uploading large files that may exceed 6MB in size\n- Network stability is a concern\n- You want to have progress events for your uploads\n\nSupabase Storage implements the [TUS protocol](https://tus.io/) to enable resumable uploads. TUS stands for The Upload Server and is an open protocol for supporting resumable uploads. The protocol allows the upload process to be resumed from where it left off in case of interruptions. This method can be implemented using the [`tus-js-client`](https://github.com/tus/tus-js-client) library, or other client-side libraries like [Uppy](https://uppy.io/docs/tus/) that support the TUS protocol.\n\n Here's an example of how to upload a file using `tus-js-client`:\n\n ```javascript\n const tus = require('tus-js-client')\n\n const projectId = ''\n\n async function uploadFile(bucketName, fileName, file) {\n const { data: { session } } = await supabase.auth.getSession()\n\n return new Promise((resolve, reject) => {\n var upload = new tus.Upload(file, {\n endpoint: `https://${projectId}.supabase.co/storage/v1/upload/resumable`,\n retryDelays: [0, 3000, 5000, 10000, 20000],\n headers: {\n authorization: `Bearer ${session.access_token}`,\n 'x-upsert': 'true', // optionally set upsert to true to overwrite existing files\n },\n uploadDataDuringCreation: true,\n removeFingerprintOnSuccess: true, // Important if you want to allow re-uploading the same file https://github.com/tus/tus-js-client/blob/main/docs/api.md#removefingerprintonsuccess\n metadata: {\n bucketName: bucketName,\n objectName: fileName,\n contentType: 'image/png',\n cacheControl: 3600,\n },\n chunkSize: 6 * 1024 * 1024, // NOTE: it must be set to 6MB (for now) do not change it\n onError: function (error) {\n console.log('Failed because: ' + error)\n reject(error)\n },\n onProgress: function (bytesUploaded, bytesTotal) {\n var percentage = ((bytesUploaded / bytesTotal) * 100).toFixed(2)\n console.log(bytesUploaded, bytesTotal, percentage + '%')\n },\n onSuccess: function () {\n console.log('Download %s from %s', upload.file.name, upload.url)\n resolve()\n },\n })\n\n // Check if there are any previous uploads to continue.\n return upload.findPreviousUploads().then(function (previousUploads) {\n // Found previous uploads so we select the first one.\n if (previousUploads.length) {\n upload.resumeFromPreviousUpload(previousUploads[0])\n }\n\n // Start the upload\n upload.start()\n })\n })\n }\n ```\n\n Here's an example of how to upload a file using `@uppy/tus` with react:\n\n ```javascript\n import { useEffect, useState } from \"react\";\n import { createClient } from \"@supabase/supabase-js\";\n import Uppy from \"@uppy/core\";\n import Tus from \"@uppy/tus\";\n import Dashboard from \"@uppy/dashboard\";\n import \"@uppy/core/dist/style.min.css\";\n import \"@uppy/dashboard/dist/style.min.css\";\n\n function App() {\n // Initialize Uppy instance with the 'sample' bucket specified for uploads\n const uppy = useUppyWithSupabase({ bucketName: \"sample\" });\n\n useEffect(() => {\n // Set up Uppy Dashboard to display as an inline component within a specified target\n uppy.use(Dashboard, {\n inline: true, // Ensures the dashboard is rendered inline\n target: \"#drag-drop-area\", // HTML element where the dashboard renders\n showProgressDetails: true, // Show progress details for file uploads\n });\n }, []);\n\n return (\n\n {/* Target element for the Uppy Dashboard */}\n );\n }\n\n export default App;\n\n /**\n * Custom hook for configuring Uppy with Supabase authentication and TUS resumable uploads\n * @param {Object} options - Configuration options for the Uppy instance.\n * @param {string} options.bucketName - The bucket name in Supabase where files are stored.\n * @returns {Object} uppy - Uppy instance with configured upload settings.\n */\n export const useUppyWithSupabase = ({ bucketName }: { bucketName: string }) => {\n // Initialize Uppy instance only once\n const [uppy] = useState(() => new Uppy());\n // Initialize Supabase client with project URL and anon key\n const supabase = createClient(projectURL, anonKey);\n\n useEffect(() => {\n const initializeUppy = async () => {\n // Retrieve the current user's session for authentication\n const {\n data: { session },\n } = await supabase.auth.getSession();\n\n uppy.use(Tus, {\n endpoint: `${projectURL}/storage/v1/upload/resumable`, // Supabase TUS endpoint\n retryDelays: [0, 3000, 5000, 10000, 20000], // Retry delays for resumable uploads\n headers: {\n authorization: `Bearer ${session?.access_token}`, // User session access token\n apikey: anonKey, // API key for Supabase\n },\n uploadDataDuringCreation: true, // Send metadata with file chunks\n removeFingerprintOnSuccess: true, // Remove fingerprint after successful upload\n chunkSize: 6 * 1024 * 1024, // Chunk size for TUS uploads (6MB)\n allowedMetaFields: [\n \"bucketName\",\n \"objectName\",\n \"contentType\",\n \"cacheControl\",\n ], // Metadata fields allowed for the upload\n onError: (error) => console.error(\"Upload error:\", error), // Error handling for uploads\n }).on(\"file-added\", (file) => {\n // Attach metadata to each file, including bucket name and content type\n file.meta = {\n ...file.meta,\n bucketName, // Bucket specified by the user of the hook\n objectName: file.name, // Use file name as object name\n contentType: file.type, // Set content type based on file MIME type\n };\n });\n };\n\n // Initialize Uppy with Supabase settings\n initializeUppy();\n }, [uppy, bucketName]);\n\n // Return the configured Uppy instance\n return uppy;\n };\n\n ```\n\n Kotlin supports resumable uploads natively for all targets:\n\n ```kotlin\n suspend fun uploadFile(file: File) {\n val upload: ResumableUpload = supabase.storage.from(\"bucket_name\")\n .resumable.createOrContinueUpload(\"file_path\", file)\n upload.stateFlow\n .onEach {\n println(it.progress)\n }\n .launchIn(yourCoroutineScope)\n upload.startOrResumeUploading()\n }\n\n // On other platforms you might have to give the bytes directly and specify a source if you want to continue it later:\n suspend fun uploadData(bytes: ByteArray) {\n val upload: ResumableUpload = supabase.storage.from(\"bucket_name\")\n .resumable.createOrContinueUpload(bytes, \"source\", \"file_path\")\n\n upload.stateFlow\n .onEach {\n println(it.progress)\n }\n .launchIn(yourCoroutineScope)\n upload.startOrResumeUploading()\n }\n ```\n\n Here's an example of how to upload a file using [`tus-py-client`](https://github.com/tus/tus-py-client):\n\n ```python\n from io import BufferedReader\n from tusclient import client\n from supabase import create_client\n\n def upload_file(\n bucket_name: str, file_name: str, file: BufferedReader, access_token: str\n ):\n # create Tus client\n my_client = client.TusClient(\n f\"{supabase_url}/storage/v1/upload/resumable\",\n headers={\"Authorization\": f\"Bearer {access_token}\", \"x-upsert\": \"true\"},\n )\n uploader = my_client.uploader(\n file_stream=file,\n chunk_size=(6 * 1024 * 1024),\n metadata={\n \"bucketName\": bucket_name,\n \"objectName\": file_name,\n \"contentType\": \"image/png\",\n \"cacheControl\": \"3600\",\n },\n )\n uploader.upload()\n\n # create client and sign in\n supabase = create_client(supabase_url, supabase_key)\n\n # retrieve the current user's session for authentication\n session = supabase.auth.get_session()\n\n # open file and send file stream to upload\n with open(\"./assets/40mb.jpg\", \"rb\") as fs:\n upload_file(\n bucket_name=\"assets\",\n file_name=\"large_file\",\n file=fs,\n access_token=session.access_token,\n )\n ```"
        },
        {
          "type": "section",
          "title": "Upload URL",
          "content": "When uploading using the resumable upload endpoint, the storage server creates a unique URL for each upload, even for multiple uploads to the same path. All chunks will be uploaded to this URL using the `PATCH` method.\n\nThis unique upload URL will be valid for **up to 24 hours**. If the upload is not completed within 24 hours, the URL will expire and you'll need to start the upload again. TUS client libraries typically create a new URL if the previous one expires.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Concurrency",
          "content": "When two or more clients upload to the same upload URL only one of them will succeed. The other clients will receive a `409 Conflict` error. Only 1 client can upload to the same upload URL at a time which prevents data corruption.\n\nWhen two or more clients upload a file to the same path using different upload URLs, the first client to complete the upload will succeed and the other clients will receive a `409 Conflict` error.\n\nIf you provide the `x-upsert` header the last client to complete the upload will succeed instead.",
          "level": 3
        },
        {
          "type": "section",
          "title": "Uppy example",
          "content": "You can check a [full example using Uppy](https://github.com/supabase/supabase/tree/master/examples/storage/resumable-upload-uppy).\n\nUppy has integrations with different frameworks:\n\n- [React](https://uppy.io/docs/react/)\n- [Svelte](https://uppy.io/docs/svelte/)\n- [Vue](https://uppy.io/docs/vue/)\n- [Angular](https://uppy.io/docs/angular/)",
          "level": 3
        },
        {
          "type": "section",
          "title": "Overwriting files",
          "content": "When uploading a file to a path that already exists, the default behavior is to return a `400 Asset Already Exists` error.\nIf you want to overwrite a file on a specific path you can set the `x-upsert` header to `true`.\n\nWe do advise against overwriting files when possible, as the CDN will take some time to propagate the changes to all the edge nodes leading to stale content.\nUploading a file to a new path is the recommended way to avoid propagation delays and stale content.\n\nTo learn more, see the [CDN](/docs/guides/storage/cdn/fundamentals) guide.",
          "level": 2
        }
      ],
      "wordCount": 1202,
      "characterCount": 9286
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-uploads-s3-uploads",
      "identifier": "storage-uploads-s3-uploads",
      "name": "S3 Uploads",
      "description": "Learn how to upload files to Supabase Storage using S3.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/uploads/s3-uploads",
      "dateModified": "2025-06-13T12:45:11.311721",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/uploads/s3-uploads.mdx",
      "frontmatter": {
        "id": "s3-uploads",
        "title": "S3 Uploads",
        "description": "Learn how to upload files to Supabase Storage using S3.",
        "subtitle": "Learn how to upload files to Supabase Storage using S3.",
        "sidebar_label": "Uploads"
      },
      "sections": [
        {
          "type": "introduction",
          "title": "Introduction",
          "content": "You can use the S3 protocol to upload files to Supabase Storage. To get started with S3, see the [S3 setup guide](/docs/guides/storage/s3/authentication).\n\nThe S3 protocol supports file upload using:\n\n- A single request\n- Multiple requests via Multipart Upload"
        },
        {
          "type": "section",
          "title": "Single request uploads",
          "content": "The `PutObject` action uploads the file in a single request. This matches the behavior of the Supabase SDK [Standard Upload](/docs/guides/storage/uploads/standard-uploads).\n\nUse `PutObject` to upload smaller files, where retrying the entire upload won't be an issue. The maximum file size on paid plans is 50 GB.\n\nFor example, using JavaScript and the `aws-sdk` client:\n\n```javascript\nimport { S3Client, PutObjectCommand } from '@aws-sdk/client-s3'\n\nconst s3Client = new S3Client({...})\n\nconst file = fs.createReadStream('path/to/file')\n\nconst uploadCommand = new PutObjectCommand({\n Bucket: 'bucket-name',\n Key: 'path/to/file',\n Body: file,\n ContentType: 'image/jpeg',\n})\n\nawait s3Client.send(uploadCommand)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Multipart uploads",
          "content": "Multipart Uploads split the file into smaller parts and upload them in parallel, maximizing the upload speed on a fast network. When uploading large files, this allows you to retry the upload of individual parts in case of network issues.\n\nThis method is preferable over [Resumable Upload](/docs/guides/storage/uploads/resumable-uploads) for server-side uploads, when you want to maximize upload speed at the cost of resumability. The maximum file size on paid plans is 50 GB.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Upload a file in parts",
          "content": "Use the `Upload` class from an S3 client to upload a file in parts. For example, using JavaScript:\n\n```javascript\nimport { S3Client } from '@aws-sdk/client-s3'\nimport { Upload } from '@aws-sdk/lib-storage'\n\nconst s3Client = new S3Client({...})\n\nconst file = fs.createReadStream('path/to/very-large-file')\n\nconst upload = new Upload(s3Client, {\n Bucket: 'bucket-name',\n Key: 'path/to/file',\n ContentType: 'image/jpeg',\n Body: file,\n})\n\nawait uploader.done()\n```",
          "level": 3
        },
        {
          "type": "section",
          "title": "Aborting multipart uploads",
          "content": "All multipart uploads are automatically aborted after 24 hours. To abort a multipart upload before that, you can use the [`AbortMultipartUpload`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_AbortMultipartUpload.html) action.",
          "level": 3
        }
      ],
      "wordCount": 295,
      "characterCount": 2257
    },
    {
      "@context": "https://schema.org",
      "@type": "DocumentationPage",
      "@id": "supabase:storage-uploads-standard-uploads",
      "identifier": "storage-uploads-standard-uploads",
      "name": "Standard Uploads",
      "description": "Learn how to upload files to Supabase Storage.",
      "category": "storage",
      "url": "https://supabase.com/docs/guides/storage/uploads/standard-uploads",
      "dateModified": "2025-06-13T12:45:11.312010",
      "publisher": {
        "@type": "Organization",
        "name": "Supabase",
        "url": "https://supabase.com"
      },
      "sourceFile": "/Users/liambray/Downloads/supabase-master/apps/docs/content/guides/storage/uploads/standard-uploads.mdx",
      "frontmatter": {
        "id": "standard-uploads",
        "title": "Standard Uploads",
        "description": "Learn how to upload files to Supabase Storage.",
        "subtitle": "Learn how to upload files to Supabase Storage.",
        "sidebar_label": "Uploads"
      },
      "sections": [
        {
          "type": "section",
          "title": "Uploading",
          "content": "The standard file upload method is ideal for small files that are not larger than 6MB.\n\nIt uses the traditional `multipart/form-data` format and is simple to implement using the supabase-js SDK. Here's an example of how to upload a file using the standard upload method:\n\nThough you can upload up to 5GB files using the standard upload method, we recommend using [TUS Resumable Upload](/docs/guides/storage/uploads/resumable-uploads) for uploading files greater than 6MB in size for better reliability.\n\n```javascript\n// @noImplicitAny: false\n\n// ---cut---\nimport { createClient } from '@supabase/supabase-js'\n\n// Create Supabase client\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\n// Upload file using standard upload\nasync function uploadFile(file) {\n const { data, error } = await supabase.storage.from('bucket_name').upload('file_path', file)\n if (error) {\n // Handle error\n } else {\n // Handle success\n }\n}\n```\n\n```dart\n// Upload file using standard upload\nFuture uploadFile(File file) async {\n await supabase.storage.from('bucket_name').upload('file_path', file);\n}\n```\n\n```swift\nimport Supabase\n\n// Create Supabase client\nlet supabase = SupabaseClient(supabaseURL: URL(string: \"your_project_url\")!, supabaseKey: \"your_supabase_api_key\")\n\ntry await supabase.storage.from(\"bucket_name\").upload(path: \"file_path\", file: file)\n```\n\n```kotlin\nsupabase.storage.from(\"bucket_name\").upload(\"file_path\", bytes)\n\n//Or on JVM/Android: (This will stream the data from the file to supabase)\nsupabase.storage.from(\"bucket_name\").upload(\"file_path\", file)\n```\n\n```python\nresponse = supabase.storage.from_('bucket_name').upload('file_path', file)\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Overwriting files",
          "content": "When uploading a file to a path that already exists, the default behavior is to return a `400 Asset Already Exists` error.\nIf you want to overwrite a file on a specific path you can set the `upsert` options to `true` or using the `x-upsert` header.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst file = new Blob()\n\n// ---cut---\n// Create Supabase client\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\nawait supabase.storage.from('bucket_name').upload('file_path', file, {\n upsert: true,\n})\n```\n\n```dart\nawait supabase.storage.from('bucket_name').upload(\n 'file_path',\n file,\n fileOptions: const FileOptions(upsert: true),\n );\n```\n\n```swift\nimport Supabase\n\n// Create Supabase client\nlet supabase = SupabaseClient(supabaseURL: URL(string: \"your_project_url\")!, supabaseKey: \"your_supabase_api_key\")\n\ntry await supabase.storage.from(\"bucket_name\")\n .upload(\n path: \"file_path\",\n file: file,\n options: FileOptions(\n upsert: true\n )\n )\n```\n\n```kotlin\nsupabase.storage.from(\"bucket_name\").upload(\"file_path\", bytes) {\n upsert = true\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket_name').upload('file_path', file, {\n 'upsert': 'true',\n})\n```\n\nWe do advise against overwriting files when possible, as our Content Delivery Network will take sometime to propagate the changes to all the edge nodes leading to stale content.\nUploading a file to a new path is the recommended way to avoid propagation delays and stale content.",
          "level": 2
        },
        {
          "type": "section",
          "title": "Content type",
          "content": "By default, Storage will assume the content type of an asset from the file extension. If you want to specify the content type for your asset, pass the `contentType` option during upload.\n\n```javascript\nimport { createClient } from '@supabase/supabase-js'\nconst file = new Blob()\n\n// ---cut---\n// Create Supabase client\nconst supabase = createClient('your_project_url', 'your_supabase_api_key')\n\nawait supabase.storage.from('bucket_name').upload('file_path', file, {\n contentType: 'image/jpeg',\n})\n```\n\n```dart\nawait supabase.storage.from('bucket_name').upload(\n 'file_path',\n file,\n fileOptions: const FileOptions(contentType: 'image/jpeg'),\n );\n```\n\n```swift\nimport Supabase\n\n// Create Supabase client\nlet supabase = SupabaseClient(supabaseURL: URL(string: \"your_project_url\")!, supabaseKey: \"your_supabase_api_key\")\n\ntry await supabase.storage.from(\"bucket_name\")\n .upload(\n path: \"file_path\",\n file: file,\n options: FileOptions(\n contentType: \"image/jpeg\"\n )\n )\n```\n\n```kotlin\nsupabase.storage.from(\"bucket_name\").upload(\"file_path\", bytes) {\n contentType = ContentType.Image.JPEG\n}\n```\n\n```python\nresponse = supabase.storage.from_('bucket_name').upload('file_path', file, {\n 'content-type': 'image/jpeg',\n})\n```",
          "level": 2
        },
        {
          "type": "section",
          "title": "Concurrency",
          "content": "When two or more clients upload a file to the same path, the first client to complete the upload will succeed and the other clients will receive a `400 Asset Already Exists` error.\nIf you provide the `x-upsert` header the last client to complete the upload will succeed instead.",
          "level": 2
        }
      ],
      "wordCount": 560,
      "characterCount": 4723
    }
  ]
}